\chapter{RadioProphet: Optimizing Smartphone Energy and Radio Resource in Cellular Networks}
\label{chap:optimize}

Achieving energy efficiency for mobile devices when connected to cellular networks without incurring excessive network signaling overhead, even despite diverse application and user behavior, still remains a rather difficult and yet important challenge to tackle. Energy use due to network access, particularly cellular networks, is becoming increasingly dominant due to numerous network-based smartphone applications. In many cases, achieving network energy savings must reside on the mobile device's OS to effectively and centrally manage the data scheduling decisions transparent to applications and with minimal changes to the network.

The key mechanism that determines the energy consumed by cellular network interface is the radio resource control (RRC) state machine~\cite{imc.3g} pre-defined by carriers (Figure~\ref{fig:rrc}) that governs when radio resources are acquired and released. Previous studies~\cite{imc.tailender, imc.3g, qian10_icnp, huang_mobisys12} have shown that the origins of low resource efficiency comes from the way radio resources are \emph{released}. To avoid unnecessary state transitions, radio resources are only released after an idle time (also known as the ``tail time'') controlled by a statically configured inactivity timer. During the tail time, radio energy is essentially wasted. Values as large as 11.6 seconds are configured~\cite{huang12_mobisys} in current networks, contributing to about half of the total radio energy on user handsets (UEs) spent in idle times for common usage scenarios.

Without knowing when network traffic will occur, large tail timer settings are essentially a conservative way to ensure low signaling overhead due to state transitions, as signaling is known to be a bottleneck for cellular networks.  Furthermore, they also help minimize performance impact experienced by users caused by state promotion delays incurred whenever radio resource is acquired.
Given that application and user behavior are not random, using a statically configured inactivity timer is clearly suboptimal. Smaller static timer values would help reduce radio energy, but is not an option due to the risk of overloading cellular networks caused by signaling load increase.

An attractive alternative is to configure the timer dynamically --- adaptively performing radio resource release either signaled by the UE or triggered by the network itself by monitoring the UE traffic, accommodating different traffic patterns, improving the overall resource efficiency. But the key challenge is determining \emph{when} to release resources, which essentially comes down to accurate and efficient prediction of the idle time period.
Clearly, the best time to do so is when the UE is about to experience a long idle time period, otherwise the incurred resource allocation overhead (\ie signaling load) is wasteful due to unnecessary radio state transitions, and the achieved resource savings are very small.
Therefore, accurate and efficient prediction of the idle time period is a critical prerequisite for dynamic timer schemes.


\iffalse
\begin{enumerate}[(1)] \itemsep=-0.0ex
%\item Reduce the tail power consumption at the UE.
\item Change the fixed inactivity timer to a smaller yet still statically configured value.
\item The UE performs resource deallocation in an \emph{adaptive} manner. Specifically, the UE can actively notify the network for immediate resource release when necessary.
\item Network monitors the UE traffic and dynamically changes the inactivity timer.
\end{enumerate}
\vspace{-.05in}

Approach (1) is orthogonal to (2) to (4), requiring potential changes in the protocol and hardware, and therefore is not considered in this work.
The disadvantage of (2) still comes from the static nature of treating all traffic according to the same resource management policy, making it difficult to balance tradeoffs among the radio resource usage efficiency and the signaling overhead~\cite{imc.3g}. Our evaluation further confirms this. Approaches (3) and (4) seem attractive in that the resource management can potentially be tailored to different traffic patterns, therefore improving the resource efficiency. But the key challenge is determining \emph{when} to release resources.
Clearly, the best time to do so is when the UE is going to experience a long idle time period otherwise the incurred resource allocation overhead (\ie signaling load) may be prohibitive due to additional radio state promotions.
Therefore, accurate and efficient prediction of the idle time period is a critical prerequisite for dynamic timer schemes (3) and (4).
\fi

%We do compare approach (2) with our proposal, as it is a simple extension to today's setup. Intuitively, reducing $T_{tail}$ saves UE energy, with increased promotion overhead, and we try to reduce this overhead for the same amount of energy saving.

%Intuitively, adapting the $T_{tail}$ during run time to application and user behavior is helpful to achieve the optimal tradeoffs. Therefore, we believe it is critical to comprehensively investigate approaches (3) and (4), where either the UE or the network actively determines radio resource deallocation.

%For either approach to work properly, it is important to accurately predict the inter-burst time (\IBT), which is the idle time between consecutive traffic bursts. Intuitively, it is preferable to deallocate radio resource right after a burst that is followed by a long \IBT.
This paper proposes \NAMEFULL (\NAME), a practical system that makes dynamic decisions to deallocate radio resources based on accurate and efficient prediction of network idle times. It makes the following contributions.

%\comment{Add pointers: for each contribution, refer to a later subsection.}


%long enough so that exchanging between resource saving and incurred signaling load is worthwhile.

First, \NAME utilizes standard online machine learning (ML) algorithms to accurately predict the network idle time, and performs resource deallocation only when the idle time is sufficiently long. We explored various ML algorithms and prediction models with tunable parameters, with the main contribution of finding robust and easy-to-measure features (not relying on packet payload), whose complex interaction with the network idle time can be automatically discovered by the ML algorithms. The model is validated using seven-month-long traces collected from real users (\S\ref{sec:evaluation}).
%We observe that small bursts dominate all traffic bursts and most bursts occur when users are idle.

Second, to reduce the runtime overhead, \NAME strategically performs \emph{binary} prediction (\ie whether the idle time is short or long) at the granularity of a traffic burst consisting of a packet train sent or received in a batch. Compared to fine-grained prediction of the precise value of packet inter-arrival time, our proposed approach is much more efficient while yielding similar optimization results.
%\comment{or even better results because binary prediction for \IBT is more robust? If so, mention it here and in Sec 3 as well}
We demonstrate that \NAME is practically deployable with negligible energy overhead on today's smartphones.
%It could be implemented at either the client or the network side. The network based approach, transparent to UEs, is a novel proposal in this work.

Third, we overcome critical limitations of previously proposed approaches, \ie RadioJockey~\cite{radiojockey} and MakeIdle/MakeActive~\cite{makeidle} are only applicable to background applications without user interaction, with 
the ideal usage scenario of RadioJockey for a single application only. With multiple concurrent applications, it suffers from low prediction accuracy with increased overhead. By design, \NAME applies to both foreground and background traffic, maximizing energy savings. Since its prediction is based on the aggregate traffic of all applications, \NAME incurs no additional performance degradation nor overhead for supporting concurrent apps.


Fourth, we evaluate \NAME using real-world smartphone traces. The overall prediction accuracy is 85.88\% for all traffic. \NAME achieves radio energy saving by 59.07\%, at the cost of 91.01\% additional signaling overhead in LTE networks, significantly outperforming previous proposals. In order to achieve such energy saving (59\%), the additional signaling overheads incurred by MakeIdle and na\"{\i}ve fast dormancy~\cite{fast.dormancy.1, fast.dormancy.2} are as high as 305\% and 215\%, respectively. The maximal energy saving achieved by RadioJockey is 27\% since it is only applicable to background traffic.


In this chapter, we cover background in \S\ref{sec:background} and present the design of \NAME in \S\ref{sec:overview}. Then we explore feature selection in \S\ref{sec:stats}. With the methodology discussed in \S\ref{sec:method}, we evaluate the performance of \NAME in \S\ref{sec:evaluation}. We summarize related work in \S\ref{sec:related}, before concluding in \S\ref{sec:conc}.





\nsection{Background}
\label{sec:background}

This section provides sufficient background on resource management in cellular networks.

\begin{figure}[t]
\centering
\IG{figures/rp/rrc.eps}\\
\ncaption{\small{RRC State Machine of (a) a large 3G UMTS carrier in the U.S., and (b) a large 4G LTE carrier in the U.S. The radio power consumption was measured by a power monitor on (a) an HTC TyTn II smartphone, (b) an HTC ThunderBolt smartphone}}
\label{fig:rrc}
\end{figure}

To efficiently utilize the limited resources, cellular networks employ a resource management policy distinguishing them from wired and Wi-Fi networks. In particular, there is a radio resource control (RRC) state machine~\cite{imc.3g} that determines radio resource usage based on application traffic patterns, affecting device energy consumption and user experience. Similar RRC state machines exist in different types of cellular networks such as UMTS~\cite{imc.3g}, EvDO~\cite{ChatterjeeD02} and 4G LTE networks~\cite{huang_mobisys12}, although the detailed state transition models may differ.

\textbf{RRC States.} In 3G UMTS networks, there are usually three RRC states~\cite{imc.3g, mobisys.aro}. \RI is the default state when the UE is turned on, with no radio resource allocated. \RD is the high-power state enabling high-speed data transmission. \RF is the low-power state in between allowing only low-speed data transmission. In 4G LTE networks, the low-power state is eliminated due to its extremely low bandwidth (less than 20 kbps) so there are only two RRC states named \RC and \RI~\cite{huang_mobisys12}.

\textbf{State Transitions.}
As shown in Figure~\ref{fig:rrc}, regardless of the specific state transition model, there are two types of state transitions. State promotions switch from a low-power state to a high-power state. They are triggered by user data transmission in either direction. State demotions go in the reverse direction, triggered by inactivity timers configured by the radio access network (RAN). For example, as shown in Figure~\ref{fig:rrc}b, at the \RC state, the RAN resets the \RC$\rightarrow$ \RI timer to a constant threshold $T_{tail}$=11.6 seconds whenever it observes any data frame. If there is no user data transmission for $T_{tail}$ seconds, the \RC$\rightarrow$ \RI timer expires and the state is demoted to \RI. The two timers in 3G UMTS networks use similar schemes (Figure~\ref{fig:rrc}a).

State promotions and demotions incur \emph{promotion delays} and \emph{tail times}, respectively, which distinguish cellular networks from other types of access networks.

\textbf{State promotions} incur a long ``ramp-up'' delay of up to several seconds during which tens of control messages are exchanged between the UE and the radio access network (RAN) for resource allocation. Excessive state promotions increase the signaling overhead at the RAN and degrade user experience, especially for short data transfers~\cite{3gpp:090941, qian10_icnp}.

\textbf{State demotions} incur \emph{tail times} that cause waste of radio resources and the UE energy~\cite{imc.tailender, imc.3g}. A tail is the idle time period matching the inactivity timer value before a state demotion, \eg the tail time is 11.6 seconds in Figure~\ref{fig:rrc}b. During the tail time, the UE still occupies transmission channels, and its radio power consumption is kept at the corresponding level of the RRC state.
As an example of the negative impact of the tail effect, periodically transferring small data bursts (\eg every one minute) can be extremely resource-inefficient in cellular networks due to the long tail appended to each periodic transfer instance which is small in size and short in duration~\cite{qian12_www}.

%A recent study~\cite{qian12_www} shows that for popular smartphone applications such as Facebook and Pandora, periodic transfers account for only 1.7\% of the overall traffic volume but contribute to 30\% of the total UE radio energy consumption.

\textbf{Adaptive Release of Radio Resources.} Why are tail times necessary? First, the overhead of resource allocation (\ie state promotions) is high and tail times prevent frequent allocation and deallocation of radio resources. Second, the current radio resource network has no easy way of predicting the network idle time of the UE, so it conservatively appends a tail to every network usage period. This naturally gives rise to the idea of letting the UE actively request for resource release: once an imminent long idle time period is predicted, the UE can actively notify the RAN to immediately perform a state demotion. Based on this intuition, a feature called fast dormancy has been proposed
to be included in 3GPP Release 7~\cite{fast.dormancy.1} and Release 8~\cite{fast.dormancy.2}. It allows the UE to send a control message to the RAN to immediately demote the RRC state to \RI (or a hibernating state called \RPCH) without experiencing the tail time. Fast dormancy is currently supported by several handsets~\cite{fast.dormancy.2}, which can dramatically reduce the radio resource and the UE energy usage while the potential penalty is the increased signaling load when used aggressively~\cite{3gpp:090941, qian10_icnp}. In this work we propose robust methodology for predicting the idle time period, enabling more effective usage of fast dormancy.


\nsection{The Design of the \NAMEFULL System}
\label{sec:overview}

As described in~\S\ref{sec:background}, the static tail times are the root cause of low resource efficiency in cellular networks. Previous work points out that around 50\% energy is incurred by the long high-power RRC tail for 3G~\cite{imc.3g} and 4G networks~\cite{huang12_mobisys}. Our proposed \NAMEFULL (\NAME) system leverages the fast dormancy feature to \emph{dynamically and intelligently determine when to release radio resources}.

\textbf{Challenge 1: the tradeoff between resource saving and signaling load.}
Clearly, the best time to perform resource deallocation is when the UE is going to experience a long idle time period $t$. Specifically, if $t$ is longer than the tail time, deallocating resources immediately after a data transfer saves resources without any penalty of signaling load (\ie state promotions). If $t$ is shorter than the tail time, performing deallocation ahead of time trades off resource saving with signaling load, because doing so will incur an additional state promotion. Such a critical tradeoff presents the key challenge of \emph{predicting the idle time} between data transfers so that fast dormancy is only invoked when the idle time is sufficiently long.

\textbf{Challenge 2: the tradeoff between prediction accuracy and system performance.} \NAME is a service running on the UE with limited computational capabilities and even more importantly, limited battery life, so we need to minimize the runtime overhead without sacrificing much of the prediction accuracy. Otherwise the resource utilized by \NAME itself may overpower its benefits.

\textbf{Challenge 3: the requirement to handle both foreground traffic and background traffic.} Idle time prediction is particularly difficult for applications involving user interactions. Previous systems, such as RadioJockey~\cite{radiojockey} and MakeActive~\cite{makeidle}, simply avoid this by only handling traffic generated by applications running in the background. To maximize the energy saving, the proposed system should be able to handle foreground traffic well, in addition to background traffic.

To address {\bf Challenge 1}, we established a novel machine-learning-based framework for idle time prediction. 
We explored a wide range of ML algorithms with tunable parameters, and evaluated their effectiveness and efficiency. More importantly, our primary focus is addressing the hard problem of selecting discriminating features that are relevant to our specific problem. We find that strategically using a few simple features (\eg packet direction, size, and application name, \etc) leads to a high prediction accuracy of 85.9\%.

%Various machine learning (ML) approaches and their applications have been extensively investigated in the past 20 years. It is known to be an effective approach for many applications of computer networking such as network traffic classification~\cite{jin11} and network troubleshooting~\cite{jin10}.

To address {\bf Challenge 2}, we perform \emph{binary} prediction at the granularity of a traffic \emph{burst} consisting of a train of packets sent or received in a batch. In other words, we find that the knowledge of whether the inter-burst time (\IBT) is short or long (determined by a threshold) is
already accurate enough for guiding the resource deallocation.
Such an approach is much more efficient while yielding similar optimization results compared to more fine-grained and more expensive prediction for the precise value of packet inter-arrival times.

Further, by design, since the prediction of \NAME is based on the aggregate traffic of all applications, it incurs no additional performance degradation nor overhead for supporting concurrent apps. In contrast, previous systems such as RadioJockey have the ideal usage case for a single app. 

To address {\bf Challenge 3}, we choose robust features for idle time prediction for all traffic. We also customize the prediction settings for different types of traffic, either background traffic without user interaction or foreground traffic likely to be triggered by users. We use smartphone screen status as a hint to indicate whether a user is interacting with the device, \ie screen-on traffic belongs to foreground applications. We then apply different settings for prediction based on the screen status and evaluate \NAME with all traffic.

\begin{figure}[t]
\centering
\IG{figures/rp/flow.eps} \\
\ncaption{Working flow of \NAMEFULL (\NAME)}
\label{fig:flow}
\end{figure}

We illustrate the design of  \NAMEFULL (\NAME) in Figure~\ref{fig:flow}. In summary, a data collector collects packet traces (only headers are required) and some other auxiliary information, \eg process association for all packets. The data collected is fed into the \IBT prediction framework, which trains models to predict the \IBT for the current burst. Then, based on the prediction result, the fast dormancy scheduler makes decision on whether to invoke fast dormancy.

For \IBT prediction, we define the traffic model as follows. For each user, the traffic trace is a sequence of network packets, $P_{i} (1 \leq i \leq n)$. $P_{i}$ could either be a downlink packet or an uplink one. Assume the timestamp of $P_{i}$ is $t_{i}$, $t_{i} \leq t_{j}$ for any $i < j$. Using a burst threshold \BT, the packets are divided into \textbf{\em bursts}, \ie $\{P_{p}, P_{p+1}, \cdots, P_{q}\}$ belongs to a burst $B$, if and only if: {\em i)} $t_{k + 1} - t_{k} \leq$ \BT for any $k \in \{p, \cdots, q - 1\}$, {\em ii)} $t_{q+1} - t_{q} > $ \BT and {\em iii)} $t_{p} - t_{p - 1} > $ \BT. We define the inter-burst time \IBT for burst $B$ to be the time gap following this burst, \ie $t_{q+1} - t_{q}$. We also use a short \IBT threshold \SBT to classify an \IBT, \ie if \IBT $\leq$ \SBT, it is {\em short}, otherwise, it is {\em long}.

%Figure~\ref{fig:flow} presents the working flow of \NAME. The core component is an \IBT prediction framework.
The \IBT prediction framework trains the prediction model based on historical traffic information collected by the data collector. The information collected is an array of bursts $\{B_{1}, \cdots, B_{m}\}$. Each $B_{i}$ is a vector

\begin{displaymath}
B_{i}:\quad <f_{1}, f_{2}, \cdots, f_{t}, ibt_{i}>
\end{displaymath}

where $\{f_{1}, \cdots, f_{t}\}$ is the list of features of $B_{i}$ and $ibt_{i}$ is the \IBT following burst $B_{i}$.
%The trained prediction model may or may not be updated periodically, depending on the design choice.
%The prediction framework collects real-time information and updates the prediction model periodically based on the recent history of burst information. The update frequency is adjustable.
With the prediction model,  \NAME monitors the UE traffic in real-time and whenever there is an idle time of \BT, \ie the last packet's arrival time was \BT time ago, the prediction process starts. The feature vector of the current burst $\{f_{1}, \cdots, f_{t}\}$ is generated and fed to the prediction framework, which predicts whether the \IBT following the current burst is short or long. If short, as shown in Figure~\ref{fig:flow}, no change is made and UE stays in the tail, since the prediction framework suggests that a packet would appear soon. Otherwise, the prediction framework triggers fast dormancy to save energy.


\nsection{Feature Selection}
\label{sec:stats}

In this section, we study the traffic burst characteristics in the \UMICH data set. Then we look at a few burst features for \IBT prediction.

\nsubsection{The \UMICH Dataset}
\label{sec:data}
The measurement data used in this study, which we call the \UMICH data set, is collected from 20 smartphone users for five months. The participants consisted of undergraduate and graduate students from University of Michigan. The 20 participants are given Motorola Atrix (11 of them) or Samsung Galaxy S smartphones (9 of them) with unlimited voice, text and data plans.
%They are encouraged to take advantage of all the features and services of the phones.
%As stated in our study consent form, we keep collected data and user identities strictly confidential.

We develop custom data collection software and deploy it on the 20 smartphones. It continuously runs in the background and collects three types of data. {\em (1)} Full packet traces in \texttt{tcpdump} format including both headers and payload. The detailed packet traces allows us to keep detailed track of traffic patterns generated by each individual user for a long period. {\em (2)} The process name responsible for sending or receiving each packet.
%This packet-to-process correspondence is derived by efficiently correlating three mappings in Android OS in realtime: \texttt{/proc/PID/fd} (inode of each TCP/UDP socket$\rightarrow$Process ID), \texttt{/proc/net/tcp(udp)} (socket$\rightarrow$inode), and \texttt{/proc/PID/cmdline} (Process ID $\rightarrow$ Process name).
{\em (3)} User input events such as pressing a button and tapping the screen. This is collected by reading \texttt{/dev/input/event*} in the Android system.
We also build a data uploader that uploads the data to our server when the phone is idle as indicated by low network throughput.
%The upload is suspended by increased network activity of user applications. Since the data collector needs to be shut down when the data is being uploaded, we set the interval of consecutive uploading attempts to 6 hours to minimize the impact of uploader on data collection. The entire data collection and uploading process is transparent to the users, although we do advise the users to keep their phones powered on as often as possible.

We deployed the user study in May 2011. From May 9 2011 to December 5 2011, we collect 152GB data (150 GB \texttt{tcpdump} traces). Although both cellular and Wi-Fi traces were collected, in this study, we consistently use 3G traces only, which contribute 57.82\% of the total traffic volume, in the \UMICH dataset for further analysis. Comparing with the Wi-Fi traces, 3G traces better represent the user and application behavioral patterns in cellular networks.

\nsubsection{Characteristics of Bursts}

For burst characterization, we first need to pick a value for \BT. Obviously, \BT should be larger than typical RTTs in cellular networks, which is around hundreds of milliseconds~\cite{mobisys.3gtest, huang_mobisys12}, so as not to break a single continuous TCP flow into separate bursts. In the meanwhile, \BT should not be set to be too large; otherwise, there could be a large gap inside a burst, during which, fast dormancy could have been triggered to save energy. We start our analysis with \BT $= 1s$, resulting in 5.44 million bursts in total, and later we would explore how different \BT values affect our results.

\begin{figure}[t]
\centering
\IG{figures/rp/cdf_ibt.eps} \\
\ncaption{CDF of IBT for sampled/all users}
\label{fig:cdf.ibt}
\end{figure}


By analyzing all 3G traces in the \UMICH data set, we plot the CDF of \IBTS for all users (\IBTS are calculated separately for each user before aggregate analysis) and two sample users in Figure~\ref{fig:cdf.ibt}. The median of \IBT for all users is 4.46 seconds, and 68.24\% of \IBTS are smaller than the tail timer $T_{tail}$ in our studied network.

In Figure~\ref{fig:cdf.ibt}, we also compare the aggregate curve of all users with those of two sample users. The difference for \IBT distribution is mainly due to the user preference over different applications, \eg sample user 1's curve has a big jump around 20 seconds and this is verified to be correlated with the heavy use of the Facebook application as shown in Figure~\ref{fig:appid1}. Therefore, \NAME predicts \IBT using a per-user model instead of a global model for all users. Even for the same user, \ie user 1, at day 1 and day 2, which are months apart, we also observe a clear difference. We observe that user 1 uses a quite different list of applications for these two days, though not completely different. Even for the same application, different versions may also result in traffic pattern changes. So we think it is not practical to build a static model for each user. Instead, \NAME uses a dynamic model based on the most recent traffic history for each user.


\begin{figure}[t]
\centering
\IG{figures/rp/cdf_screenOn.eps} \\
\ncaption{CDF of IBT for different screen status}
\label{fig:cdf.screen}
\end{figure}
Choosing a proper value for \SBT is challenging, \ie a large \SBT reduces the chance of \NAME for saving energy and a small \SBT would result in excessive signaling load, since \NAME would trigger fast dormancy for long \IBTS. Previous work~\cite{imc.screen} indicates that traffic pattern is different between screen-on state and screen-off state. In Figure~\ref{fig:cdf.screen}, we also observe that screen-off traffic has higher \IBTS than screen-on traffic. The median \IBT for screen-off and screen-on traffic is 5.00s and 3.14s, respectively. This motivates us to select different \SBT values based on the different screen states, \ie we could use an more aggressive (smaller) value of \SBT to trigger more fast dormancy for screen-off traffic.

\begin{figure}[t]
\centering
\IG{figures/rp/packetNum.eps} \\
\ncaption{Distribution of bursts grouped by \# of packets}
\label{fig:packetNum}
\end{figure}
Figure~\ref{fig:packetNum} shows the distribution of the number of packets of a burst. We observe that the distribution is heavy-tailed. In particular, 27.69\% of all bursts only contain 1 single packet, and 75.00\% of these 1-packet bursts are short bursts. About 52.63\% of all bursts consist of no more than 3 packets and 73.47\% bursts contain no more than 10 packets, indicating that small bursts dominate the user traffic.


\nsubsection{Selecting Features for Burst Classification}

To predict whether an \IBT is larger or smaller than \SBT, we naturally choose to look at the features of the burst preceding the \IBT. Based on our observation, the correlation between \IBT and the bursts further beyond the preceding burst is small, hence we do not use those bursts for training features. For the preceding burst, we look at the detailed features of the last three packets. The choice of three packets is because in most cases, bursts are small, and even for large bursts, we can tell the nature of a burst based on the last three packets, \eg TCP three-way handshake for connection establishment and termination. For each of the last three packets, we only look at header information, not the payload, since many of these packet either do not have any payload, or use encryption. The features of the last three packets~\footnote{If a burst contains less than three packets, all features for the missing packet(s) have a value of 0.} used for training are listed in Table~\ref{tab:feature} and we study some of them as follows. These features are selected empirically that are most relevant to \IBT.


\begin{table}[t]
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|}\hline
Name & Symbol & Description\\\hline
Packet direction & {\em Dir(i)$^{\star}$} & Downlink or uplink \\\hline
Server port number &{\em Port(i)} & The remote port number\\\hline
Total packet length &{\em Len(i)} & Including headers\\\hline
Protocol  & {\em Prot(i)} & Protocol field in the IP header\\\hline
\MR{TCP flags}  & \MR{{\em Flag(i)}} & TCP flags field in  the TCP\\
  &  & header and 0 if not TCP\\\hline
\MR{Application ID} & \MR{{\em AppID(i)}} & The global application ID \\
 &  & which generates this packet\\\hline
\end{tabular}
\begin{tabular}{l}
\\{\small $^\star$} {\em Dir(i)} (i = 1, 2 or 3) refers to the $i$-th to last packet,  \eg when i = 1, \\
{\small} \ \ it refers to the last packet. Similar rule applies to all other features.
\end{tabular}
\label{tab:feature}
\ncaption{Features for the last three packets of a burst}
\end{center}
\end{table}

\begin{figure}[t]
\centering
\IG{figures/rp/cdf_dir1.eps} \\
\ncaption{CDF of \IBT v.s. direction of the last packet}
\label{fig:dir1}
\end{figure}
In Figure~\ref{fig:dir1}, CDF of \IBT for the two possible directions of the last packet is shown. In general, \IBT following a burst whose last packet is downlink is larger than that for uplink, \eg the median \IBT is 6.49 seconds for downlink and 3.79 seconds for uplink.
%Intuitively, this is because when the last packet is uplink, to some extent, it indicates that UE is at a ``pending'' state awaiting for response from the server side and \IBT after such bursts are typically smaller.

\begin{figure}[t]
\centering
\IG{figures/rp/cdf_port1.eps} \\
\ncaption{CDF of \IBT v.s. port of the last packet}
\label{fig:port1}
\end{figure}
Figure~\ref{fig:port1} shows the CDF of \IBTS of all users for the top 6 ports ordered from top to bottom in the legend, \ie port 80 is the top 1 port and port 993 ranks 6-th. \IBT distribution for different ports have clear differences, especially for port 53. By further investigation on the traffic traces, we know that the sudden jump for port 53 at \IBT $=5$ seconds is related with the DNS request retransmission behavior, \ie when UE fails to get DNS response within 5 seconds, either due to poor signal strength or slow DNS response, it resends the DNS request and the initial DNS request itself forms a burst. We also observe that the jump for port 5222 around 20 seconds corresponds to the periodic keep-alive messages for Facebook app.

\begin{figure}[t]
\centering
\IG{figures/rp/len1.eps} \\
\ncaption{Distribution of bursts grouped by packet length of the last packet}
\label{fig:len1}
\end{figure}

Figure~\ref{fig:len1} summarizes the number of bursts for different packet length of the last packet. Packet length outside the plotted range has much fewer bursts. As expected, we observe that most bursts ends with small packets, \ie 84.59\% of all bursts have the last packet $\leq$ 100 bytes, since large packet typically indicates being in the middle of data transfer, thus not the end of a burst. For some other values, \eg 121 bytes, 93.04\% bursts are short bursts indicating high correlation. Machine learning algorithms could learn such rules and make prediction. Although there is less correlation between \IBT and packet length for the two major spikes at 40 bytes (71.05\% short bursts) and 52 bytes (70.00\% short bursts), machine learning algorithms would consider other features for prediction automatically for higher accuracy.

\begin{figure}[t]
\centering
\IG{figures/rp/flags1.eps} \\
\ncaption{Distribution of \IBT grouped by the type of the last packet}
\label{fig:flag1.bar}
\end{figure}

In Figure~\ref{fig:flag1.bar}, the number of bursts for different {\em Flag(1)} is presented, sorted in a descending order from left to right. Specifically, for {\em Flag(1) == 0}, such bursts fall into two groups, UDP packets and ICMP packets. Besides UDP packets, we also observe strong correlation between {\em Flag(1)} and short \IBTS for SYN and PUSH-ACK packets, with 90.44\% and 90.72\% confidence, respectively.

\begin{figure}[h]
\centering
\IG{figures/rp/cdf_appid1.eps} \\
\ncaption{CDF of \IBT v.s. the application generating the last packet}
\label{fig:appid1}
\end{figure}
The last feature we look at for each packet is the application ID. Notice that this feature is not directly obtainable from packet traces and only available at the client side~\footnote{At the network side, using the \textbf{User-Agent} field in each TCP flow, the application name may be inferred, however, it is not 100\% accurate and for non-HTTP traffic, no such information is available.}. In Figure~\ref{fig:appid1}, the legend shows the sorted top list of applications contributing to most bursts with Facebook as the top 1 app. The application-wise differences of \IBT is obvious which necessities the choice of this feature. We also observe that for some applications, periodic transfer behavior contributes to clear jumps in \IBT distribution, \eg Faceboook and LiveProfile.





	+ screen-aware optimization
