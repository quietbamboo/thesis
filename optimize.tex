\chapter{RadioProphet: Optimizing Smartphone Energy and Radio Resource in Cellular Networks}
\label{chap:optimize}

Achieving energy efficiency for mobile devices when connected to cellular networks without incurring excessive network signaling overhead, even despite diverse application and user behavior, still remains a rather difficult and yet important challenge to tackle. Energy use due to network access, particularly cellular networks, is becoming increasingly dominant due to numerous network-based smartphone applications. In many cases, achieving network energy savings must reside on the mobile device's OS to effectively and centrally manage the data scheduling decisions transparent to applications and with minimal changes to the network.

The key mechanism that determines the energy consumed by cellular network interface is the radio resource control (RRC) state machine~\cite{imc.3g} pre-defined by carriers (Figure~\ref{fig:rrc}) that governs when radio resources are acquired and released. Previous studies~\cite{imc.tailender, imc.3g, qian10_icnp, huang_mobisys12} have shown that the origins of low resource efficiency comes from the way radio resources are \emph{released}. To avoid unnecessary state transitions, radio resources are only released after an idle time (also known as the ``tail time'') controlled by a statically configured inactivity timer. During the tail time, radio energy is essentially wasted. Values as large as 11.6 seconds are configured~\cite{huang12_mobisys} in current networks, contributing to about half of the total radio energy on user handsets (UEs) spent in idle times for common usage scenarios.

Without knowing when network traffic will occur, large tail timer settings are essentially a conservative way to ensure low signaling overhead due to state transitions, as signaling is known to be a bottleneck for cellular networks.  Furthermore, they also help minimize performance impact experienced by users caused by state promotion delays incurred whenever radio resource is acquired.
Given that application and user behavior are not random, using a statically configured inactivity timer is clearly suboptimal. Smaller static timer values would help reduce radio energy, but is not an option due to the risk of overloading cellular networks caused by signaling load increase.

An attractive alternative is to configure the timer dynamically --- adaptively performing radio resource release either signaled by the UE or triggered by the network itself by monitoring the UE traffic, accommodating different traffic patterns, improving the overall resource efficiency. But the key challenge is determining \emph{when} to release resources, which essentially comes down to accurate and efficient prediction of the idle time period.
Clearly, the best time to do so is when the UE is about to experience a long idle time period, otherwise the incurred resource allocation overhead (\ie signaling load) is wasteful due to unnecessary radio state transitions, and the achieved resource savings are very small.
Therefore, accurate and efficient prediction of the idle time period is a critical prerequisite for dynamic timer schemes.


\iffalse
\begin{enumerate}[(1)] \itemsep=-0.0ex
%\item Reduce the tail power consumption at the UE.
\item Change the fixed inactivity timer to a smaller yet still statically configured value.
\item The UE performs resource deallocation in an \emph{adaptive} manner. Specifically, the UE can actively notify the network for immediate resource release when necessary.
\item Network monitors the UE traffic and dynamically changes the inactivity timer.
\end{enumerate}
\vspace{-.05in}

Approach (1) is orthogonal to (2) to (4), requiring potential changes in the protocol and hardware, and therefore is not considered in this work.
The disadvantage of (2) still comes from the static nature of treating all traffic according to the same resource management policy, making it difficult to balance tradeoffs among the radio resource usage efficiency and the signaling overhead~\cite{imc.3g}. Our evaluation further confirms this. Approaches (3) and (4) seem attractive in that the resource management can potentially be tailored to different traffic patterns, therefore improving the resource efficiency. But the key challenge is determining \emph{when} to release resources.
Clearly, the best time to do so is when the UE is going to experience a long idle time period otherwise the incurred resource allocation overhead (\ie signaling load) may be prohibitive due to additional radio state promotions.
Therefore, accurate and efficient prediction of the idle time period is a critical prerequisite for dynamic timer schemes (3) and (4).
\fi

%We do compare approach (2) with our proposal, as it is a simple extension to today's setup. Intuitively, reducing $T_{tail}$ saves UE energy, with increased promotion overhead, and we try to reduce this overhead for the same amount of energy saving.

%Intuitively, adapting the $T_{tail}$ during run time to application and user behavior is helpful to achieve the optimal tradeoffs. Therefore, we believe it is critical to comprehensively investigate approaches (3) and (4), where either the UE or the network actively determines radio resource deallocation.

%For either approach to work properly, it is important to accurately predict the inter-burst time (\IBT), which is the idle time between consecutive traffic bursts. Intuitively, it is preferable to deallocate radio resource right after a burst that is followed by a long \IBT.
This paper proposes \NAMEFULL (\NAME), a practical system that makes dynamic decisions to deallocate radio resources based on accurate and efficient prediction of network idle times. It makes the following contributions.

%\comment{Add pointers: for each contribution, refer to a later subsection.}


%long enough so that exchanging between resource saving and incurred signaling load is worthwhile.

First, \NAME utilizes standard online machine learning (ML) algorithms to accurately predict the network idle time, and performs resource deallocation only when the idle time is sufficiently long. We explored various ML algorithms and prediction models with tunable parameters, with the main contribution of finding robust and easy-to-measure features (not relying on packet payload), whose complex interaction with the network idle time can be automatically discovered by the ML algorithms. The model is validated using seven-month-long traces collected from real users (\S\ref{sec:evaluation}).
%We observe that small bursts dominate all traffic bursts and most bursts occur when users are idle.

Second, to reduce the runtime overhead, \NAME strategically performs \emph{binary} prediction (\ie whether the idle time is short or long) at the granularity of a traffic burst consisting of a packet train sent or received in a batch. Compared to fine-grained prediction of the precise value of packet inter-arrival time, our proposed approach is much more efficient while yielding similar optimization results.
%\comment{or even better results because binary prediction for \IBT is more robust? If so, mention it here and in Sec 3 as well}
We demonstrate that \NAME is practically deployable with negligible energy overhead on today's smartphones.
%It could be implemented at either the client or the network side. The network based approach, transparent to UEs, is a novel proposal in this work.

Third, we overcome critical limitations of previously proposed approaches, \ie RadioJockey~\cite{radiojockey} and MakeIdle/MakeActive~\cite{makeidle} are only applicable to background applications without user interaction, with 
the ideal usage scenario of RadioJockey for a single application only. With multiple concurrent applications, it suffers from low prediction accuracy with increased overhead. By design, \NAME applies to both foreground and background traffic, maximizing energy savings. Since its prediction is based on the aggregate traffic of all applications, \NAME incurs no additional performance degradation nor overhead for supporting concurrent apps.


Fourth, we evaluate \NAME using real-world smartphone traces. The overall prediction accuracy is 85.88\% for all traffic. \NAME achieves radio energy saving by 59.07\%, at the cost of 91.01\% additional signaling overhead in LTE networks, significantly outperforming previous proposals. In order to achieve such energy saving (59\%), the additional signaling overheads incurred by MakeIdle and na\"{\i}ve fast dormancy~\cite{fast.dormancy.1, fast.dormancy.2} are as high as 305\% and 215\%, respectively. The maximal energy saving achieved by RadioJockey is 27\% since it is only applicable to background traffic.


In this chapter, we cover background in \S\ref{sec:background} and present the design of \NAME in \S\ref{sec:overview}. Then we explore feature selection in \S\ref{sec:stats}. With the methodology discussed in \S\ref{sec:method}, we evaluate the performance of \NAME in \S\ref{sec:evaluation}. We summarize related work in \S\ref{sec:related}, before concluding in \S\ref{sec:conc}.





\nsection{Background}
\label{sec:background}

This section provides sufficient background on resource management in cellular networks.

\begin{figure}[t]
\centering
\IG{figures/rp/rrc.eps}\\
\ncaption{\small{RRC State Machine of (a) a large 3G UMTS carrier in the U.S., and (b) a large 4G LTE carrier in the U.S. The radio power consumption was measured by a power monitor on (a) an HTC TyTn II smartphone, (b) an HTC ThunderBolt smartphone}}
\label{fig:rrc}
\end{figure}

To efficiently utilize the limited resources, cellular networks employ a resource management policy distinguishing them from wired and Wi-Fi networks. In particular, there is a radio resource control (RRC) state machine~\cite{imc.3g} that determines radio resource usage based on application traffic patterns, affecting device energy consumption and user experience. Similar RRC state machines exist in different types of cellular networks such as UMTS~\cite{imc.3g}, EvDO~\cite{ChatterjeeD02} and 4G LTE networks~\cite{huang_mobisys12}, although the detailed state transition models may differ.

\textbf{RRC States.} In 3G UMTS networks, there are usually three RRC states~\cite{imc.3g, mobisys.aro}. \RI is the default state when the UE is turned on, with no radio resource allocated. \RD is the high-power state enabling high-speed data transmission. \RF is the low-power state in between allowing only low-speed data transmission. In 4G LTE networks, the low-power state is eliminated due to its extremely low bandwidth (less than 20 kbps) so there are only two RRC states named \RC and \RI~\cite{huang_mobisys12}.

\textbf{State Transitions.}
As shown in Figure~\ref{fig:rrc}, regardless of the specific state transition model, there are two types of state transitions. State promotions switch from a low-power state to a high-power state. They are triggered by user data transmission in either direction. State demotions go in the reverse direction, triggered by inactivity timers configured by the radio access network (RAN). For example, as shown in Figure~\ref{fig:rrc}b, at the \RC state, the RAN resets the \RC$\rightarrow$ \RI timer to a constant threshold $T_{tail}$=11.6 seconds whenever it observes any data frame. If there is no user data transmission for $T_{tail}$ seconds, the \RC$\rightarrow$ \RI timer expires and the state is demoted to \RI. The two timers in 3G UMTS networks use similar schemes (Figure~\ref{fig:rrc}a).

State promotions and demotions incur \emph{promotion delays} and \emph{tail times}, respectively, which distinguish cellular networks from other types of access networks.

\textbf{State promotions} incur a long ``ramp-up'' delay of up to several seconds during which tens of control messages are exchanged between the UE and the radio access network (RAN) for resource allocation. Excessive state promotions increase the signaling overhead at the RAN and degrade user experience, especially for short data transfers~\cite{3gpp:090941, qian10_icnp}.

\textbf{State demotions} incur \emph{tail times} that cause waste of radio resources and the UE energy~\cite{imc.tailender, imc.3g}. A tail is the idle time period matching the inactivity timer value before a state demotion, \eg the tail time is 11.6 seconds in Figure~\ref{fig:rrc}b. During the tail time, the UE still occupies transmission channels, and its radio power consumption is kept at the corresponding level of the RRC state.
As an example of the negative impact of the tail effect, periodically transferring small data bursts (\eg every one minute) can be extremely resource-inefficient in cellular networks due to the long tail appended to each periodic transfer instance which is small in size and short in duration~\cite{qian12_www}.

%A recent study~\cite{qian12_www} shows that for popular smartphone applications such as Facebook and Pandora, periodic transfers account for only 1.7\% of the overall traffic volume but contribute to 30\% of the total UE radio energy consumption.

\textbf{Adaptive Release of Radio Resources.} Why are tail times necessary? First, the overhead of resource allocation (\ie state promotions) is high and tail times prevent frequent allocation and deallocation of radio resources. Second, the current radio resource network has no easy way of predicting the network idle time of the UE, so it conservatively appends a tail to every network usage period. This naturally gives rise to the idea of letting the UE actively request for resource release: once an imminent long idle time period is predicted, the UE can actively notify the RAN to immediately perform a state demotion. Based on this intuition, a feature called fast dormancy has been proposed
to be included in 3GPP Release 7~\cite{fast.dormancy.1} and Release 8~\cite{fast.dormancy.2}. It allows the UE to send a control message to the RAN to immediately demote the RRC state to \RI (or a hibernating state called \RPCH) without experiencing the tail time. Fast dormancy is currently supported by several handsets~\cite{fast.dormancy.2}, which can dramatically reduce the radio resource and the UE energy usage while the potential penalty is the increased signaling load when used aggressively~\cite{3gpp:090941, qian10_icnp}. In this work we propose robust methodology for predicting the idle time period, enabling more effective usage of fast dormancy.






	+ screen-aware optimization
