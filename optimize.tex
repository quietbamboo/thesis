\chapter{RadioProphet: Optimizing Smartphone Energy and Radio Resource in Cellular Networks}
\label{chap:optimize}

Achieving energy efficiency for mobile devices when connected to cellular networks without incurring excessive network signaling overhead, even despite diverse application and user behavior, still remains a rather difficult and yet important challenge to tackle. Energy use due to network access, particularly cellular networks, is becoming increasingly dominant due to numerous network-based smartphone applications. In many cases, achieving network energy savings must reside on the mobile device's OS to effectively and centrally manage the data scheduling decisions transparent to applications and with minimal changes to the network.

The key mechanism that determines the energy consumed by cellular network interface is the radio resource control (RRC) state machine~\cite{imc.3g} pre-defined by carriers (Figure~\ref{fig:rrc}) that governs when radio resources are acquired and released. Previous studies~\cite{imc.tailender, imc.3g, qian10_icnp, huang_mobisys12} have shown that the origins of low resource efficiency comes from the way radio resources are \emph{released}. To avoid unnecessary state transitions, radio resources are only released after an idle time (also known as the ``tail time'') controlled by a statically configured inactivity timer. During the tail time, radio energy is essentially wasted. Values as large as 11.6 seconds are configured~\cite{huang_mobisys12} in current networks, contributing to about half of the total radio energy on user handsets (UEs) spent in idle times for common usage scenarios.

Without knowing when network traffic will occur, large tail timer settings are essentially a conservative way to ensure low signaling overhead due to state transitions, as signaling is known to be a bottleneck for cellular networks.  Furthermore, they also help minimize performance impact experienced by users caused by state promotion delays incurred whenever radio resource is acquired.
Given that application and user behavior are not random, using a statically configured inactivity timer is clearly suboptimal. Smaller static timer values would help reduce radio energy, but is not an option due to the risk of overloading cellular networks caused by signaling load increase.

An attractive alternative is to configure the timer dynamically --- adaptively performing radio resource release either signaled by the UE or triggered by the network itself by monitoring the UE traffic, accommodating different traffic patterns, improving the overall resource efficiency. But the key challenge is determining \emph{when} to release resources, which essentially comes down to accurate and efficient prediction of the idle time period.
Clearly, the best time to do so is when the UE is about to experience a long idle time period, otherwise the incurred resource allocation overhead (\ie signaling load) is wasteful due to unnecessary radio state transitions, and the achieved resource savings are very small.
Therefore, accurate and efficient prediction of the idle time period is a critical prerequisite for dynamic timer schemes.


\iffalse
\begin{enumerate}[(1)] \itemsep=-0.0ex
%\item Reduce the tail power consumption at the UE.
\item Change the fixed inactivity timer to a smaller yet still statically configured value.
\item The UE performs resource deallocation in an \emph{adaptive} manner. Specifically, the UE can actively notify the network for immediate resource release when necessary.
\item Network monitors the UE traffic and dynamically changes the inactivity timer.
\end{enumerate}
\vspace{-.05in}

Approach (1) is orthogonal to (2) to (4), requiring potential changes in the protocol and hardware, and therefore is not considered in this work.
The disadvantage of (2) still comes from the static nature of treating all traffic according to the same resource management policy, making it difficult to balance tradeoffs among the radio resource usage efficiency and the signaling overhead~\cite{imc.3g}. Our evaluation further confirms this. Approaches (3) and (4) seem attractive in that the resource management can potentially be tailored to different traffic patterns, therefore improving the resource efficiency. But the key challenge is determining \emph{when} to release resources.
Clearly, the best time to do so is when the UE is going to experience a long idle time period otherwise the incurred resource allocation overhead (\ie signaling load) may be prohibitive due to additional radio state promotions.
Therefore, accurate and efficient prediction of the idle time period is a critical prerequisite for dynamic timer schemes (3) and (4).
\fi

%We do compare approach (2) with our proposal, as it is a simple extension to today's setup. Intuitively, reducing $T_{tail}$ saves UE energy, with increased promotion overhead, and we try to reduce this overhead for the same amount of energy saving.

%Intuitively, adapting the $T_{tail}$ during run time to application and user behavior is helpful to achieve the optimal tradeoffs. Therefore, we believe it is critical to comprehensively investigate approaches (3) and (4), where either the UE or the network actively determines radio resource deallocation.

%For either approach to work properly, it is important to accurately predict the inter-burst time (\IBT), which is the idle time between consecutive traffic bursts. Intuitively, it is preferable to deallocate radio resource right after a burst that is followed by a long \IBT.
This paper proposes \NAMEFULL (\NAME), a practical system that makes dynamic decisions to deallocate radio resources based on accurate and efficient prediction of network idle times. It makes the following contributions.

%\comment{Add pointers: for each contribution, refer to a later subsection.}


%long enough so that exchanging between resource saving and incurred signaling load is worthwhile.

First, \NAME utilizes standard online machine learning (ML) algorithms to accurately predict the network idle time, and performs resource deallocation only when the idle time is sufficiently long. We explored various ML algorithms and prediction models with tunable parameters, with the main contribution of finding robust and easy-to-measure features (not relying on packet payload), whose complex interaction with the network idle time can be automatically discovered by the ML algorithms. The model is validated using seven-month-long traces collected from real users (\S\ref{sec:evaluation}).
%We observe that small bursts dominate all traffic bursts and most bursts occur when users are idle.

Second, to reduce the runtime overhead, \NAME strategically performs \emph{binary} prediction (\ie whether the idle time is short or long) at the granularity of a traffic burst consisting of a packet train sent or received in a batch. Compared to fine-grained prediction of the precise value of packet inter-arrival time, our proposed approach is much more efficient while yielding similar optimization results.
%\comment{or even better results because binary prediction for \IBT is more robust? If so, mention it here and in Sec 3 as well}
We demonstrate that \NAME is practically deployable with negligible energy overhead on today's smartphones.
%It could be implemented at either the client or the network side. The network based approach, transparent to UEs, is a novel proposal in this work.

Third, we overcome critical limitations of previously proposed approaches, \ie RadioJockey~\cite{radiojockey} and MakeIdle/MakeActive~\cite{makeidle} are only applicable to background applications without user interaction, with 
the ideal usage scenario of RadioJockey for a single application only. With multiple concurrent applications, it suffers from low prediction accuracy with increased overhead. By design, \NAME applies to both foreground and background traffic, maximizing energy savings. Since its prediction is based on the aggregate traffic of all applications, \NAME incurs no additional performance degradation nor overhead for supporting concurrent apps.


Fourth, we evaluate \NAME using real-world smartphone traces. The overall prediction accuracy is 85.88\% for all traffic. \NAME achieves radio energy saving by 59.07\%, at the cost of 91.01\% additional signaling overhead in LTE networks, significantly outperforming previous proposals. In order to achieve such energy saving (59\%), the additional signaling overheads incurred by MakeIdle and na\"{\i}ve fast dormancy~\cite{fast.dormancy.1, fast.dormancy.2} are as high as 305\% and 215\%, respectively. The maximal energy saving achieved by RadioJockey is 27\% since it is only applicable to background traffic.


In this chapter, we present the design of \NAME in Section~\ref{sec:overview}. Then we explore feature selection in Section~\ref{sec:stats}. With the methodology discussed in Section~\ref{sec:method}, we evaluate the performance of \NAME in Section~\ref{sec:evaluation}, before summarizing in Section~\ref{sec:rp.summary}.



\nsection{The Design of the \NAMEFULL System}
\label{sec:overview}

As described in Chapter~\ref{chap:bkg}, the static tail times are the root cause of low resource efficiency in cellular networks. Previous work points out that around 50\% energy is incurred by the long high-power RRC tail for 3G~\cite{imc.3g} and 4G networks~\cite{huang_mobisys12}. Our proposed \NAMEFULL (\NAME) system leverages the fast dormancy feature to \emph{dynamically and intelligently determine when to release radio resources}.

\textbf{Challenge 1: the tradeoff between resource saving and signaling load.}
Clearly, the best time to perform resource deallocation is when the UE is going to experience a long idle time period $t$. Specifically, if $t$ is longer than the tail time, deallocating resources immediately after a data transfer saves resources without any penalty of signaling load (\ie state promotions). If $t$ is shorter than the tail time, performing deallocation ahead of time trades off resource saving with signaling load, because doing so will incur an additional state promotion. Such a critical tradeoff presents the key challenge of \emph{predicting the idle time} between data transfers so that fast dormancy is only invoked when the idle time is sufficiently long.

\textbf{Challenge 2: the tradeoff between prediction accuracy and system performance.} \NAME is a service running on the UE with limited computational capabilities and even more importantly, limited battery life, so we need to minimize the runtime overhead without sacrificing much of the prediction accuracy. Otherwise the resource utilized by \NAME itself may overpower its benefits.

\textbf{Challenge 3: the requirement to handle both foreground traffic and background traffic.} Idle time prediction is particularly difficult for applications involving user interactions. Previous systems, such as RadioJockey~\cite{radiojockey} and MakeActive~\cite{makeidle}, simply avoid this by only handling traffic generated by applications running in the background. To maximize the energy saving, the proposed system should be able to handle foreground traffic well, in addition to background traffic.

To address {\bf Challenge 1}, we established a novel machine-learning-based framework for idle time prediction. 
We explored a wide range of ML algorithms with tunable parameters, and evaluated their effectiveness and efficiency. More importantly, our primary focus is addressing the hard problem of selecting discriminating features that are relevant to our specific problem. We find that strategically using a few simple features (\eg packet direction, size, and application name, \etc) leads to a high prediction accuracy of 85.9\%.

%Various machine learning (ML) approaches and their applications have been extensively investigated in the past 20 years. It is known to be an effective approach for many applications of computer networking such as network traffic classification~\cite{jin11} and network troubleshooting~\cite{jin10}.

To address {\bf Challenge 2}, we perform \emph{binary} prediction at the granularity of a traffic \emph{burst} consisting of a train of packets sent or received in a batch. In other words, we find that the knowledge of whether the inter-burst time (\IBT) is short or long (determined by a threshold) is
already accurate enough for guiding the resource deallocation.
Such an approach is much more efficient while yielding similar optimization results compared to more fine-grained and more expensive prediction for the precise value of packet inter-arrival times.

Further, by design, since the prediction of \NAME is based on the aggregate traffic of all applications, it incurs no additional performance degradation nor overhead for supporting concurrent apps. In contrast, previous systems such as RadioJockey have the ideal usage case for a single app. 

To address {\bf Challenge 3}, we choose robust features for idle time prediction for all traffic. We also customize the prediction settings for different types of traffic, either background traffic without user interaction or foreground traffic likely to be triggered by users. We use smartphone screen status as a hint to indicate whether a user is interacting with the device, \ie screen-on traffic belongs to foreground applications. We then apply different settings for prediction based on the screen status and evaluate \NAME with all traffic.

\begin{figure}[t]
\centering
\IG{figures/rp/flow.eps} \\
\ncaption{Working flow of \NAMEFULL (\NAME)}
\label{fig:flow}
\end{figure}

We illustrate the design of  \NAMEFULL (\NAME) in Figure~\ref{fig:flow}. In summary, a data collector collects packet traces (only headers are required) and some other auxiliary information, \eg process association for all packets. The data collected is fed into the \IBT prediction framework, which trains models to predict the \IBT for the current burst. Then, based on the prediction result, the fast dormancy scheduler makes decision on whether to invoke fast dormancy.

For \IBT prediction, we use the traffic model as defined in Section~\ref{sec:burst.model}. We also use a short \IBT threshold \SBT to classify an \IBT, \ie if \IBT $\leq$ \SBT, it is {\em short}, otherwise, it is {\em long}.

%Figure~\ref{fig:flow} presents the working flow of \NAME. The core component is an \IBT prediction framework.
The \IBT prediction framework trains the prediction model based on historical traffic information collected by the data collector. The information collected is an array of bursts $\{B_{1}, \cdots, B_{m}\}$. Each $B_{i}$ is a vector

\begin{displaymath}
B_{i}:\quad <f_{1}, f_{2}, \cdots, f_{t}, ibt_{i}>
\end{displaymath}

where $\{f_{1}, \cdots, f_{t}\}$ is the list of features of $B_{i}$ and $ibt_{i}$ is the \IBT following burst $B_{i}$.
%The trained prediction model may or may not be updated periodically, depending on the design choice.
%The prediction framework collects real-time information and updates the prediction model periodically based on the recent history of burst information. The update frequency is adjustable.
With the prediction model,  \NAME monitors the UE traffic in real-time and whenever there is an idle time of \BT, \ie the last packet's arrival time was \BT time ago, the prediction process starts. The feature vector of the current burst $\{f_{1}, \cdots, f_{t}\}$ is generated and fed to the prediction framework, which predicts whether the \IBT following the current burst is short or long. If short, as shown in Figure~\ref{fig:flow}, no change is made and UE stays in the tail, since the prediction framework suggests that a packet would appear soon. Otherwise, the prediction framework triggers fast dormancy to save energy.


\nsection{Feature Selection}
\label{sec:stats}

In this section, we study the traffic burst characteristics in the \UMICH data set described in Section~\ref{sec:data.umich}. Then we look at a few burst features for \IBT prediction.

\nsubsection{Characteristics of Bursts}
For burst characterization, we first need to pick a value for \BT. Obviously, \BT should be larger than typical RTTs in cellular networks, which is around hundreds of milliseconds~\cite{mobisys.3gtest, huang_mobisys12}, so as not to break a single continuous TCP flow into separate bursts. In the meanwhile, \BT should not be set to be too large; otherwise, there could be a large gap inside a burst, during which, fast dormancy could have been triggered to save energy. We start our analysis with \BT $= 1s$, resulting in 5.44 million bursts in total, and later we would explore how different \BT values affect our results.

\begin{figure}[t]
\centering
\IGML{figures/rp/cdf_ibt.eps} \\
\ncaption{CDF of IBT for sampled/all users}
\label{fig:cdf.ibt}
\end{figure}


By analyzing all 3G traces in the \UMICH data set, we plot the CDF of \IBTS for all users (\IBTS are calculated separately for each user before aggregate analysis) and two sample users in Figure~\ref{fig:cdf.ibt}. The median of \IBT for all users is 4.46 seconds, and 68.24\% of \IBTS are smaller than the tail timer $T_{tail}$ in our studied network.

In Figure~\ref{fig:cdf.ibt}, we also compare the aggregate curve of all users with those of two sample users. The difference for \IBT distribution is mainly due to the user preference over different applications, \eg sample user 1's curve has a big jump around 20 seconds and this is verified to be correlated with the heavy use of the Facebook application as shown in Figure~\ref{fig:appid1}. Therefore, \NAME predicts \IBT using a per-user model instead of a global model for all users. Even for the same user, \ie user 1, at day 1 and day 2, which are months apart, we also observe a clear difference. We observe that user 1 uses a quite different list of applications for these two days, though not completely different. Even for the same application, different versions may also result in traffic pattern changes. So we think it is not practical to build a static model for each user. Instead, \NAME uses a dynamic model based on the most recent traffic history for each user.


\begin{figure}[t]
\centering
\IGML{figures/rp/cdf_screenOn.eps} \\
\ncaption{CDF of IBT for different screen status}
\label{fig:cdf.screen}
\end{figure}
Choosing a proper value for \SBT is challenging, \ie a large \SBT reduces the chance of \NAME for saving energy and a small \SBT would result in excessive signaling load, since \NAME would trigger fast dormancy for long \IBTS. Previous work~\cite{imc.screen} indicates that traffic pattern is different between screen-on state and screen-off state. In Figure~\ref{fig:cdf.screen}, we also observe that screen-off traffic has higher \IBTS than screen-on traffic. The median \IBT for screen-off and screen-on traffic is 5.00s and 3.14s, respectively. This motivates us to select different \SBT values based on the different screen states, \ie we could use an more aggressive (smaller) value of \SBT to trigger more fast dormancy for screen-off traffic.

\begin{figure}[t]
\centering
\IGML{figures/rp/packetNum.eps} \\
\ncaption{Distribution of bursts grouped by \# of packets}
\label{fig:packetNum}
\end{figure}
Figure~\ref{fig:packetNum} shows the distribution of the number of packets of a burst. We observe that the distribution is heavy-tailed. In particular, 27.69\% of all bursts only contain 1 single packet, and 75.00\% of these 1-packet bursts are short bursts. About 52.63\% of all bursts consist of no more than 3 packets and 73.47\% bursts contain no more than 10 packets, indicating that small bursts dominate the user traffic.


\nsubsection{Selecting Features for Burst Classification}

\begin{table}[t]
\begin{center}
\begin{tabular}{|c|c|c|}\hline
Name & Symbol & Description\\\hline
Packet direction & {\em Dir(i)$^{\star}$} & Downlink or uplink \\\hline
Server port number &{\em Port(i)} & The remote port number\\\hline
Total packet length &{\em Len(i)} & Including headers\\\hline
Protocol  & {\em Prot(i)} & Protocol field in the IP header\\\hline
\MR{TCP flags}  & \MR{{\em Flag(i)}} & TCP flags field in  the TCP\\
  &  & header and 0 if not TCP\\\hline
\MR{Application ID} & \MR{{\em AppID(i)}} & The global application ID \\
 &  & which generates this packet\\\hline
\end{tabular}
\begin{tabular}{l}
\\{ $^\star$} {\em Dir(i)} (i = 1, 2 or 3) refers to the $i$-th to last packet,  \eg when i = 1, \\
\ \ \ it refers to the last packet. Similar rule applies to all other features.
\end{tabular}
\ncaption{Features for the last three packets of a burst}
\label{tab:optimize.feature}
\end{center}
\end{table}

To predict whether an \IBT is larger or smaller than \SBT, we naturally choose to look at the features of the burst preceding the \IBT. Based on our observation, the correlation between \IBT and the bursts further beyond the preceding burst is small, hence we do not use those bursts for training features. For the preceding burst, we look at the detailed features of the last three packets. The choice of three packets is because in most cases, bursts are small, and even for large bursts, we can tell the nature of a burst based on the last three packets, \eg TCP three-way handshake for connection establishment and termination. For each of the last three packets, we only look at header information, not the payload, since many of these packet either do not have any payload, or use encryption. The features of the last three packets~\footnote{If a burst contains less than three packets, all features for the missing packet(s) have a value of 0.} used for training are listed in Table~\ref{tab:optimize.feature} and we study some of them as follows. These features are selected empirically that are most relevant to \IBT.


\begin{figure}[t]
\centering
\IGML{figures/rp/cdf_dir1.eps} \\
\ncaption{CDF of \IBT v.s. direction of the last packet}
\label{fig:dir1}
\end{figure}

In Figure~\ref{fig:dir1}, CDF of \IBT for the two possible directions of the last packet is shown. In general, \IBT following a burst whose last packet is downlink is larger than that for uplink, \eg the median \IBT is 6.49 seconds for downlink and 3.79 seconds for uplink.
%Intuitively, this is because when the last packet is uplink, to some extent, it indicates that UE is at a ``pending'' state awaiting for response from the server side and \IBT after such bursts are typically smaller.

\begin{figure}[t]
\centering
\IGML{figures/rp/cdf_port1.eps} \\
\ncaption{CDF of \IBT v.s. port of the last packet}
\label{fig:port1}
\end{figure}
Figure~\ref{fig:port1} shows the CDF of \IBTS of all users for the top 6 ports ordered from top to bottom in the legend, \ie port 80 is the top 1 port and port 993 ranks 6-th. \IBT distribution for different ports have clear differences, especially for port 53. By further investigation on the traffic traces, we know that the sudden jump for port 53 at \IBT $=5$ seconds is related with the DNS request retransmission behavior, \ie when UE fails to get DNS response within 5 seconds, either due to poor signal strength or slow DNS response, it resends the DNS request and the initial DNS request itself forms a burst. We also observe that the jump for port 5222 around 20 seconds corresponds to the periodic keep-alive messages for Facebook app.

\begin{figure}[t]
\centering
\IGML{figures/rp/len1.eps} \\
\ncaption{Distribution of bursts grouped by packet length of the last packet}
\label{fig:len1}
\end{figure}

Figure~\ref{fig:len1} summarizes the number of bursts for different packet length of the last packet. Packet length outside the plotted range has much fewer bursts. As expected, we observe that most bursts ends with small packets, \ie 84.59\% of all bursts have the last packet $\leq$ 100 bytes, since large packet typically indicates being in the middle of data transfer, thus not the end of a burst. For some other values, \eg 121 bytes, 93.04\% bursts are short bursts indicating high correlation. Machine learning algorithms could learn such rules and make prediction. Although there is less correlation between \IBT and packet length for the two major spikes at 40 bytes (71.05\% short bursts) and 52 bytes (70.00\% short bursts), machine learning algorithms would consider other features for prediction automatically for higher accuracy.

\begin{figure}[t]
\centering
\IGML{figures/rp/flags1.eps} \\
\ncaption{Distribution of \IBT grouped by the type of the last packet}
\label{fig:flag1.bar}
\end{figure}

In Figure~\ref{fig:flag1.bar}, the number of bursts for different {\em Flag(1)} is presented, sorted in a descending order from left to right. Specifically, for {\em Flag(1) == 0}, such bursts fall into two groups, UDP packets and ICMP packets. Besides UDP packets, we also observe strong correlation between {\em Flag(1)} and short \IBTS for SYN and PUSH-ACK packets, with 90.44\% and 90.72\% confidence, respectively.


\begin{figure}[t]
\centering
\IGML{figures/rp/cdf_appid1.eps} \\
\ncaption{CDF of \IBT v.s. the application generating the last packet}
\label{fig:appid1}
\end{figure}
The last feature we look at for each packet is the application ID. Notice that this feature is not directly obtainable from packet traces and only available at the client side~\footnote{At the network side, using the \textbf{User-Agent} field in each TCP flow, the application name may be inferred, however, it is not 100\% accurate and for non-HTTP traffic, no such information is available.}. In Figure~\ref{fig:appid1}, the legend shows the sorted top list of applications contributing to most bursts with Facebook as the top 1 app. The application-wise differences of \IBT is obvious which necessities the choice of this feature. We also observe that for some applications, periodic transfer behavior contributes to clear jumps in \IBT distribution, \eg Faceboook and LiveProfile.




\nsection{Evaluation Methodology}
\label{sec:method}

We present the methodology for evaluating \NAME.

\nsubsection{Evaluation Metrics}


Given that 3G and 4G have similar tail mechanisms, we select the 4G LTE network as a representative and consistently use the same terms described below. Specifically, assuming a UE is used for some period $t$. We define the total radio energy usage of the UE in $t$ to be $E$. Notice that $E$ is one of the most significant components for UE's total energy usage, along with screen energy and CPU energy, \etc~\cite{mobisys.aro}. We then define the total duration of UE being in \RC to be $R$, to quantify the channel occupation time, since radio resource is only allocated to UE in \RC in the LTE network. $E$ and $R$ are approximately proportional, because the power level in \RC is much higher than that in \RI, and the radio energy $E$ energy is dominated by that in \RC: $E \approx PR$, where $P$ is the average power level in \RC, with small variation in most cases. For the rest of the paper, we only study $E$, and assume the channel occupation time $R$ is proportional to $E$.

To quantify the state promotion overhead, we define signaling overhead $S$ as the number of state promotions, since one promotion corresponds to fixed number of signaling messages~\cite{radiojockey}. To save the radio energy $E$, UE would spend less time in \RC and hence there is a higher probability for RRC state promotions, resulting in increased $S$. The design goal of our system is to reduce $E$, while minimizing the increase of $S$. A state promotion also incurs a fixed delay ($T_{pro}$). Since the total of such delays is proportional to $S$, we do not use a separate metric to quantify the introduced delay by promotions. In addition, a state promotion requires fixed energy ($T_{pro}P_{pro}$), which is taken into consideration in calculating $E$.

Assume a specific user trace is fed to the LTE network and power model simulator with all default settings, the values for $E$ and $S$ are $E_{d}$ and $S_{d}$, respectively (the subscript $_{d}$ stands for \textbf{d}efault). Then assume after the energy saving scheme is applied and the resulting $E$, $S$ values are $E'$, $S'$. We define $\Delta(T) = (T' - T_{d}) / T_{d}$, where $T = E, S$. $\Delta(S)$ is positive, while $\Delta(E)$ is always negative. For simplicity, we redefine $\Delta(E) = |E' - E_{d}| / E_{d}$, making it positive, representing the ratio of energy saved. In sum, the goal is to maximize $\Delta(E)$ while minimizing $\Delta(S)$.


%\nsubsection{Trace-driven Analysis}

%We first discuss the \UMICH data set used throughout this study, followed by the burst analysis, network modeling and power modeling methodology.

\nsubsection{RRC and Radio Power Simulator}
\label{sec:model}
We build a 4G LTE RRC state machine simulator with the LTE power model for the trace-driven analysis.

\paragraph{The LTE state machine simulator} takes as input the 3G packet trace. It first preprocesses the trace using the methodology introduced in previous work~\cite{imc.3g} by removing existing state promotion delays. The purpose is to decouple promotion delays introduced by the existing 3G state machine from the actual traffic patterns. Therefore the 3G trace can be simulated in a different (4G LTE) network. Subsequently, the simulator injects new promotion delays into the trace, then infers the RRC states experienced by the UE based on the timing, size, and direction of each packet, according to the 4G LTE state transition model depicted in Figure~\ref{fig:rrc}(b). The output of network model simulator is an array of packets with adjusted timestamps, as well as the RRC states of the UE at any given time.

\paragraph{The power model simulator} takes as input the output of the state machine simulator and calculates the energy consumed by the 4G radio interface. The total energy consumption consists of four components: idle, promotion, tail, data transfer. In idle mode, the radio power consumption is very low (almost zero). For promotion energy, both the promotion delay and the average promotion radio power are fixed. The radio power consumed on the tail is also fixed, based on our measurement using a real LTE phone (detailed below).
%For data transfer energy, we empirically observe that the power consumption has a strong linear correlation with the throughput. Specifically, assume uplink throughput is $t_u$ (Mbps) and downlink throughput is $t_d$ (Mbps), the instant power level (mW) of the UE is $P = \alpha_u t_u + \alpha_d t_d+ \beta$.


\begin{table}[t]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}\hline
            & Power$^{a}$ (mW)        & Duration (ms) \\
\hline
Screen off       & $P_{base}$: 11.4$\pm$0.4      & N/A       \\
Idle average$^{b}$   & $P_{idle}$: 31.1$\pm$0.4      & N/A       \\
Promotion   & $P_{pro}$: 1210.7$\pm$85.6   & $T_{pro}$: 260.1$\pm$15.8 \\
Tail average$^{b}$        & $P_{tail}$: 1075.5$\pm$3.3    & $T_{tail}$: 11576.0$\pm$26.1 \\
\hline
\end{tabular}
\begin{tabular}{l}
\\{$^a$} All power values include the base power with screen off.
\\{$^b$} The average power is measured with DRX considered.
\end{tabular}
\ncaption{The LTE power model parameters measured from a real cellular ISP}
%\comment{not considering throughput energy in this work, for simplicity, we do not consider the impact of throughput on power in this work, however, we observe that the throughput for most applications are small and the impact on the results is minor}
\label{tab:lte_para}
\end{center}
\end{table}

\paragraph{The parameters of the model} are measured by us using an HTC ThunderBolt smartphone with an LTE data plan of a large cellular ISP in the U.S. The phone has 768MB RAM memory and 1 GHz Qualcomm MSM8655 CPU, running Android 2.2.1. We use the Monsoon power monitor~\cite{monsoon} to measure the UE power consumption.
%Then radio power was be approximated by subtracting the constant power baseline (11.4 mW) at the IDLE state from the overall UE power consumption reported by the power monitor.
The measured parameters are summarized in Table~\ref{tab:lte_para}. We validate the LTE state machine and the power model simulator by comparing the radio energy measured by power monitor with simulated radio energy for four popular Android applications. The error rate is less than 6\%.

\nsubsection{Prediction Framework Evaluation Methodology}
\label{sec:method.eval}


To evaluate the performance of the prediction framework, we need to calculate the prediction accuracy and the resulting $\Delta(E)$ and $\Delta(S)$.

It is straightforward to calculate the prediction accuracy. Assume $n$ bursts are predicted and $n_{s}$ of them are followed by short \IBTS and $n_{l}$ are followed by long \IBTS ($n = n_{s} + n_{l}$). Then assume the prediction framework correctly predicts $n_{1}$ out of the $n_{s}$ \IBTS to be short and $n_{2}$ out of the $n_{l}$ \IBTS to be long, the prediction accuracy is $(n_{1} + n_{2}) / n$.

For calculating $\Delta(E)$ and $\Delta(S)$ , if an \IBT is predicted to be short, the prediction framework would have no impact since it keeps UE stay in the high power tail and there is no change to the RRC state machine. When an \IBT is predicted to be long, the prediction framework would make UE demote to \RI to save energy.

\begin{algorithm}[t]
\begin{algorithmic}
\STATE $P_{tail} \leftarrow $ power level at \RC tail
\STATE $P_{idle} \leftarrow $ power level at \RI state
\STATE $P_{pro} \leftarrow $ power level during promotion
\STATE $T_{pro} \leftarrow $ promotion delay
\IF{$ibt_{i}$ is predicted to be $>$ \SBT}
\IF{$ibt_{i} \leq T_{tail}$}
\STATE //Demote to \RI at time \BT $+$ $t_{0}$, save energy
\STATE $\Delta(E) $ += $(P_{tail} - P_{idle}) (ibt_{i} - \BT) / E_{D}$
\STATE //Extra idle to active promotion overhead
\STATE $\Delta(E) $ $-$= $P_{pro} T_{pro} / E_{D}$
\STATE $\Delta(S) $ += $1 / S_{D}$
%\STATE $\Delta(D) $ += $T_{pro} / D_{D}$
\ELSE
\STATE //Demote to \RI at time \BT$+$ $t_{0}$, save energy
\STATE $\Delta(E) $ += $(P_{tail} - P_{idle}) (T_{tail} - \BT) / E_{D}$
\STATE //No extra promotion overhead
\ENDIF
\ELSE
\STATE // $ibt_{i}$ predicted to be short, remain in the tail, no impact
\ENDIF
\end{algorithmic}
\ncaption{Update $\Delta(E), \Delta(S)$ for burst $B_{i}$}
\label{algo:update}
\end{algorithm}



Assume the current burst $B_{i}$ ends at time $t_{0}$ and the next burst $B_{i+1}$ comes at time $t_{1}$. At time $t_{0}+$\BT, when the existence of $B_{i}$ is detected by the prediction framework, it collects information about $B_{i}$ and predicts the \IBT following it, denoted as $ibt_{i}$, whose actual value is $t_{1}-t_{0}$. We also assume that the short burst threshold \SBT $\leq T_{tail}$, the tail timer. Otherwise, if \SBT $> T_{tail}$, the energy saving opportunity would be much less, since when an \IBT is predicted to be short, it is still possible that \IBT $> T_{tail}$, and it is an indication that the tail could be ended earlier to further save energy. Based on these definitions, we provide the detailed pseudocode of updating $\Delta(E)$ and $\Delta(S)$ for burst $B_{i}$ in Algorithm~\ref{algo:update}. If $ibt_{i} > T_{tail}$ and $ibt_{i}$ is predicted to be long, for the original state machine, the arrival of $B_{i+1}$ already triggers a promotion, so the prediction framework does not incur any extra promotion overhead. Otherwise, if $ibt_{i} \leq T_{tail}$, an extra promotion would occur.


For using machine learning algorithms, we use MATLAB R2011b (7.13.0.564) running on a desktop with Intel Xeon CPU (3.16GHz) and 16GB memory, running Linux 2.6.38. With the same C++ benchmark programs, we observe that the running time on the studied Android device is around 4 times of the desktop. We use this ratio as a simple heuristic to estimate the training time and prediction time of different algorithms. To validate this heuristic, we compare the running time of SVM algorithm with MATLAB on the desktop and with OpenCV~\cite{opencv} and on Android on the same input, the performance ratio is also around 4. For simplicity, in the rest of our study, we rely on MATLAB and its implementation of different machine learning algorithms.



\nsection{Evaluation Results}
\label{sec:evaluation}

In this section, we evaluate different prediction models for \IBT prediction using various machine learning algorithms, and analyze the impact on energy saving and signaling overhead. We then discuss the practicality for real deployment on either client side or network side.
%followed by a case study our framework to a one-application scenario.

\nsubsection{Accuracy v.s. Performance Metrics}

%Talk about the impact of prediction, relation between $\Delta(E, S, D)$ and prediction accuracy.
We start by discussing the impact of \IBT prediction accuracy on the energy saving $\Delta(E)$ and signaling load change $\Delta(S)$ when \NAME is deployed.


\begin{figure}[h]
\centering
\IG{figures/rp/accuracy.eps} \\
\ncaption{$\Delta(E), \Delta(S)$ v.s. prediction accuracy}
\label{fig:accuracy}
\end{figure}


Given a sequence of bursts $B_{1}, B_{2}, \cdots, B_{n}$, let the \IBT after $B_{i}$ be $t_{i}$. Ideally, the prediction framework could accurately predict whether $t_{i} \leq $ \SBT. We calculate $\Delta(E)$ and $\Delta(S)$ as described in \S\ref{sec:method.eval} by varying the binary prediction accuracy. Figure~\ref{fig:accuracy} shows that $\Delta(E)$ and $\Delta(S)$ have a linear pattern as accuracy changes. When the accuracy is 0, namely all \IBTS are predicted to be wrong, there is little energy saving of 2.60\% with $\Delta(S) = 129.52\%$. The energy saving is so small because a large number of promotions also incurs significant promotion energy.
% This is because when an actual short \IBT is predicted to be long, \NAME would demotes UE immediately. Though incurring extra promotion overhead, the total energy is still reduced.
As accuracy increases, \NAME saves more energy with reduced $\Delta(S)$. In the optimal case, when the accuracy achieves 100\%, 59.80\% of the total energy could be saved while the signaling overhead increases by 85.37\%. The signaling overhead is not 0, because for \IBTS smaller than $T_{tail}$ but larger than \SBT, even if the prediction is correct, triggering fast dormancy would still incur an extra state promotion. Such overhead is inherent for any optimization technique, such as a smaller static $T_{tail}$ setting and RadioJockey. The design goal of \NAME is to approach this optimal case.

\nsubsection{Prediction Model Comparison}

\begin{table}[t]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}\hline
$\alpha=$ & 100 & 500 & 1000 & 2000 & 5000\\\hline
$\beta=1$ & 81.52\% & 83.73\% & \textbf{84.15\%} & 82.4\% & 80.15\%\\\hline
$\beta=2$ & 80.05\% & 81.37\% & 82.87\% & 81.95\% & 80.00\%\\\hline
$\beta=5$ & 79.75\% & 80.91\% & 81.37\% & 81.03\% & 79.31\%\\\hline
$\beta=10$ & 79.40\% & 80.01\% & 80.89\% & 80.01\% & 78.98\%\\\hline
$\beta=20$ & 78.94\% & 79.62\% & 80.17\% & 79.54\% & 78.66\%\\\hline
\end{tabular}
\ncaption{Tuning $\alpha$, $\beta$ for \MostRecent prediction model}
\label{tab:alphabeta}
\end{center}
\end{table}

For \IBT prediction, historical \IBT information is needed to train a prediction model. For \NAME, we propose to use the most recent historical information to train a model for each user, denoted as \MostRecent. Specifically, for each user, the most recent $\alpha$ bursts are used to predict the next $\beta$ bursts. We study the impact of changing $\alpha, \beta$ in Table~\ref{tab:alphabeta}, using the Ensemble {\em bagging}~\cite{ensemble_bagging} learning algorithm as an example (number of trees set to 20). We calculate the prediction accuracy for different ($\alpha$, $\beta$) settings. We observe that if $\alpha$ is too small, there is not enough history information for learning; in the opposite, if $\alpha$ is too large, the user is more likely to switch to new applications so old rules do not apply. Based on Table~\ref{tab:alphabeta}, we choose $\alpha = 1000$ and $\beta = 1$ that maximize the accuracy in the experimental measurement. In practice, $\alpha$ and $\beta$ could also be dynamically adjusted based on prediction history.

\begin{table}[t]
\begin{center}
\begin{tabular}{|c|c|c|}\hline
Name & Description & Accuracy\\\hline
\MR{\MostRecent} & Use the most recent $\alpha$ bursts to &  \MR{84.15\%} \\
& predict the next $\beta$ bursts for each user & \\\hline
\MR{\PerUser} & Use $n$ historical bursts of a user to & \MR{80.78\%} \\
& train a fixed model for this user & \\\hline
\MR{\AllUsers} & Use $k$ historical bursts of all users & \MR{77.48\%} \\
& train a fixed model for all users & \\\hline
\end{tabular}
\ncaption{Summary of prediction models}
\label{tab:model}
\end{center}
\end{table}

In Table~\ref{tab:model}, we compare the \MostRecent model with two other models, \PerUser and \AllUsers. For fair comparison, we use the same machine learning algorithm (Ensemble \emph{bagging}) as in Table~\ref{tab:alphabeta}. We set $\alpha = 1000$ and $\beta = 1$ for the \MostRecent model as discussed previously. Similarly, we set $n = 10,000$ for \PerUser and $k = 10,000$ for \AllUsers ($n$ and $k$ are defined in Table~\ref{tab:model}). The values of $n$ and $k$ here are empirically selected, which yield good prediction accuracy for each prediction model. We observe that \MostRecent has higher prediction accuracy than the other two modes, and \PerUser is more accurate than \AllUsers. This suggests that it is necessary to have a separate model based on the most recent history for each individual user.

\nsubsection{Comparing Resource Optimization Approaches}

\begin{table*}[t]
\begin{center}
\scriptsize
\begin{tabular}{|c|c|c|c|c|c|c|}\hline
\MR{Name} & \MR{Description \& Configuration} & Training  & Prediction & \MR{Accuracy} & \MR{$\Delta(E)$} & \MR{$\Delta(S)$}\\
& &  time$^a$ (ms) &  time$^b$ (ms) &  &  & \\\hline
Fast dormancy 1s & Invoke fast dormancy after 1s idle time  & 0 & 0 & NA & 62.74\% & 214.89\% \\\hline
Fast dormancy 3s & Invoke fast dormancy after 3s idle time  & 0 & 0 & NA & 40.85\% & 85.36\% \\\hline

{\bf RadioJockey} & RadioJockey applied to & \MR{NA} & \MR{NA} & \MR{NA}  & \MR{30.08\%} & \MR{51.69\%}  \\
Assume 100\% accurate &  background traffic$^c$ &  &  &  &  &  \\\hline
{\bf RadioJockey} & RadioJockey applied to & \MR{NA} & \MR{NA} & \MR{NA}  & \MR{27.19\%} & \MR{52.00\%}  \\
Assume 90\% accurate &  background traffic &  &  &  &  &  \\\hline

{\bf MakeIdle} & MakeIdle: based on previous M packets, & \MR{NA} & \MR{NA} & \MR{NA}  & \MR{64.92\%} & \MR{305.20\%} \\
M:1000, N:100 &  predict next N packets &  &  &  &  &   \\\hline
{\bf MakeIdle} &  MakeIdle: based on previous M packets,  & \MR{NA} & \MR{NA} & \MR{NA}  & \MR{44.93\%} & \MR{195.2\%} \\
M:10, N:10 &   predict next N packets &  &  &  &  &  \\\hline
%\NAME: All Short & Predict all \IBTS to be short & 0 & 0 & 41.13\% & 0.00\% & 0.00\% & 0.00\%\\\hline
%\MR{\NAME: 50\% Random} & Predict an \IBT to be short & \MR{0} & \MR{0} & \MR{50.00\%} & \MR{31.19\%} & \MR{107.44\%} & \MR{50.00\%}\\
% & with a probability of 50\% & & & & & & \\\hline
%\NAME: All Long & Predict all \IBTS to be long & 0 & 0 & 58.87\% & 62.74\% & 214.89\% & 100.00\%\\\hline
\MR{\NAME: \NB} & \NB classification with {\em mvmn}:  & \MR{25.68} & \MR{9.84} & \MR{72.89\%} & \MR{48.03\%} & \MR{102.81\%} \\
 & multivariate multinomial distribution  & & & & & \\\hline
\MR{\NAME: Discriminant} & Discriminant analysis with  & \MR{151.36} & \MR{27.28} & \MR{75.40\%} & \MR{54.72\%} & \MR{118.93\%} \\
 & discriminant type as {\em pseudoLinear} & & & & &\\\hline
\NAME: Regression Tree & Binary decision tree for regression & 490.84 & 2.60 & 78.42\% & 57.75\% & 139.56\% \\\hline
\NAME: Classification Tree & Binary decision tree for classification & 547.72 & 23.68 & 82.65\%  & 52.10\% & 101.64\% \\\hline
\NAME: SVM & SVM with {\em Multilayer Perceptron} kernel & 591.2 & 1.64 & 66.38\% & 45.03\% & 141.17\% \\\hline
\NAME: Ensemble & Method: {\em RobustBoost}; Error goal: 0.15 & \MR{2534.16} & \MR{412.8} & \MR{81.91\%} & \MR{56.30\%} & \MR{106.32\%} \\
(RobustBoost) & weak leaner: decision tree; \# of trees: 20 &  &  &  &  & \\\hline
\MR{\NAME: Ensemble (Bag)} & Method: {\em Bag}; type: classification & \MR{2504.2} & \MR{426.20} & \MR{84.15\%}  & \MR{56.55\%} & \MR{92.19\%} \\
 & weak leaner: decision tree; \# of trees: 20  &  &  &  &  &  \\\hline
\NAME: Optimal & Predict all IBTs correctly  & 0 & 0 & 100.00\% & 59.80\% & 85.36\% \\
\hline
\end{tabular}
\begin{tabular}{l}
\\{\small $^\star$} All evaluations in this table for \NAME are using {\em MostRecent} prediction model with $\alpha=1000$ and $\beta=1$.
\\{\small $^a$} Training time refers to the median time to train one model with $\alpha=1000$ on the studied Android device.
\\{\small $^b$} Prediction time refers to the time to predict one burst on the studied Android device. The median value is listed.
%\\{\small $^c$} $\Delta(E)/\Delta(S)$ shows the average energy saving per unit signaling overhead. The higher this metric, the better.
\\{\small $^c$} We use smartphone screen status to distinguish between foreground and background traffic.
\end{tabular}
\ncaption{Comparison of different optimization algorithms}
\label{tab:ml}
\end{center}
\end{table*}


In Table~\ref{tab:ml}, we compare various radio resource optimization techniques, including the basic fast dormancy with static timers, RadioJockey~\cite{radiojockey}, MakeIdle~\cite{makeidle} and \NAME using different machine learning algorithms. Using the network and power model simulators (\S\ref{sec:model}), we apply the above optimization techniques to the \UMICH data set. We ignore the overhead of invoking fast dormancy, since invoking fast dormancy from the client only requires sending one special message called {\em Signaling Connection Release Indication} (SCRI). In contrast, a state promotion from the idle state to the connected state involves 12 or 30 signaling messages~\cite{radiojockey}.

\paragraph{Basic fast dormancy}: we simply change $T_{tail}$ to be a constant smaller than its original value.

\paragraph{RadioJockey}~\cite{radiojockey}: it uses system call traces to predict the {\em end-of-session} (EOS) for background application with no user interaction, with the ideal usage scenario for a single application. For multiple concurrent applications, it invokes fast dormancy only if {\em i)} EOS of one application is predicted, and {\em ii)} there is no active session of any application. RadioJockey does not handle foreground traffic, because user interactions may violate the correlation between system calls and EOS~\cite{radiojockey}. Given that we do not have system call traces in the \UMICH data set, we assume that RadioJockey could predict the EOS for {\bf each} of the applications, even when multiple applications are running concurrently. This is a strong assumption, because the authors admit that the accuracy for prediction would be lower for concurrent applications, and RadioJockey does not use real traces to evaluate concurrent applications.

\paragraph{MakeIdle}~\cite{makeidle}: it relies purely on packet timing without considering signaling overhead, \ie it tries to find a wait time $T_{wait}$ which maximizes the energy saving if $T_{tail}$ is set to be $T_{wait}$ for the previous $M$ packets, it then applies this $T_{wait}$ for the next $N$ packets. The range we search for an optimal $T_{wait}$ is $[0.5, 11.5]$, as suggested by the author.  No recommendations have been made for the values of $M$ and $N$, so we empirically select their values. MakeActive, proposed by the same set of authors tries to reduce the signaling overhead of MakeIdle. However, MakeActive is not a perfect fix without causing bad user experiences, especially for foreground traffic.

\paragraph{\NAMEFULL}: we explore various machine learning algorithms with the \MostRecent prediction model ($\alpha=1000$ and $\beta=1$).
%The features used for machine learning are listed in Table~\ref{tab:featureSum}. To validate our approach that groups all categories with less than 0.1\% of total bursts for each feature, we use \NB algorithm to compare the prediction accuracy. We observe that the accuracy improves from 68.99\% to 73.20\% after grouping for the features, and this is mainly due to the reduced over fitting problem.
%To validate our random sampling evaluation method, we compare the performance of \NB algorithm at different sampling rate in Table~\ref{tab:sample}. The smallest sampling rate 0.1\% results in largest error for $\Delta(E)$, $\Delta(S)$ and $\Delta(D)$, all smaller than 4\%. This demonstrates the effectiveness of the random sampling approach. We use 0.5\% sampling rate for evaluation experiments for the rest of the paper.
We explore both parametric learning and nonparametric learning algorithms. For parametric learning, we look at \NB classification and discriminant analysis. While for nonparametric learning, we compare decision tree, including regression tree and classification tree, SVM and Ensemble algorithms. All of them are off-the-shelf machine learning algorithms used widely.

In Table~\ref{tab:ml}, we observe that ``fast dormancy 1s'' is an aggressive approach, saving 62.74\% of energy while incurring 214.89\% of signaling overhead. ``Fast dormancy 3s'' significantly reduces $\Delta(S)$ to 85.36\%, with less energy saving of 40.85\% as expected. For RadioJockey, by assuming the accuracy for predicting EOS for each background application to be 90\%, it saves 27.19\% of energy with 52.00\% of $\Delta(S)$.
There is slight improvement when the accuracy increases to 100\%. However, we are not able to evaluate RadioJockey for foreground traffic on which the prediction accuracy is unknown. For MakeIdle, we pick two sample ($M, N$) settings. For ($1000, 100$), the energy saving is significant, but the additionally incurred signaling overhead has an unacceptable value of 305.20\%. Even for ($10, 10$), $\Delta(S)$ is as high as 195.2\%. This is because MakeIdle does not consider balancing between energy saving and signaling overhead~\footnote{Another orthogonal approach proposed by~\cite{makeidle} to reduce the signaling overhead (called MakeActive) is not applicable, as it requires traffic shifting that could result in unacceptably negative impact on user experience for foreground traffic.}.
%Further investigation reveals that since MakeIdle purely relies on the packet timing as a feature for prediction without considering the traffic context, the chosen $T_{wait}$ is often suboptimal, \eg $T_{wait}$ is selected to be 0.5s

For \NAME, in the optimal case assuming there is an oracle with complete knowledge of traffic, it saves 59.80\% of energy with 85.36\% of extra signaling overhead incurred. Note that this signaling overhead depends on the choice of \SBT and is inherent for any fast dormancy based optimization. For example, if there are two packets $P_{1}$ and $P_{2}$, such that $P_{2}$ arrives 11.5s after $P_{1}$ and there is no other packet in between, in the default setting where $T_{tail} = 11.6s$ (Figure~\ref{fig:rrc}), there is no state promotion between $P_{1}$ and $P_{2}$. However, if we want to eliminate the long radio-on period after $P_{1}$, we have to invoke fast dormancy at some time between $P_{1}$ and $P_{2}$, and then the extra state promotion at $P_{2}$ is inevitable. Among all explored machine learning algorithms, {\em ensemble (bag)} performs best, achieving 84.15\% of accuracy, 56.55\% of $\Delta(E)$ and 92.19\% of $\Delta(S)$.

For the metric $\Delta(E)/\Delta(S)$, which quantifies how well the tradeoff between saving resources and incurring signaling load is handled, \NAME outperforms other algorithms. RadioJockey has comparable performance for background traffic, however, \NAME has good results for both background and foreground traffic from concurrent applications.

\nsubsection{Selecting Burst Thresholds}

In the previous subsections, we only examine one setting of \BT and \SBT, \ie \BT = 1s and \SBT = 3s. Here we study how different settings of these parameters affect the performance of \NAME. In the top half of Table~\ref{tab:param}, we select some combinations of \BT and \SBT values and compare them with the setting of $S_{0}$. We notice that $S_{1}$ sets \SBT to be 2s, which significantly increases $\Delta(S)$. Among all settings explored, $S_{4}$ yields the highest $\Delta(E)/\Delta(S)$.

\begin{table}[t]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}\hline
Settings (unit: sec) & Accuracy & $\Delta(E)$ & $\Delta(S)$ & $\frac{\Delta(E)}{\Delta(S)}$\\\hline
$S_{0}$ \BT: 1 \SBT: 3 & 82.65\%  & 52.10\% & 101.64\%  & 0.513\\\hline
$S_{1}$ \BT: 1 \SBT: 2 & 84.80\%  & 56.69\% & 158.99\%  & 0.357\\\hline
$S_{2}$ \BT: 1 \SBT: 4 & 81.94\%  & 49.07\% & 83.34\%  & 0.589\\\hline
$S_{3}$ \BT: 0.5 \SBT: 3 & 84.71\%  & 53.74\% & 100.36\%  & 0.535\\\hline
$S_{4}$ \BT: 1.5 \SBT: 3 & 85.39\%  & 58.85\% & 93.75\%  & 0.628\\\hline
$S_{5}$ \BT: 1/1.5 off/on & \MR{85.88\%}  & \MR{59.07\%} & \MR{91.01\%}  & \MR{0.649}\\
\SBT: 2.5/3 off/on & & & &\\\hline
\end{tabular}
\begin{tabular}{l}
\\{$^\star$} All evaluations in this table for \NAME are using {\em Classification Tree} with \\
\ \ \ {\em MostRecent} prediction model ($\alpha=1000$, $\beta=1$).
\end{tabular}
\ncaption{Comparison of different parameter settings}
\label{tab:param}
\end{center}
\end{table}

As suggested by previous work~\cite{imc.screen}, a different parameter setting distinguishing screen-on and screen-off could yield better optimization results given their traffic pattern difference, because screen-off traffic is very likely to be generated by background applications without user interaction. A more aggressive setting could be applied to screen-off traffic to save energy without incurring much signaling overhead. Specifically, here a more aggressive setting means a smaller \BT, which gives \NAME an opportunity to predict \IBT earlier, and a smaller \SBT, which classifies more \IBTS into the long category hence resulting in more fast dormancy invocations. In Table~\ref{tab:param}, $S_{5}$ is an example of such screen-aware setting that yields good results. Compared with $S_{4}$, $S_{5}$ saves more energy with less signaling overhead incurred. In fact, $S_{5}$ achieves comparable optimization results as the optimal case shown in Table~\ref{tab:ml} does.

In practice, given the training data, we can periodically search within a large range of \BT and \SBT values for an optimal setting. The setting can also be made dynamic and customized for users.

To summarize, \NAME achieves radio energy saving by 59.07\%, at the cost of 91.01\% additional signaling overhead in LTE networks, outperforming previous proposals. Specifically, as shown in Table~\ref{tab:ml}, in order to achieve such energy saving (59\%), the additional signaling overheads incurred by MakeIdle and na\"{\i}ve fast dormancy are as high as 305\% and 215\%, respectively. The maximal energy saving achieved by RadioJockey is 27\% since it is only applicable to background traffic, ideally generated by a single application running in the background.

\nsubsection{Discussions on Real Deployment}

The proposed \NAME system is fully practical for today's smartphones. The overhead includes data collection, model training and prediction, and invoking fast dormancy. As discussed in previous work~\cite{radiojockey}, invoking fast dormancy incurs negligible overhead compared with state promotion. For ``fast dormancy 3s'', the total promotion energy is only 2.27\% of the total energy. Hence, it is reasonable to ignore the fast dormancy invocation overhead.

\paragraph{Data collector}: unlike RadioJockey requiring system call traces for all applications, \NAME only need to monitor packet traces, which is also required by RadioJockey. The data collector runs in the background and does not interfere with other user applications. It incurs no more than 5\% of CPU overhead when collecting packet headers, although the overhead is much lower when the throughput is low (\eg less than 200 kbps). The additional power to run the data collector is less than 17.1mW most of the time, which is only 1.6\% of the tail power for the studied LTE network.
%In fact, only packet headers are required by \NAME, and the overhead of data collector could be further reduced. \comment{update CPU number here with collecting only packet header}

\paragraph{Model training and prediction}: the {\em Ensemble (RobustBoost)} and {\em Ensemble (Bag)} require about 2.5 seconds to train a model, which is not acceptable if the model needs to be rebuilt frequently. In practice, we could use {\em Classification Tree} for prediction, which requires 547.72ms to build the model and 23.68ms to predict. We measure that the power level with CPU 100\% used is 340.5mW. For \MostRecent model with $(\alpha, \beta) = (1000, 1)$, the total energy overhead is 2.41\%, which is also very small. Also, by changing $\beta$ to 2, we can reduce this overhead to 1.25\% with minor performance degradation. In practice, we could put the \NAME service into lower priority so that it does not compete for CPU cycles when the user applications are busy.

In terms of the {\bf storage cost}, for the \MostRecent model, information of the $\alpha$ = 1000 historical bursts needs to be stored at the UE side, and according to our implementation, the storage of one burst is less than 200 bytes. So the total storage cost is only 200KB, negligible for modern smartphones.

\nsection{Summary}
\label{sec:rp.summary}

In summary, we propose a novel, practical, and effective system called \NAMEFULL to address the radio resource deallocation problem for cellular networks. \NAME takes as input the most recent history information of UE traffic to build a prediction framework for traffic inter-burst time (\IBT), leveraging machine learning to predict \IBT based on a list of lightweight and carefully selected features. It performs resource deallocation only when \IBT is sufficiently long to ensure low signaling overhead. We demonstrate that \NAME can be implemented at the UE transparently to applications, with negligible energy overhead and fast response times. Using 7-month-data collected from 20 real users with accurate RRC and radio power simulator for LTE 4G networks, we evaluated the performance of \NAME. With 85.88\% of accuracy in predicting \IBTS, \NAME provides the option of saving radio energy by up to 59.07\%, at the cost of 91.01\% of additional signaling overhead. We believe the \NAME system represents a significant step in systematically optimizing energy and radio resource for 3G and 4G LTE networks.

