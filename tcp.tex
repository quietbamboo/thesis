\chapter{An In-depth Study of LTE: Effect of Network Protocol and Application Behavior on Performance}
\label{chap:tcp}

4G LTE is the latest deployed cellular network technology that
provides high-speed data services for mobile devices with advertised
bandwidths matching and even exceeding the home broadband network
speeds. Recent work~\cite{huang_mobisys12} has demonstrated the power model of the LTE
network compared to 3G provides the promise of higher energy
efficiency as a result of the new RRC state machine design and higher
achievable throughput. However, this new technology has not been
extensively studied empirically in a deployed commercial network setting to
understand how network resources are utilized across different
protocol layers for real users. It is important to evaluate the
benefits of increased bandwidth for popular mobile applications and
essential network protocols such as TCP to identify their limitations
for needed improvements. Intuitively, network protocol overheads can
be significant enough to prevent efficient usage of available network
resources~\cite{Zhuang:A3:Mobicom2006}. This has been shown in network settings with high network
capacity but potentially unpredictable network conditions~\cite{rfc1323}.

In this chapter, we evaluated the usage of LTE network resources by
analyzing an extensive data trace collected in a large geographic
region of a commercial LTE network. As far as we know, this is the
first in-depth analysis of deployed LTE technology in a commercial
setting.  We systematically complement the data analysis with local
experiments using controlled traffic patterns to confirm or further
investigate our observations based on data traces. Given the
prevalence of proxy deployment in cellular networks for improving user
perceived performance due to inherently limited radio network
resources, we also study the impact of such middleboxes on
performance. No previous work has performed any detailed evaluation of
such impact.

Our approach to characterizing the usage of a commercial LTE network
starts with careful analysis of basic network characteristics in terms
of TCP flow properties, network latency, followed by the congestion
control statistics of observed TCP flows. To answer the question of
whether application traffic is effectively utilizing available network
resources, we propose a lightweight method to estimate the available
network bandwidth based on the fine-grained TCP data packet and ACK
packet exchange close in time, while making use of the TCP Timestamp
option. We comprehensively validate the accuracy of our bandwidth estimation
algorithm using controlled experiments. We expect this algorithm to be
helpful in identifying protocol level and application level
inefficiencies even in the presence of sufficiently available network
resources.  Besides performance overhead, network usage efficiency has
direct impact on the energy usage of mobile devices.
We highlight the potential energy waste due to ineffective use of available network resources.
Given the prevalence of video and audio applications on
cellular networks and their significant contribution to the network
resource usage, we perform a case study on popular multimedia
applications of Shazam and Netflix from the perspectives of
network resource usage and identify application-specific reasons.

In summary, we make the following contributions:
\begin{itemize}
\item Using the TCP Timestamps option, we propose an accurate way to estimate the
available bandwidth in either direction by observing the TCP packet
streams between the mobile device and the server.
\item We develop a set of pragmatic techniques for passively capturing TCP flow
characteristics such as flow size, flow duration, flow rate, loss rate, queuing delay, LTE promotion delay
from a monitor placed between the LTE Radio Access Network (RAN) and
the Serving Gateway (SGW) or Packet Data Network Gateway (PGW).
\item To evaluate performance of TCP flows, we design simple
heuristics to identify abnormal TCP behavior based on duplicate ACKs,
out of order packets, and slow start through the analysis of packet
traces and congestion window size.
\end{itemize}

Besides these methodological contributions, we make the following
insightful observations about LTE network usage.

\begin{itemize}
\item For large TCP flows, long queueing delay may significantly increase RTT to a few times the normal value. Such long RTTs are found to cause congestion window collapse in the middle of a flow upon a single packet loss or reordering for 12.25\% of all large TCP flows.
\item We observe that 52.61\% of all downlink TCP flows have experienced full TCP receive window or even zero receive window, limiting the sending rate.
\item Overall, with the bandwidth estimation algorithm, we observe
that for 71.26\% of the large flows, the bandwidth utilization ratio
is below 50\%. And on average, data transfer takes 52.91\% longer to
complete, incurring additional radio energy overhead.
\end{itemize}

Based on our observations, we make several recommendations on protocol and application design to more effectively take
advantage of the available network resources. We believe our findings likely apply to other LTE networks given the extensive coverage of the data trace used and independent controlled experiments carried out locally.

Here is the roadmap for this chapter. Section~\ref{sec:tcp.method} describes the data set studied and setup for controlled experiments. We then characterize the LTE network characteristics in Section~\ref{sec:tcp.char} and discuss a newly identified TCP performance issue in LTE networks in Section~\ref{sec:tcp.tcp}. We investigate the network resource usage efficiency with a devised bandwidth estimation algorithm in Section~\ref{sec:tcp.estimate}, and then explore the network application behaviors that cause network inefficiency in Section~\ref{sec:tcp.app}, before concluding in \S\ref{sec:tcp.conc}.


\nsection{LTE Data and Local Testbed}
\label{sec:tcp.method}

We give an overview of the LTE network topology before describing our measurement data. We then describe how we perform controlled experiments for validating our findings.

\nsubsection{The LTE Measurement Data}
\label{sec:tcp.data}


\begin{figure}[t]
\centering
\IG{figures/traffic/arch_new.eps}\\
\ncaption{Simplified network topology of the large LTE carrier from which we obtained our measurement data}
\label{fig:tcp.architecture}
\end{figure}


As depicted in Figure~\ref{fig:tcp.architecture}, an LTE network consists of three subsystems: user equipment (UE), the radio access network (RAN), and the core network (CN).
UEs are essentially mobile handsets carried by
end users. The RAN allows connectivity between a UE
and the CN. It consists of multiple base stations called Evolved Node B (eNB).
The centralized CN is the backbone of the cellular network. It connects to the Internet. In Figure~\ref{fig:tcp.architecture}, within the CN, ``Monitor'' is our data collection point. ``SGW'' and ``PGW'' refer to the serving gateway and the packet data network gateway, respectively. ``PEP'' corresponds to the performance enhancing proxy to be described shortly. From the perspective of UEs, we define \emph{downlink} as the network path from the Internet to UEs, and \emph{uplink} as the path in the reverse direction. Similarly, we also use the terms \emph{downstream} and \emph{upstream} from the perspective of the Monitor to indicate the relative locations of network elements, \eg \emph{downstream} refers to the path between monitor and UEs.

\textbf{The Performance Enhancing Proxy (PEP).}
The data collection point is located within the core network of the studied LTE network. TCP traffic from or to server port 80 or 8080 traverses the PEP on the upstream side of the monitor. The PEP splits the end-to-end TCP connection into two, one between the UE and the PEP and the other between the PEP and the server. It can potentially improve the Web performance by, for example, performing compression and caching. Also the PEP makes the split transparent to UEs by spoofing its IP address to be the server's IP address. We will show later how PEP impacts our measurement results.

\textbf{Data Collection.} Our measurement data is a large packet header trace covering a fixed set of 22 eNBs at a large metropolitan area in the U.S. The data collection was started on October 12 2012 and lasted for 240 hours. We record IP and transport-layer headers, as well as a 64-bit timestamp for each packet. No payload data is captured except for headers of HTTP, the dominant application-layer protocol for today's smartphones~\cite{xu11_imc}. No user, protocol, or flow-based sampling is performed. During the 10 days, we obtained 3.84 billion packets, corresponding to 2.9 TB of LTE traffic (324 GB of packet header data, including HTTP headers). To our knowledge, this is the first large real-world LTE packet trace studied in the research community.

\textbf{Subscriber Identification.} Due to concerns of user privacy, we do not collect any subscriber ID or phone numbers. We instead use private IP addresses (anonymized using a consistent hash function) as approximated subscriber IDs, since private IPs of the carrier are very stable. They change only at the interval of several hours. In contrast, public IP addresses observed by servers may change rapidly~\cite{Mahesh:Ephemera:IMC09}. Private IPs can also be reused. We take this into account by using a timing gap threshold of one hour in our analysis. If a private IP has not been seen for one hour, we assume its corresponding user session has terminated. This potentially overestimates the user base, but its impact on our subsequent analyses is expected to be small since changing this threshold to 30 minutes or 2 hours does not qualitatively affect the measurement results in Section~\ref{sec:tcp.char}, Section~\ref{sec:tcp.estimate}, and Section~\ref{sec:tcp.app}. In total, we observe about 379K anonymized client IPs and 719K server IPs.

\textbf{Flow Extraction.}  From the dataset, we extract flows based on a 5-tuple of src/dst IP, src/dst port numbers,
and protocol (TCP or UDP). We conservatively use a threshold of 1 hour to determine that a flow has
terminated if no flow termination packets are observed. We found that similar to the idle period threshold for subscriber identification, the impact of this value on subsequent analysis results is negligible. Overall, 47.06 million flows are extracted from the trace.

We emphasize here that no customer private information is used in our
analysis and all customer identities are anonymized before any
analysis is conducted. Similarly, to adhere to the confidentiality
under which we had access to the data, in subsequent sections, we present normalized views of our results while retaining the scientifically
relevant bits.

\nsubsection{Controlled Local Experiments}
\label{sec:tcp.testbed}

We also set up a measurement testbed in our lab for controlled experiments. The UE used is a fairly new smartphone model --- Samsung Galaxy S III (SGH-I747) running Android 4.0.4 (IceCream Sandwich, Linux kernel version 3.0.8). It connects to a commercial LTE network in the U.S.  We configure a server with 2GB memory and 2.40GHz Intel Core 2 CPU, running Ubuntu 12.04 with \texttt{3.2.0-36-generic} Linux kernel. Both the UE and the server use TCP CUBIC as their TCP implementation.

Note that the purpose of using local experiments from a potentially different LTE carrier at locations that may not match where our studied dataset comes from is to provide a different perspective and also evaluate whether observations from analyzing the dataset can be empirically observed.

When measuring TCP throughput and RTT (Figures~\ref{fig:rtt.bif.linux},\ref{fig:bw.time}, and~\ref{fig:bw.cdf}), the UE establishes a TCP connection to the server, which then transfers randomized data without any interruption.
%We also install the \texttt{tcpprobe} kernel module on the server for capturing the TCP congestion window size.
For throughput measurement, we ignore the first 10 seconds of the TCP connection (skip the slow start phase), and calculate the throughput every 500 ms from the continuously transferred data. The RTT is measured by computing the gap between timestamps of transmitting a data packet and receiving the corresponding ACK from the sender-side trace collected by the \texttt{tcpdump} tool.

%Mention later:
%The bytes-in-flight is measured by computing the difference between the sequence number of data packet and the ack number of last ack packet at each data packet in the sender-side trace.



\nsection{LTE Networks Characteristics}
\label{sec:tcp.char}

We study LTE traffic characteristics using the aforementioned 10-day packet trace collected from the large commercial LTE service provider. We also compare our results with two previous measurement studies of cellular and Wi-Fi performance on mobile devices (Section~\ref{sec:tcp.compare}).

%Such analysis, the first in this kind, helps us understand the operational LTE networks in more depth.
%\comment{Talk about user count and server count, IP/UDP/TCP count, \etc for general statistics of the data set}

\nsubsection{Flow Size, Duration, Rate, Concurrency}
\label{subsec:char_flow}

We begin by showing the protocol breakdown of the dataset. For the transport-layer protocol, TCP dominates the dataset (95.25\% flow-wise and 97.24\% byte-wise), with majority of the remaining traffic in UDP. Within TCP, as the dominant application-layer protocol, HTTP (port 80/8080) contributes 76.55\% and 50.13\% of all TCP bytes and TCP flows, respectively. We also notice the popularity of HTTPS (port 443), which account for 14.83\% and 42.11\% of TCP bytes and flows, respectively. We present a more detailed app-layer content analysis and compare the findings with those for 3G networks in~\S\ref{sec:http_content}.

%Most information already covered in the data section.
%We use 5-tuple to define a TCP flow: \emph{(src\_ip, dst\_ip, src\_port, dst\_port, protocol)}. Since almost all TCP flows in the LTE networks are initiated by the client, the 5-tuple can be also written as \emph{(client\_ip, server\_ip, client\_port, server\_port, protocol)}. We use the pattern of {\sf 10.*.*.*} to identify client IPs. We use FIN or RESET packets to determine the termination of a flow. If the last packet of a unterminated flow is not a FIN or RESET packet and no packet of has been observed for a hour after its last packet, we assume it has been closed by its last packet. Although the client\_ip and client\_port may be reused, within 1 hour, we believe it is very unlikely to observe two different flows with the same 5-tuple. We do not require a TCP flow to be established successfully. Instead, as long as there is a SYN packet, we consider it as a flow and in our data set, we observe that 8.49\% of the TCP flows are not established successfully and only consist of SYN packets.

Following previous measurement studies of wired and Wi-Fi networks~\cite{trat, qian09, chen12}, we are interested in three characteristics of LTE TCP flows: size, duration, and rate. Size is the total number of payload bytes
within the flow (excluding IP/transport layer headers). Duration is the time span between the
first and last packet of a flow. Flow rate is calculated by dividing
flow size by flow duration. Understanding these characteristics is vital to many aspects in cellular networks such as eNB scheduling, usage-based billing policy, and RAN resource balancing and optimization. Our focus is TCP since its accounts for the vast majority of the traffic (95.25\% of flows and 97.24\% of bytes).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
\centering
\IG{figures/traffic/flow_payload.eps}\\
\ncaption{Distribution of TCP flow sizes}
%\comment{updated this figure by adding two curves, HTTP traffic(80 and 8080), and HTTPS traffic (443), need to mention that for uplink HTTPS$>$HTTP, but for downlink HTTPS is more centered between the middle, \ie, for downlink payload $\geq$ 20KB,  HTTP 20.18\% and HTTPS 9.14\%}
\label{fig:flow.payload}
\end{figure}

\textbf{TCP Flow Size.}
Figure~\ref{fig:flow.payload} plots the CDF of uplink and downlink payload sizes, both exhibiting strong heavy-tail distributions. Most flows are small: 90\% of flows have less than 2.93 KB uplink payload and 90\% of flows carry no more than 35.88 KB downlink payload. In particular, 11.26\% (10.86\%) of flows do not have any downlink (uplink) payload as they only contain complete or incomplete TCP handshakes. On the other hand, a very small fraction of large flows, which are known as ``heavy-hitter'' flows~\cite{qian09}, contribute to the majority of the traffic volume. For downlink, the top 0.57\% of flows ranked by payload sizes, each with over 1 MB of downlink payload, account for 61.73\% of the total downlink bytes. For uplink, the top 0.13\% of flows, each with over 100 KB of uplink payload, consist of 63.86\% of the overall uplink bytes. Such a distribution is as skewed as that in wired networks~\cite{qian09}.

%total, 2469658
%HTTP request >= 1, 1982897
%%80/8080, 1968418
We next examined the top 5\% of downlink flows ranked by their downlink payload sizes. Each of them contains at least 85.9KB of downlink payload data and 80.29\% of them use HTTP. By examining the HTTP headers (if exist) of the top 5\% downlink flows, we found that 74.35\% of their contents (in bytes) are video or audio. Regarding to the top 5\% uplink flows, 73.56\% of their bytes are images. Most of such traffic corresponds to users uploading photos to social networks such as Instagram.
%The above results indicate that large flows can be accurately detected at the beginning by only looking at HTTP headers \comment{Junxian: the content length is actually in the HTTP 200 OK packet, which is the first data packet after the HTTP request, so we can directly read content length!}. \fengcomment{Validate this in your results.} The RAN can subsequently allocate more resources to users with such large flows to ensure their performance. \comment{this assumption is probably too strong, TBD}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
\centering
\IG{figures/traffic/flow_duration.eps}\\
\ncaption{Distribution of flow duration and the duration between the last payload byte to the end of the flow}
\label{fig:flow.duration}
\end{figure}

\textbf{TCP Flow Duration.}
Figure~\ref{fig:flow.duration} shows the distribution of TCP flow duration (the solid line), defined to be the time span between the first and the last packets of a flow. Most flows are short: 48.07\% are less than 5 seconds. 8.49\% of the TCP flows are not even established successfully and they only consist of SYN packets. For the long-tailed part, 6.80\% of the flows last at least 3 minutes and 2.77\% are longer than 10 minutes.
%\comment{any info on how many RTTs the data transfer took place, did we detect any TCP connection timeouts, resets?}

\begin{figure}[t]
\centering
\IG{figures/traffic/delayed_fin.eps}\\
\ncaption{An example of delayed FIN packet and its impact on radio resource management}
\label{fig:delayed_fin}
\end{figure}

The dotted curve in Figure~\ref{fig:flow.duration} denotes the timing gap between the packet carrying the last payload byte and the last packet of a flow. Note that most flows in the dataset are properly terminated by either FIN (86.16\% of flows) or RESET (5.35\%), and for the remaining flows, they consist of only one or more SYN packets (8.49\%). One example of the cause of the aforementioned timing gap is persistent HTTP that tries to reuse the same TCP connection for transferring multiple web objects so there is a timeout before the connection is closed. This does not cause any issue in wired and Wi-Fi networks. However, in LTE networks, there exists a timeout for shutting down the radio interface after a data transfer. Such a timeout, which is called \emph{tail time}, saves energy by taking the device to the idle state once it finishes, and prevents frequent radio state switches~\cite{huang12_mobisys}. We measured the timeout (\ie the tail time) to be 10 seconds for the studied LTE network. A delayed FIN or RESET packet will incur additional radio-on time of 10 seconds and one additional off-on switch if the delay is longer than 10 seconds, leading to waste of device energy~\cite{mobisys.aro}. Figure~\ref{fig:delayed_fin} shows one such example, which is found to be prevalent: delaying FIN or RESET for longer than 10 seconds occurs in 23.14\% of the flows in our dataset as shown in Figure~\ref{fig:flow.duration}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
\centering
\IG{figures/traffic/tp.eps}\\
\ncaption{Distributions of normalized TCP flow rates}
\label{fig:flow.rate}
\end{figure}

\textbf{TCP Flow Rate.} Figure~\ref{fig:flow.rate} measures the flow rate. We observe a huge disparity between uplink and downlink rates, due to \emph{(i)} mobile devices usually do not perform bulk data uploading (\eg FTP and P2P upload), and \emph{(ii}) cellular uplink channel is significantly slower than the downlink channel, even in LTE networks~\cite{4gbook}.  The four downlink throughput distributions for flows with different sizes in Figure~\ref{fig:flow.rate} indicate that larger flows tend to be faster. Previous measurements for wired networks also suggest that for Internet flows, there exist correlations among their size, duration, and rate~\cite{trat, qian09}. We quantitatively confirm that similar behaviors also hold for LTE flows. Let $S$, $D$, and $R$ be downlink flow size, duration, and rate, respectively, and $(X, Y)$ be the correlation coefficient between $X$ and $Y$. We calculate the values of $(logS, logD)$, $(logD, logR)$, and $(logR, logS)$ to be 0.196, -0.885, and 0.392, respectively. For uplink flows, the values of $(logS, logD)$, $(logD, logR)$, and $(logR, logS)$ are 0.030, -0.986, and 0.445, respectively. We found the flow duration and the rate are much more negatively correlated, compared with Internet flows studied in~\cite{qian09}, whose correlation coefficients are between -0.60 and -0.69 for Internet backbone, VPN, and DSL flows. This is worth further investigation to confirm if the sessions are terminated early due to bad performance.

\textbf{Concurrent TCP Flows.} We explore the concurrency of TCP flows per user in the LTE data set, as shown in Figure~\ref{fig:concurrency}. Specifically, we use 1 second as a threshold to determine the concurrency, \ie for the sampled time point, we count the number of TCP flows that have the downlink data transfers within the last 1 second. We observe that for 72.14\% of the time, there is only one TCP flow actively downloading data, and this percentage might be even larger for smartphone users, considering that our data set also consists of a small share of users that uses LTE data cards on their laptops, which may have high TCP flow concurrency.

\begin{figure}[t]
\centering
\IG{figures/traffic/concurrency.eps}\\
\ncaption{Concurrency for TCP flows per user uniformly sampled by time}
\label{fig:concurrency}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\nsubsection{Network Latency}
\label{sec:tcp.latency}

\begin{figure}[t]
\centering
\IG{figures/traffic/rtt.eps}\\
\ncaption{Distributions of normalized handshake RTT and DNS lookup time}
\label{fig:rtt}
\end{figure}

\begin{figure}[t]
\centering
\IG{figures/traffic/rtt_ratio.eps}\\
\ncaption{Distribution of the radio between uplink and downlink RTT (for non-PEP traffic)}
\label{fig:rtt_ratio}
\end{figure}

Figure~\ref{fig:rtt} measures distributions of TCP handshake RTT. ``C'', ``M'', ``P'', and ``S'' correspond to the client (UE), the monitor (the data collection point), the PEP, and the remote server, respectively. Since the monitor lies in the LTE core network, we can break down the overall RTT into two components: the \emph{downstream} RTT between a client and the monitor (``C-M'', for all traffic), and the \emph{upstream} RTT between either the monitor and the PEP (``M-P'', for TCP port 80/8080 traffic) or server (``M-S'', for other traffic). The downstream RTT is an estimation of the latency in the RAN (Figure~\ref{fig:tcp.architecture}). In a TCP three-way handshake, let the monitor's reception time of SYN (uplink), SYNACK (downlink), and ACK (uplink) be $t_1$, $t_2$, and $t_3$, respectively. Then the upstream RTT is computed as $t_2-t_1$, and the downstream RTT is $t_3-t_2$. The ``C-S'' curve combines both the ``C-M'' and the ``M-S'' components (for non-PEP traffic only).

It is well known that in 2G/3G data networks, usually the RAN latency dominates the overall end-to-end delay~\cite{sigmetrics.cluster}. This is no longer the case in LTE networks. Figure~\ref{fig:rtt} shows that the upstream RTT to a remote server (``M-S'') has a higher variance, and is usually larger than the downstream RTT (``C-M''). This is further confirmed by Figure~\ref{fig:rtt_ratio}, which plots the distribution of ratios between the upstream RTT and the downstream RTT for  non-PEP (``C-S'') flows. For 55\% of the non-PEP flows, their upstream RTTs are larger than the corresponding downstream RTT, whose reduction (\ie the reduction of the RAN latency) is mostly attributed to the flattened network topology in the LTE RAN. For example, the two-layered RAN architecture (NodeB and the Radio Network Controller, RNC) in 3G UMTS/HSPA networks is reduced into the single-layered eNB architecture in LTE, helping significantly reducing the RAN latency~\cite{4gbook} (See Section~\ref{sec:tcp.compare} for quantitative comparisons).
Further, the ``M-P'' curve in Figure~\ref{fig:rtt} indicates the latency between the monitor and the PEP is very small.

\begin{figure}[t]
\centering
\IG{figures/traffic/promo2.eps}\\
\ncaption{Estimating the promotion delay}
\label{fig:promo2}
\end{figure}

\textbf{LTE Promotion Delay.}
In cellular networks, the end-to-end latency of a packet that triggers a UE's radio interface to turn on is significantly long. Such a packet incurs a radio resource control (RRC) promotion delay during which multiple control messages are exchanged between a UE and the RAN for resource allocation. The promotion delay can be as long as 2 seconds in 3G networks~\cite{imc.3g}, and it also exists in LTE networks~\cite{huang_mobisys12}. The promotion delay is not included in either the upstream RTT or the downstream RTT in Figure~\ref{fig:rtt}, since the promotion (if any) has already finished when the monitor observes a SYN packet, as illustrated in Figure~\ref{fig:promo2}. However, we are able to infer the promotion delay using the TCP timestamp embedded into a TCP packet when the packet is about to leave the UE. In a three-way handshake, let the TCP timestamp of the SYN and the ACK packet be $TS_{b}$ and $TS_{a}$, respectively. Then the round-trip time (including the promotion delay) experienced by the UE is $G(TS_{b}-TS_{a})$ where $G$ is the inverse of the ticking frequency of UE's clock generating the TCP timestamp. Note that the TCP timestamps are not wall-clock times. Their units depend on the ticking frequency of the UE. We detail how to compute $G$ in Section~\ref{sec:tcp.estimation}. Finally the promotion delay (if exists) could be derived by subtracting the RTT between the UE and the server/PEP (estimated in Figure~\ref{fig:rtt}) from $G(TS_{b}-TS_{a})$, as shown in Figure~\ref{fig:promo2}.

We calculated promotion delays using the aforementioned method, by examining TCP handshakes with the following property: the user does not send or receive a packet within the time window ($t-T, t$) where $t$ is the reception time of SYN and $T$ is the window size. We conservatively choose $T=13$ seconds which is larger than the 10-second timeout of the studied LTE network. This restriction ensures the UE is in the idle state when the handshake is initiated. Therefore, the SYN packet must trigger a state promotion. The 25\%, 50\%, and 75\% percentiles of the promotion delay are 319 ms, 435 ms, and 558 ms, respectively. We found they are significantly shorter than the 3G promotion delays (around 2 seconds from idle to high-power state, and around 1.5 seconds from low-power to high-power state~\cite{imc.3g}), possibly due to the simplified signaling protocol in LTE networks~\cite{4gbook}.

\textbf{DNS Lookup.} The ``DNS'' curve in Figure~\ref{fig:rtt} measures the DNS lookup delay, computed as the delta between the reception time of a DNS request and its response at the monitor. Note this is the latency between monitor and the DNS server, and we are not able to measure the downstream latency since DNS messages are transferred over UDP. We found that the upstream latency is usually very short \ie less than 10 ms for 87.28\% of request-response pairs. Since the studied LTE network (Figure~\ref{fig:tcp.architecture}) has its own DNS server, the short lookup delay indicates the desired effectiveness of the DNS server, which caches most DNS responses so their domain names are effectively resolved locally within the LTE core network.

\begin{figure}[t]
\centering
\IG{figures/traffic/rtt_bif.eps}\\
\ncaption{Downlink bytes in flight vs. downstream RTT}
\label{fig:rtt.bif}
\end{figure}

\begin{figure}[t]
\centering
\IG{figures/traffic/rtt_bif_linux.eps}\\
\ncaption{Downlink bytes in flight vs. downstream RTT (controlled lab experiments with a large LTE carrier)}
\label{fig:rtt.bif.linux}
\end{figure}

%no need to include this figure
%\begin{figure}[h]
%\centering
%\IGML{../figures/traffic/download_local.eps}\\
%\ncaption{SEQ/ACK plot for server/client traces for bulking data transfer}
%\label{fig:seq.ack.local}
%\end{figure}

\begin{figure}[h]
\centering
\IG{figures/traffic/bytes_in_flight.eps}\\
\ncaption{Distribution of downlink bytes in flight for large flows ($>$ 1 MB)}
\label{fig:bytes.in.flight}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\nsubsection{Queuing Delay and Retransmission Rate}
\label{sec:tcp.queue}

\S\ref{sec:tcp.latency} focuses on the RTT of TCP connection establishment during which the small TCP handshake packets are usually unlikely to be buffered by the network. During the data transfer phase, a TCP sender will increase its congestion window, allowing the number of unacknowledged packets to grow. Such ``in-flight'' packets can potentially be buffered by routers and middleboxes on their network paths, incurring queueing delays. In LTE networks, buffers are extensively used to accommodate the varying cellular network conditions and to conceal packet losses~\cite{4gbook}.

Figure~\ref{fig:rtt.bif} shows the relationship between the downstream RTT and the number of downlink in-flight bytes, which are computed by counting the unacknowledged bytes. As shown in Figure~\ref{fig:rtt.bif}, the downstream RTT tends to inflate as the number of in-flight bytes increases. The in-flight bytes in our studied LTE network can be larger than 1200 KB, causing very high latency due to the queuing delay. We verified this in our local experiments (Section~\ref{sec:tcp.testbed}) where we measured both the RTT and the bytes-in-flight on UE. As shown in Figure~\ref{fig:rtt.bif.linux}, the trend is even more obvious: the RTT can be as high as 1 second when the number of in-flight packets is large. Our observation is also consistent with a recent study~\cite{jiang12} that shows the usage of large buffers in today's cellular networks may cause high queuing delays. In addition to that, we further demonstrate its prevalence in today's LTE networks: as shown in Figure~\ref{fig:bytes.in.flight}, which plots the distribution of downlink in-flight bytes for large flows ($>$ 1MB), about 10\% of measured instances have in-flight bytes greater than 200 KB, potentially leading to long queuing delays.

Clearly, for short flows or traffic triggered by user interactions (\eg web browsing), queues are not likely to build up. For long-lived flows, usually it is the throughput instead of latency that matters. However, when short-lived and long-lived flows coexist (\eg performing browsing while streaming in the background), queuing delay may severely deteriorate user experience by introducing unacceptable delays for short flows. Moreover, as a new observation, we found that a high downstream queuing delay may often cause TCP's congestion window to collapse upon a single packet loss. We discuss this newly identified and rather severe issue caused by large buffers in Section~\ref{sec:tcp.tcp}.

%Existing solutions to bufferbloat, such as delay-based TCP congestion control (\eg TCP Vegas) and setting a smaller receive window size are suboptimal due to under-utilizing the available bandwidth, as suggested by a recent study~\cite{jiang12}, which also shows such a tradeoff could be better balanced by changing the receive window size dynamically. \fengcomment{Don't mentioned this again in~\S\ref{sec:tcp} and~\S\ref{sec:estimate}}

\textbf{Retransmission Rate.} We study TCP downlink retransmission rate, defined as the number of retransmitted packets divided by all packets, across all downlink flows in our dataset. 38.12\% of the flows have retransmission rates of zero, and the median value is only 0.06\%. Such low retransmission rates are comparable to those in wired networks~\cite{qian09}. There are even fewer packet losses since the retransmission rate is an upper bound of the packet loss rate in the downstream (\ie between UE and the monitor, note we are not able to capture losses occurring on the upstream side of the monitor). In fact, in cellular networks, most transport-layer losses are concealed by the physical/MAC-layer retransmission and reduced by buffering.
In particular, buffers in LTE networks upstream from the airmile can play an important in absorbing the burstiness of the
traffic transmitted over the lossy wireless link, helping achieve a low loss rate.

%In particular, large buffers in LTE networks help absorb the burstiness of the traffic transmitted over the lossy wireless link, helping achieve a low loss rate.

%\fengcomment{Figure~\ref{fig:retran} can be removed if run out of space. Can just mention in text. }
%\comment{X-axis label should be retransmission rate, not loss rate}

\nsubsection{Comparison to Previous Studies}
\label{sec:tcp.compare}

\begin{table*}[t]
\scriptsize
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Study         & Our Results       & 3GTest~\cite{mobisys.3gtest}  & \MC{4GTest~\cite{huang_mobisys12}} & \multicolumn{6}{|c|}{SpeedTest~\cite{sommers12}}\\
Time          & Oct 2012      & Aug-Dec 2009               & \MC{Oct-Dec 2011}               & \multicolumn{6}{|c|}{Feb 21 - Jun 5 2011 (15 weeks)}\\
\cline{4-11}
Location      & 1 US City & Across U.S.                   & \MC{Across U.S.}                   & \MC{New York City}    & \MC{Madison WI} & \MC{Manchester UK}\\
\cline{4-11}
Type          & LTE Only          & Four 3G ISPs                  & LTE   & WiMAX                      & Cellular & WiFi      & Cell' & WiFi   & Cell' & WiFi\\
\hline
5\% TCP DL$^*$& 569    & 74 - 222$^{\dag}$    & 2112     & 431     & 108      & 404        & 99       & 347     & 28  & 267\\
50\% TCP DL   & 9185     & 556 - 970          & 12740    & 4670    & 1678     & 7040       & 895      & 5742    & 1077     & 4717\\
95\% TCP DL   & 24229     & 1921 - 2943        & 30812    & 10344   & 12922    & 17617      & 3485     & 14173   & 3842     & 15635\\
\hline
5\% TCP UL    & 38                & 24 - 52            & 387      & 172     & 52       & 177        & 55       & 168     & 25       & 180\\
50\% TCP UL   & 2286              & 207 - 331          & 5640     & 1160    & 772      & 2020       & 478      & 1064    & 396      & 745\\
95\% TCP UL   & 8361              & 434 - 664          & 19358    & 1595    & 5428     & 10094      & 1389     & 5251    & 1659     & 5589\\
\hline
5\% HS RTT    & 30                & 125 - 182          & 37       & 89      & 68       & 21         & 99       & 24      & 98       & 34\\
50\% HS RTT   & 70                & 160 - 200          & 70       & 125     & 159      & 54         & 184      & 69      & 221      & 92\\
95\% HS RTT   & 467               & 645 - 809          & 127      & 213     & 786      & 336        & 773      & 343     & 912      & 313\\
\hline
\end{tabular}
\\
\begin{tabular}{l}
{$^*$ TCP DL: downlink throughput (kbps). TCP UL: uplink throughput (kbps). HS RTT: TCP handshake RTT (ms). 5\%, 50\%, 95\%}\\
{\ \ \ are percentiles.}\\
{$^{\dag}$ For a range $x$\ --\ $y$, $x$ and $y$ are the result of the worst and the best carriers, respectively, for that particular test.}\\
\end{tabular}
\ncaption{Comparing with previous measurement studies}
\label{table:tcp.compare}
\end{table*}


We compare our results with three previous measurement studies, focusing on three important metrics: TCP downlink throughput, TCP uplink throughput, and TCP handshake RTT.
The 3GTest study~\cite{mobisys.3gtest} deployed an app that measures network performance metrics on users' handsets. Their data consisted of 35K cellular (3G only) tests from customers of four large U.S. cellular carriers in late 2009. The 4GTest study~\cite{huang_mobisys12} adopts a similar approach while focusing on LTE users. Its data comprises of about 1K LTE tests and a few WiMAX tests across the U.S. in late 2011. A recent study~\cite{sommers12} examined a 15-week dataset from \texttt{speedtest.net} in 2011. Table~\ref{table:tcp.compare} shows their reported performance metrics for handheld device users from three locations: New York City (246K Wi-Fi tests / 79K cellular tests), Madison Wisconsin U.S. (24K Wi-Fi / 4K cellular), and Manchester U.K. (291K / 31K). The cellular technology ranges from 2G EDGE to 4G LTE, but is dominated by 3G (UMTS/EvDO/HSPA).

%Another study~\cite{canadi12} examined another large \texttt{speedtest} dataset to investigate today's broadband performance for all types of end hosts. Table~\ref{table:compare} considers a small subset of their data \ie 4.4 million tests from wired and wireless Internet users at Los Angeles, CA from Jun to Nov 2011.

We discuss three major issues that may affect the comparison. First, all three previous studies perform throughput measurement using bulk data transfer of a large file without any pause while our flows may consist of idle time periods (\eg due to user think time), leading to a lower throughput. To obtain more fair comparison, here we only consider large non-PEP flows in our dataset (with at least 200 KB for uplink and 1 MB for downlink) with no visible idle time period (with maximum inter-packet time of less than 1 second, which is larger than 99.9$^{th}$ percentile of RTT). Second, in our case, remote servers may impose rate limit~\cite{gerber10} while all previous studies perform active probing using dedicated test servers without any limitation on throughput. Third, we infer performance metrics from traces of real Internet servers, while 3GTest, 4GTest, and SpeedTest employ different server selection policies: 3GTest uses a single server located in U.S. while SpeedTest picks a server geographically close to the UE. This in particular affects the latency estimation.

The comparison results are shown in Table~\ref{table:tcp.compare}. Despite aforementioned differences among diverse measurement approaches, we believe the comparison can still demonstrate the advantage of LTE over other types of cellular access technology, since their performance difference is quite significant: the median downlink throughput, uplink throughput, and handshake RTT are 9.5x, 6.9x, and 0.43x compared with the median values of the best U.S. 3G carrier in 2009, respectively. Compared with the 2011 New York City cellular results, the ratios are 5.5x, 3.0x, and 0.44x for DL throughput, UL throughput, and RTT, respectively. Moreover, on mobile devices, LTE also outperforms Wi-Fi in many cases. Specifically, for the 5$^{th}$/50$^{th}$/95$^{th}$ percentiles of downlink throughput and the median uplink throughput shown in Table~\ref{table:tcp.compare}, LTE performs better handheld Wi-Fi. Based on Table~\ref{table:tcp.compare}, LTE's latency appears higher than that of Wi-Fi. However, recall that Speedtest always picks a nearby test server while we are measuring the RTT between UE and real servers that may be far away. This may lead to an unfair RTT comparison. Furthermore, our performance values are consistently lower than those reported by LTE tests of 4GTest, very likely due to the rate limiting imposed by remote servers as mentioned before. A recent study~\cite{gerber10} indicates such rate limiting is prevalent across today's Internet servers. We also observe that LTE significantly outperforms WiMAX in all three metrics.





\nsection{Abnormal TCP behavior}
\label{sec:tcp.tcp}

%\begin{figure}[h]
%\centering
%\IGML{../figures/traffic/seq_ack2.eps}\\ original local controlled experiment plot
%\ncaption{Sequence number and ACK number plot for a long list of duplicate ACKs \comment{use real example to plot this figure}}
%\label{fig:seq.ack}
%\end{figure}

%\comment{Should have talked about the definition of big flows and their statistics in the general statistics description section.}
Due to their resource usage, we focus on \emph{large flows} defined to be relatively long flows, with more than 5 seconds data transfer time, and total downlink payload exceeding 1MB. These large flows account for only 0.33\% of all TCP flows in our data set, but their total downlink payload contributes to  47.66\% of all downlink payload.

\begin{figure}[h]
\centering
\IG{figures/traffic/dupack.eps}\\
\ncaption{Observed duplicate ACKs and packet reordering in large TCP flows}
\label{fig:dup.ack}
\end{figure}

TCP receiver sends an immediate duplicate ACK upon receiving an out-of-order unacknowledged segment~\cite{rfc5681}. From the sender's perspective, duplicate ACKs can be caused by reordering or loss. In the LTE networks, when there is a large amount of bytes in flight and one data segment is lost, each data segment with sequence number after this lost one, before receiving the retransmitted segment, triggers a duplicate ACK. So a long sequence of duplicate ACKs strongly suggests a packet loss. When TCP detects 3 duplicate ACKs, it infers a data packet loss and retransmits it according to the fast retransmit~\cite{rfc5681}. In the monitor traces, we detect this behavior as the data packet sent by fast retransmit is out-of-order relative to other packets.

Figure~\ref{fig:dup.ack} summarizes the observation of duplicate ACKs and packet reordering in the large TCP flows. Although the median of duplicate ACKs in large flows is 17, for over 29.04\% of large flows, there are over 100 duplicate ACKs. We observe that the number of out-of-order data packets in large flows is substantially smaller than that of duplicate ACKs, with a median value of only 2. By studying the ratio between duplicate ACKs and out-of-order data packets, 24.72\% of flows have a ratio of over 25, and for some flows, this ratio can reach up to 5,000!

%, and if the corresponding ACK of the retransmitted segment arrives at the sender within the
Fast retransmission allows TCP to directly send the lost segment to the receiver possibly preventing retransmission timeout (RTO). If so, TCP would resume data transfer with
 the congestion window size reduced to the slow-start threshold, according to Fast Recovery. However, as shown earlier, we identified significant queuing build up due to large buffers between the UE and the monitor.
Such large in-network queues capable of holding up to a few megabytes data could delay the receipt of the retransmitted data packet. If the corresponding ACK does not arrive at the server within the RTO, the congestion window would drop to 1 segment, triggering slow start, significantly hurting TCP performance.

\begin{figure}[t]
\centering
\IG{figures/traffic/seq_ack_dupack.eps}\\
\ncaption{Duplicate ACKs not triggering a slow start}
%\ncaption{One real sample flow, dup ACK does not trigger slow start \comment{break down cases where slow start happens and not} iPhone, support SACK. Last RTT sample before duplicate ACK, 371ms.RTT for the fast retransmitted packet, 68ms. RTT has high variation, so RTO is relatively larger}
\label{fig:seq.ack1}
\vspace{-.1in}
\end{figure}

\begin{figure}[t]
\centering
\IG{figures/traffic/seq_ack_dupack2.eps}\\
\ncaption{Duplicate ACKs triggering a slow start}
%\ncaption{One real sample flow, dup ACK DOES trigger slow start. Android Support SACK. Last RTT sample before duplicate ACK, 262ms.RTT for the fast retransmitted packet, 356ms. RTO < 356.\comment{ECO, ECN?}}
\label{fig:seq.ack2}
\vspace{-.1in}
\end{figure}

Figures~\ref{fig:seq.ack1} and~\ref{fig:seq.ack2} demonstrate two sample cases in the data set, where Figure~\ref{fig:seq.ack1} shows that the train of duplicate ACKs does not trigger slow start and Figure~\ref{fig:seq.ack2} includes a case that slow start is triggered. One key difference is that Figure~\ref{fig:seq.ack2} has about 500KB bytes in flight before the first duplicate ACK, while Figure~\ref{fig:seq.ack1} has much fewer bytes in flight.

RTO of TCP is calculated by the sender using smoothed round-trip time and round-trip time variation~\cite{rfc6298}. However, using duplicate ACKs to update RTO is not standardized. In Figure~\ref{fig:seq.ack2}, between 1.1s and 1.5s, the sender receives many duplicate ACKs. Due to the growing queueing size, RTT grows from 262ms (the last RTT sample before the first duplicate ACK) to 356ms, the RTT for the retransmitted packet. The sender's TCP implementation apparently ignores these duplicate ACKs for updating RTO and RTO remains the same without considering the duplicate ACKs. Following the method for calculating RTO~\cite{rfc6298}, we observe that RTO is around 290ms before the first duplicate ACK, which is smaller than the RTT of the retransmitted packet (356ms). This problem does not happen in Figure~\ref{fig:seq.ack1}, because the RTT before the first duplicate ACK is close to that after the last duplicate ACK, due to the small number of bytes in flight. Although it is recommended that the RTO should be at least 1 second~\cite{rfc6298}, depending on the operating systems, different minimum values are used, \eg Linux's minimum RTO is 200ms~\cite{linuxrto}.

We also study the extent of such problem as it affects other users. To tell whether there is a slow start following a long list of duplicate ACKs, we use a simple heuristic metric $R_{ss}$, ratio of slow start:
$R_{ss} = \frac{\theta_{[100, 200]}}{\theta_{[0, 100]}}$,
where $\theta_{[t_{1}, t_{2}]}$ is the average downlink throughput from $t_{1}$ ms to $t_{2}$ ms after the last duplicate ACK. We choose 200 ms empirically as it is observed to be shorter than a typical slow start in the LTE networks. During slow start, $R_{ss}$ is expected to be larger than that when there is no slow start. For example, the $R_{ss}$ is 0.96 for Figure~\ref{fig:seq.ack1} and $R_{ss}$ is 3.72 for Figure~\ref{fig:seq.ack2}. In practice, we observe that 1.5 is a good threshold for $R_{ss}$ in determining slow start. Using this threshold, we have determined that for all the large TCP flows with at least one lost data packet, 20.07\% of them suffer from the slow start problem, which consists of 12.25\% of all large TCP flows. In one case, a 153-second flow even experience 50 slow starts, resulting in an average throughput of only 2.83Mbps, while the estimated bandwidth actually larger than 10Mbps.
%, \ie if $R_{ss} \geq 1.5$, we classify it as slow start
%\comment{Also discuss the fact that during the DUP ACK time, 1 is still sending data, due to the congestion window inflation by the duplicate ACKs, 2 is not sending data, because congestion window is full, especially after timeout of the retransmitted packet, congestion window is set to be 1 full-sized segment.} Based on this observation, we propose a patch to the existing TCP RTT estimation algorithm in the LTE networks, that the sender should keep track of the bytes in flight $B_{if}$ and take it into consideration for RTT estimation. \comment{how does existing TCP RTT works, can we make head-to-head comparison?}

There are different ways to mitigate this problem. One approach is to update RTO with the help of duplicate ACKs with TCP Selective Acknowledgment options (SACK)~\cite{rfc2018}. Assuming there is no packet reordering, by taking the difference between the SACK window of two consecutive duplicate ACKs, we can identify the exact data packets corresponding to these ACKs. If there is ambiguity, either due to ACK reordering or additional packet loss, we simply ignore this sample. In our data sets, packet reordering rate is less than 1\%, making this approach promising.

If SACK is disabled, we can use a fall-back approach to estimate RTT based on duplicate ACKs by assuming that they are in response to the data packets sent out in order. This assumption holds in most cases as the packet reordering rate is low. Using these approaches, we can obtain RTT estimations for duplicate ACKs and update RTO accordingly, which effectively prevents the timeout of retransmitted packet due to queueing delay. Note that the RTT estimation method used in TCP Vegas~\cite{tcp.vegas} by help of TCP Timestamps option is not applicable to duplicate ACKs, since the echo timestamps of all duplicate ACKs are all the same, which is the timestamp of the segment before the lost segment, rather than the timestamp triggering the duplicate ACK. Our initial analysis shows that these two approaches are able to prevent more than 95\% of the slow starts. From the mobile network operators' perspective, one simple solution for this problem could be prioritizing the retransmitted packet. The retransmitted packet could be inferred by tracking the TCP flow status. However, the security and performance implications of this approach are yet to be studied.


