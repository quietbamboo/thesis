\chapter{An In-depth Study of LTE: Effect of Network Protocol and Application Behavior on Performance}
\label{chap:tcp}

4G LTE is the latest deployed cellular network technology that
provides high-speed data services for mobile devices with advertised
bandwidths matching and even exceeding the home broadband network
speeds. Recent work~\cite{huang_mobisys12} has demonstrated the power model of the LTE
network compared to 3G provides the promise of higher energy
efficiency as a result of the new RRC state machine design and higher
achievable throughput. However, this new technology has not been
extensively studied empirically in a deployed commercial network setting to
understand how network resources are utilized across different
protocol layers for real users. It is important to evaluate the
benefits of increased bandwidth for popular mobile applications and
essential network protocols such as TCP to identify their limitations
for needed improvements. Intuitively, network protocol overheads can
be significant enough to prevent efficient usage of available network
resources~\cite{Zhuang:A3:Mobicom2006}. This has been shown in network settings with high network
capacity but potentially unpredictable network conditions~\cite{rfc1323}.


We are motivated by the fact that LTE uses unique backhaul and radio network technologies, and has unique features distinguishing it from other access technologies (\eg much higher available bandwidth and lower RTT), requiring some existing topics to be revisited. Also, the {\em prevalence} of these problems in commercial LTE networks is very important for both academia and industry. In this chapter, we evaluate the usage of LTE network resources by analyzing an extensive data trace collected in a large geographic region of a commercial LTE network. As far as we know, this is the first in-depth analysis of deployed LTE technology in a commercial setting.  We systematically complement the data analysis with local experiments using controlled traffic patterns to confirm or further investigate our observations based on data traces. Given the prevalence of proxy deployment in cellular networks for improving user perceived performance due to inherently limited radio network resources, we also study the impact of such middleboxes on performance. No previous work has performed any detailed evaluation of such impact.

Our approach to characterizing the usage of a commercial LTE network
starts with a careful analysis of basic network characteristics in terms
of TCP flow properties, network latency, followed by the congestion
control statistics of observed TCP flows. To answer the question 
whether application traffic is effectively utilizing available network
resources, we devise a lightweight method to estimate the available
network bandwidth based on the fine-grained TCP data packet and ACK
packet exchange close in time, while making use of the TCP Timestamps
option. We validate the accuracy of our bandwidth estimation
algorithm using controlled experiments. We expect this algorithm to be
helpful in identifying protocol level and application level
inefficiencies even in the presence of sufficiently available network
resources.  Besides performance overhead, network usage efficiency has
direct impact on the energy usage of mobile devices.
We highlight the potential energy waste due to ineffective use of available network resources.
Given the prevalence of video and audio applications in cellular networks and their significant contribution to the network
resource usage, we perform a case study on popular multimedia
applications from the perspectives of network resource usage. 


In summary, we make the following contributions:
\begin{itemize}
\item Using the TCP Timestamps option, we devise a {\em passive} method to estimate the
available bandwidth by observing the TCP packet streams between the mobile device and the server.
\item We develop a set of pragmatic techniques for passively capturing TCP flow
characteristics such as flow size, flow duration, flow rate, loss rate, queuing delay, LTE promotion delay
from a monitor placed between the LTE Radio Access Network (RAN) and
the Serving Gateway (SGW) or Packet Data Network Gateway (PGW).
\item To evaluate performance of TCP flows, we design simple
heuristics to identify abnormal TCP behavior based on duplicate ACKs,
out of order packets, and slow start through the analysis of packet
traces and congestion window size.
\end{itemize}

Besides these methodological contributions, we make the following
insightful observations about LTE network usage.

\begin{itemize}
\item For large TCP flows, queueing delay may increase RTT to a few times the normal value. However, as TCP does not use duplicate ACKs to update RTT estimate (thus retransmission timeout RTO), undesired slow start may occur in the middle of a flow upon a single packet loss or reordering, and this phenomenon is observed for 12.3\% of all large TCP flows.
\item We observe that 52.6\% of all downlink TCP flows have experienced full TCP receive window or even zero receive window, limiting the sending rate.
\item Overall, with the bandwidth estimation algorithm, we observe that for 71.3\% of the large flows, the bandwidth utilization ratio
is below 50\%. And on average, data transfer takes 52.9\% longer than if the bandwidth was fully utilized, incurring additional radio energy overhead.
\end{itemize}

Based on these observations, we make several recommendations on protocol and application design to more effectively take
advantage of the available network resources. We believe our findings apply to other LTE networks given the extensive coverage of the data set and independent controlled experiments carried out locally.

Here is the roadmap for this chapter. Section~\ref{sec:tcp.method} describes the data set studied and setup for controlled experiments. We then characterize the LTE network characteristics in Section~\ref{sec:tcp.char} and discuss a newly identified TCP performance issue in LTE networks in Section~\ref{sec:tcp.tcp}. We investigate the network resource usage efficiency with a devised bandwidth estimation algorithm in Section~\ref{sec:tcp.estimate}, followed by exploring the network application behaviors that cause network inefficiency in Section~\ref{sec:tcp.app}.


\nsection{LTE Data and Local Testbed}
\label{sec:tcp.method}

We give an overview of the LTE network topology before describing our measurement data. We then describe how we perform controlled experiments for validating our findings.

\nsubsection{The LTE Measurement Data}
\label{sec:tcp.data}


\begin{figure}[t]
\centering
\IG{figures/traffic/arch_new.eps}\\
\ncaption{Simplified network topology of the large LTE carrier from which we obtained our measurement data}
\label{fig:tcp.architecture}
\end{figure}


As depicted in Figure~\ref{fig:tcp.architecture}, an LTE network consists of three subsystems: user equipment (UE), the radio access network (RAN), and the core network (CN).
UEs are essentially mobile handsets carried by
end users. The RAN allows connectivity between a UE
and the CN. It consists of multiple base stations called Evolved Node B (eNB).
The centralized CN is the backbone of the cellular network. It connects to the Internet. In Figure~\ref{fig:tcp.architecture}, within the CN, ``Monitor'' is our data collection point. ``SGW'' and ``PGW'' refer to the serving gateway and the packet data network gateway, respectively. ``PEP'' corresponds to the performance enhancing proxy to be described shortly. From the perspective of UEs, we define \emph{downlink} as the network path from the Internet to UEs, and \emph{uplink} as the path in the reverse direction. Similarly, we also use the terms \emph{downstream} and \emph{upstream} from the perspective of the Monitor to indicate the relative locations of network elements, \eg \emph{downstream} refers to the path between monitor and UEs.

\textbf{The Performance Enhancing Proxy (PEP).}
The data collection point is located within the core network of the studied LTE network. TCP traffic from or to server port 80 or 8080 traverses the PEP on the upstream side of the monitor. The PEP splits the end-to-end TCP connection into two, one between the UE and the PEP and the other between the PEP and the server. It can potentially improve the Web performance by, for example, performing compression and caching. Also the PEP makes the split transparent to UEs by spoofing its IP address to be the server's IP address. We will show later how PEP impacts our measurement results.

\textbf{Data Collection.} Our measurement data is a large packet header trace covering a fixed set of 22 eNBs at a large metropolitan area in the U.S. The data collection was started on October 12 2012 and lasted for 240 hours. We record IP and transport-layer headers, as well as a 64-bit timestamp for each packet. No payload data is captured except for headers of HTTP, the dominant application-layer protocol for today's smartphones~\cite{xu11_imc}. No user, protocol, or flow-based sampling is performed. During the 10 days, we obtained 3.84 billion packets, corresponding to 2.9 TB of LTE traffic (324 GB of packet header data, including HTTP headers). To our knowledge, this is the first large real-world LTE packet trace studied in the research community.

\textbf{Subscriber Identification.} Due to concerns of user privacy, we do not collect any subscriber ID or phone numbers. We instead use private IP addresses (anonymized using a consistent hash function) as approximated subscriber IDs, since private IPs of the carrier are very stable. They change only at the interval of several hours. In contrast, public IP addresses observed by servers may change rapidly~\cite{Mahesh:Ephemera:IMC09}. Private IPs can also be reused. We take this into account by using a timing gap threshold of one hour in our analysis. If a private IP has not been seen for one hour, we assume its corresponding user session has terminated. This potentially overestimates the user base, but its impact on our subsequent analyses is expected to be small since changing this threshold to 30 minutes or 2 hours does not qualitatively affect the measurement results in Section~\ref{sec:tcp.char}, Section~\ref{sec:tcp.estimate}, and Section~\ref{sec:tcp.app}. In total, we observe about 379K anonymized client IPs and 719K server IPs.

\textbf{Flow Extraction.}  From the dataset, we extract flows based on a 5-tuple of src/dst IP, src/dst port numbers,
and protocol (TCP or UDP). We conservatively use a threshold of 1 hour to determine that a flow has
terminated if no flow termination packets are observed. We found that similar to the idle period threshold for subscriber identification, the impact of this value on subsequent analysis results is negligible. Overall, 47.06 million flows are extracted from the trace.

We emphasize here that no customer private information is used in our
analysis and all customer identities are anonymized before any
analysis is conducted. Similarly, to adhere to the confidentiality
under which we had access to the data, in subsequent sections, we present normalized views of our results while retaining the scientifically
relevant bits.

\nsubsection{Controlled Local Experiments}
\label{sec:tcp.testbed}

We also set up a measurement testbed in our lab for controlled experiments. The UE used is a fairly new smartphone model --- Samsung Galaxy S III (SGH-I747) running Android 4.0.4 (IceCream Sandwich, Linux kernel version 3.0.8). It connects to a commercial LTE network in the U.S.  We configure a server with 2GB memory and 2.40GHz Intel Core 2 CPU, running Ubuntu 12.04 with \texttt{3.2.0-36-generic} Linux kernel. Both the UE and the server use TCP CUBIC as their TCP implementation.

Note that the purpose of using local experiments from a potentially different LTE carrier at locations that may not match where our studied dataset comes from is to provide a different perspective and also evaluate whether observations from analyzing the dataset can be empirically observed.

When measuring TCP throughput and RTT (Figures~\ref{fig:rtt.bif.linux}, \ref{fig:bw.time}, and~\ref{fig:bw.cdf}), the UE establishes a TCP connection to the server, which then transfers randomized data without any interruption.
%We also install the \texttt{tcpprobe} kernel module on the server for capturing the TCP congestion window size.
For throughput measurement, we ignore the first 10 seconds of the TCP connection (skip the slow start phase), and calculate the throughput every 500 ms from the continuously transferred data. The RTT is measured by computing the gap between timestamps of transmitting a data packet and receiving the corresponding ACK from the sender-side trace collected by the \texttt{tcpdump} tool.

%Mention later:
%The bytes-in-flight is measured by computing the difference between the sequence number of data packet and the ack number of last ack packet at each data packet in the sender-side trace.



\nsection{LTE Networks Characteristics}
\label{sec:tcp.char}


We study LTE traffic characteristics using the aforementioned 10-day packet trace collected from the large commercial LTE service provider. We also compare our results with two previous measurement studies of cellular and Wi-Fi performance on mobile devices (Section~\ref{sec:tcp.compare}).

%Such analysis, the first in this kind, helps us understand the operational LTE networks in more depth.
%\comment{Talk about user count and server count, IP/UDP/TCP count, \etc for general statistics of the data set}

\nsubsection{Flow Size, Duration, Rate, Concurrency}
\label{subsec:char_flow}

We begin by showing the protocol breakdown of the dataset. For the transport-layer protocol, TCP dominates the dataset (95.25\% flow-wise and 97.24\% byte-wise), with majority of the remaining traffic in UDP. Within TCP, as the dominant application-layer protocol, HTTP (port 80/8080) contributes 76.55\% and 50.13\% of all TCP bytes and TCP flows, respectively. We also notice the popularity of HTTPS (port 443), which account for 14.83\% and 42.11\% of TCP bytes and flows, respectively. We present a more detailed app-layer content analysis and compare the findings with those for 3G networks in~\S\ref{sec:http_content}.

%Most information already covered in the data section.
%We use 5-tuple to define a TCP flow: \emph{(src\_ip, dst\_ip, src\_port, dst\_port, protocol)}. Since almost all TCP flows in the LTE networks are initiated by the client, the 5-tuple can be also written as \emph{(client\_ip, server\_ip, client\_port, server\_port, protocol)}. We use the pattern of {\sf 10.*.*.*} to identify client IPs. We use FIN or RESET packets to determine the termination of a flow. If the last packet of a unterminated flow is not a FIN or RESET packet and no packet of has been observed for a hour after its last packet, we assume it has been closed by its last packet. Although the client\_ip and client\_port may be reused, within 1 hour, we believe it is very unlikely to observe two different flows with the same 5-tuple. We do not require a TCP flow to be established successfully. Instead, as long as there is a SYN packet, we consider it as a flow and in our data set, we observe that 8.49\% of the TCP flows are not established successfully and only consist of SYN packets.

Following previous measurement studies of wired and Wi-Fi networks~\cite{trat, qian09, chen12}, we are interested in three characteristics of LTE TCP flows: size, duration, and rate. Size is the total number of payload bytes
within the flow (excluding IP/transport layer headers). Duration is the time span between the
first and last packet of a flow. Flow rate is calculated by dividing
flow size by flow duration. Understanding these characteristics is vital to many aspects in cellular networks such as eNB scheduling, usage-based billing policy, and RAN resource balancing and optimization. Our focus is TCP since its accounts for the vast majority of the traffic (95.25\% of flows and 97.24\% of bytes).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
\centering
\IG{figures/traffic/flow_payload.eps}\\
\ncaption{Distribution of TCP flow sizes}
%\comment{updated this figure by adding two curves, HTTP traffic(80 and 8080), and HTTPS traffic (443), need to mention that for uplink HTTPS$>$HTTP, but for downlink HTTPS is more centered between the middle, \ie, for downlink payload $\geq$ 20KB,  HTTP 20.18\% and HTTPS 9.14\%}
\label{fig:flow.payload}
\end{figure}

\textbf{TCP Flow Size.}
Figure~\ref{fig:flow.payload} plots the CDF of uplink and downlink payload sizes, both exhibiting strong heavy-tail distributions. Most flows are small: 90\% of flows have less than 2.93 KB uplink payload and 90\% of flows carry no more than 35.88 KB downlink payload. In particular, 11.26\% (10.86\%) of flows do not have any downlink (uplink) payload as they only contain complete or incomplete TCP handshakes. On the other hand, a very small fraction of large flows, which are known as ``heavy-hitter'' flows~\cite{qian09}, contribute to the majority of the traffic volume. For downlink, the top 0.57\% of flows ranked by payload sizes, each with over 1 MB of downlink payload, account for 61.73\% of the total downlink bytes. For uplink, the top 0.13\% of flows, each with over 100 KB of uplink payload, consist of 63.86\% of the overall uplink bytes. Such a distribution is as skewed as that in wired networks~\cite{qian09}.

%total, 2469658
%HTTP request >= 1, 1982897
%%80/8080, 1968418
We next examined the top 5\% of downlink flows ranked by their downlink payload sizes. Each of them contains at least 85.9KB of downlink payload data and 80.29\% of them use HTTP. By examining the HTTP headers (if exist) of the top 5\% downlink flows, we found that 74.35\% of their contents (in bytes) are video or audio. Regarding to the top 5\% uplink flows, 73.56\% of their bytes are images. Most of such traffic corresponds to users uploading photos to social networks such as Instagram.
%The above results indicate that large flows can be accurately detected at the beginning by only looking at HTTP headers \comment{Junxian: the content length is actually in the HTTP 200 OK packet, which is the first data packet after the HTTP request, so we can directly read content length!}. \fengcomment{Validate this in your results.} The RAN can subsequently allocate more resources to users with such large flows to ensure their performance. \comment{this assumption is probably too strong, TBD}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
\centering
\IG{figures/traffic/flow_duration.eps}\\
\ncaption{Distribution of flow duration and the duration between the last payload byte to the end of the flow}
\label{fig:flow.duration}
\end{figure}

\textbf{TCP Flow Duration.}
Figure~\ref{fig:flow.duration} shows the distribution of TCP flow duration (the solid line), defined to be the time span between the first and the last packets of a flow. Most flows are short: 48.07\% are less than 5 seconds. 8.49\% of the TCP flows are not even established successfully and they only consist of SYN packets. For the long-tailed part, 6.80\% of the flows last at least 3 minutes and 2.77\% are longer than 10 minutes.
%\comment{any info on how many RTTs the data transfer took place, did we detect any TCP connection timeouts, resets?}

\begin{figure}[t]
\centering
\IG{figures/traffic/delayed_fin.eps}\\
\ncaption{An example of delayed FIN packet and its impact on radio resource management}
\label{fig:delayed_fin}
\end{figure}

The dotted curve in Figure~\ref{fig:flow.duration} denotes the timing gap between the packet carrying the last payload byte and the last packet of a flow. Note that most flows in the dataset are properly terminated by either FIN (86.16\% of flows) or RESET (5.35\%), and for the remaining flows, they consist of only one or more SYN packets (8.49\%). One example of the cause of the aforementioned timing gap is persistent HTTP that tries to reuse the same TCP connection for transferring multiple web objects so there is a timeout before the connection is closed. This does not cause any issue in wired and Wi-Fi networks. However, in LTE networks, there exists a timeout for shutting down the radio interface after a data transfer. Such a timeout, which is called \emph{tail time}, saves energy by taking the device to the idle state once it finishes, and prevents frequent radio state switches~\cite{huang_mobisys12}. We measured the timeout (\ie the tail time) to be 10 seconds for the studied LTE network. A delayed FIN or RESET packet will incur additional radio-on time of 10 seconds and one additional off-on switch if the delay is longer than 10 seconds, leading to waste of device energy~\cite{mobisys.aro}. Figure~\ref{fig:delayed_fin} shows one such example, which is found to be prevalent: delaying FIN or RESET for longer than 10 seconds occurs in 23.14\% of the flows in our dataset as shown in Figure~\ref{fig:flow.duration}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
\centering
\IG{figures/traffic/tp.eps}\\
\ncaption{Distributions of normalized TCP flow rates}
\label{fig:flow.rate}
\end{figure}

\textbf{TCP Flow Rate.} Figure~\ref{fig:flow.rate} measures the flow rate. We observe a huge disparity between uplink and downlink rates, due to \emph{(i)} mobile devices usually do not perform bulk data uploading (\eg FTP and P2P upload), and \emph{(ii}) cellular uplink channel is significantly slower than the downlink channel, even in LTE networks~\cite{4gbook}.  The four downlink throughput distributions for flows with different sizes in Figure~\ref{fig:flow.rate} indicate that larger flows tend to be faster. Previous measurements for wired networks also suggest that for Internet flows, there exist correlations among their size, duration, and rate~\cite{trat, qian09}. We quantitatively confirm that similar behaviors also hold for LTE flows. Let $S$, $D$, and $R$ be downlink flow size, duration, and rate, respectively, and $(X, Y)$ be the correlation coefficient between $X$ and $Y$. We calculate the values of $(logS, logD)$, $(logD, logR)$, and $(logR, logS)$ to be 0.196, -0.885, and 0.392, respectively. For uplink flows, the values of $(logS, logD)$, $(logD, logR)$, and $(logR, logS)$ are 0.030, -0.986, and 0.445, respectively. We found the flow duration and the rate are much more negatively correlated, compared with Internet flows studied in~\cite{qian09}, whose correlation coefficients are between -0.60 and -0.69 for Internet backbone, VPN, and DSL flows. This is worth further investigation to confirm if the sessions are terminated early due to bad performance.

\textbf{Concurrent TCP Flows.} We explore the concurrency of TCP flows per user in the LTE data set, as shown in Figure~\ref{fig:concurrency}. Specifically, we use 1 second as a threshold to determine the concurrency, \ie for the sampled time point, we count the number of TCP flows that have the downlink data transfers within the last 1 second. We observe that for 72.14\% of the time, there is only one TCP flow actively downloading data, and this percentage might be even larger for smartphone users, considering that our data set also consists of a small share of users that uses LTE data cards on their laptops, which may have high TCP flow concurrency.

\begin{figure}[t]
\centering
\IG{figures/traffic/concurrency.eps}\\
\ncaption{Concurrency for TCP flows per user uniformly sampled by time}
\label{fig:concurrency}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\nsubsection{Network Latency}
\label{sec:tcp.latency}

\begin{figure}[t]
\centering
\IG{figures/traffic/rtt.eps}\\
\ncaption{Distributions of normalized handshake RTT and DNS lookup time}
\label{fig:rtt}
\end{figure}

\begin{figure}[t]
\centering
\IG{figures/traffic/rtt_ratio.eps}\\
\ncaption{Distribution of the radio between uplink and downlink RTT (for non-PEP traffic)}
\label{fig:rtt_ratio}
\end{figure}

Figure~\ref{fig:rtt} measures distributions of TCP handshake RTT. ``C'', ``M'', ``P'', and ``S'' correspond to the client (UE), the monitor (the data collection point), the PEP, and the remote server, respectively. Since the monitor lies in the LTE core network, we can break down the overall RTT into two components: the \emph{downstream} RTT between a client and the monitor (``C-M'', for all traffic), and the \emph{upstream} RTT between either the monitor and the PEP (``M-P'', for TCP port 80/8080 traffic) or server (``M-S'', for other traffic). The downstream RTT is an estimation of the latency in the RAN (Figure~\ref{fig:tcp.architecture}). In a TCP three-way handshake, let the monitor's reception time of SYN (uplink), SYNACK (downlink), and ACK (uplink) be $t_1$, $t_2$, and $t_3$, respectively. Then the upstream RTT is computed as $t_2-t_1$, and the downstream RTT is $t_3-t_2$. The ``C-S'' curve combines both the ``C-M'' and the ``M-S'' components (for non-PEP traffic only).

It is well known that in 2G/3G data networks, usually the RAN latency dominates the overall end-to-end delay~\cite{sigmetrics.cluster}. This is no longer the case in LTE networks. Figure~\ref{fig:rtt} shows that the upstream RTT to a remote server (``M-S'') has a higher variance, and is usually larger than the downstream RTT (``C-M''). This is further confirmed by Figure~\ref{fig:rtt_ratio}, which plots the distribution of ratios between the upstream RTT and the downstream RTT for  non-PEP (``C-S'') flows. For 55\% of the non-PEP flows, their upstream RTTs are larger than the corresponding downstream RTT, whose reduction (\ie the reduction of the RAN latency) is mostly attributed to the flattened network topology in the LTE RAN. For example, the two-layered RAN architecture (NodeB and the Radio Network Controller, RNC) in 3G UMTS/HSPA networks is reduced into the single-layered eNB architecture in LTE, helping significantly reducing the RAN latency~\cite{4gbook} (See Section~\ref{sec:tcp.compare} for quantitative comparisons).
Further, the ``M-P'' curve in Figure~\ref{fig:rtt} indicates the latency between the monitor and the PEP is very small.

\begin{figure}[t]
\centering
\IG{figures/traffic/promo2.eps}\\
\ncaption{Estimating the promotion delay}
\label{fig:promo2}
\end{figure}

\textbf{LTE Promotion Delay.}
In cellular networks, the end-to-end latency of a packet that triggers a UE's radio interface to turn on is significantly long. Such a packet incurs a radio resource control (RRC) promotion delay during which multiple control messages are exchanged between a UE and the RAN for resource allocation. The promotion delay can be as long as 2 seconds in 3G networks~\cite{imc.3g}, and it also exists in LTE networks~\cite{huang_mobisys12}. The promotion delay is not included in either the upstream RTT or the downstream RTT in Figure~\ref{fig:rtt}, since the promotion (if any) has already finished when the monitor observes a SYN packet, as illustrated in Figure~\ref{fig:promo2}. However, we are able to infer the promotion delay using the TCP timestamp embedded into a TCP packet when the packet is about to leave the UE. In a three-way handshake, let the TCP timestamp of the SYN and the ACK packet be $TS_{b}$ and $TS_{a}$, respectively. Then the round-trip time (including the promotion delay) experienced by the UE is $G(TS_{b}-TS_{a})$ where $G$ is the inverse of the ticking frequency of UE's clock generating the TCP timestamp. Note that the TCP timestamps are not wall-clock times. Their units depend on the ticking frequency of the UE. We detail how to compute $G$ in Section~\ref{sec:tcp.estimation}. Finally the promotion delay (if exists) could be derived by subtracting the RTT between the UE and the server/PEP (estimated in Figure~\ref{fig:rtt}) from $G(TS_{b}-TS_{a})$, as shown in Figure~\ref{fig:promo2}.

We calculated promotion delays using the aforementioned method, by examining TCP handshakes with the following property: the user does not send or receive a packet within the time window ($t-T, t$) where $t$ is the reception time of SYN and $T$ is the window size. We conservatively choose $T=13$ seconds which is larger than the 10-second timeout of the studied LTE network. This restriction ensures the UE is in the idle state when the handshake is initiated. Therefore, the SYN packet must trigger a state promotion. The 25\%, 50\%, and 75\% percentiles of the promotion delay are 319 ms, 435 ms, and 558 ms, respectively. We found they are significantly shorter than the 3G promotion delays (around 2 seconds from idle to high-power state, and around 1.5 seconds from low-power to high-power state~\cite{imc.3g}), possibly due to the simplified signaling protocol in LTE networks~\cite{4gbook}.

\textbf{DNS Lookup.} The ``DNS'' curve in Figure~\ref{fig:rtt} measures the DNS lookup delay, computed as the delta between the reception time of a DNS request and its response at the monitor. Note this is the latency between monitor and the DNS server, and we are not able to measure the downstream latency since DNS messages are transferred over UDP. We found that the upstream latency is usually very short \ie less than 10 ms for 87.28\% of request-response pairs. Since the studied LTE network (Figure~\ref{fig:tcp.architecture}) has its own DNS server, the short lookup delay indicates the desired effectiveness of the DNS server, which caches most DNS responses so their domain names are effectively resolved locally within the LTE core network.

\begin{figure}[t]
\centering
\IG{figures/traffic/rtt_bif.eps}\\
\ncaption{Downlink bytes in flight vs. downstream RTT}
\label{fig:rtt.bif}
\vspace{-.1in}
\end{figure}

\begin{figure}[t]
\centering
\IG{figures/traffic/rtt_bif_linux.eps}\\
\ncaption{Downlink bytes in flight vs. downstream RTT (controlled lab experiments with LTE Carrier A)}
\label{fig:rtt.bif.linux}
\end{figure}

\begin{figure}[t]
\centering
\IG{figures/traffic/rtt_bif_verizon.eps}\\
\ncaption{Downlink bytes in flight vs. downstream RTT (controlled lab experiments with LTE Carrier B)}
\label{fig:rtt.bif.verizon}
\end{figure}


%no need to include this figure
%\begin{figure}[h]
%\centering
%\IGML{../figures/traffic/download_local.eps}\\
%\ncaption{SEQ/ACK plot for server/client traces for bulking data transfer}
%\label{fig:seq.ack.local}
%\end{figure}

\begin{figure}[h]
\centering
\IG{figures/traffic/bytes_in_flight.eps}\\
\ncaption{Distribution of downlink bytes in flight for large flows ($>$ 1 MB)}
\label{fig:bytes.in.flight}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\nsubsection{Queuing Delay and Retransmission Rate}
\label{sec:tcp.queue}

Section~\ref{sec:tcp.latency} focuses on the RTT of TCP connection establishment during which the small TCP handshake packets are usually unlikely to be buffered by the network. During the data transfer phase, a TCP sender will increase its congestion window, allowing the number of unacknowledged packets to grow. Such ``in-flight'' packets can potentially be buffered by routers and middleboxes on their network paths, incurring queueing delays. In LTE networks, buffers are extensively used to accommodate the varying cellular network conditions and to conceal packet losses~\cite{4gbook}.

Figure~\ref{fig:rtt.bif} shows the relationship between the downstream RTT and the number of downlink in-flight bytes, which is computed by counting the unacknowledged bytes. As shown in Figure~\ref{fig:rtt.bif}, the downstream RTT tends to inflate as the number of in-flight bytes increases. The in-flight bytes in our studied LTE network can be larger than 1200 KB, causing high latency due to the queuing delay. We very this in local experiments (Section~\ref{sec:tcp.testbed}) where we measure both the RTT and the bytes in flight at UE for two large commercial LTE networks. As shown in Figure~\ref{fig:rtt.bif.linux} and~\ref{fig:rtt.bif.verizon}, the trend that RTT grows with the number of in-flight packets is obvious. Our observation is also consistent with a recent study~\cite{jiang12} that shows the usage of large buffers in today's cellular networks may cause high queuing delays. In addition to that, we further demonstrate its prevalence in today's LTE networks: as shown in Figure~\ref{fig:bytes.in.flight}, which plots the distribution of downlink in-flight bytes for large flows ($>$ 1MB), about 10\% of measured instances have in-flight bytes greater than 200 KB, potentially leading to long queuing delays.

Clearly, for short flows or traffic triggered by user interactions (\eg web browsing), queues are not likely to build up. For long-lived flows, usually it is the throughput instead of latency that matters. However, when short-lived and long-lived flows coexist (\eg performing browsing while streaming in the background), queuing delay may severely deteriorate user experience by introducing unacceptable delays for short flows. Moreover, as a new observation, we found that a high downstream queuing delay may often cause TCP's congestion window to collapse upon a single packet loss. We discuss this newly identified and rather severe issue in Section~\ref{sec:tcp.tcp}.

%Existing solutions to bufferbloat, such as delay-based TCP congestion control (\eg TCP Vegas) and setting a smaller receive window size are suboptimal due to under-utilizing the available bandwidth, as suggested by a recent study~\cite{jiang12}, which also shows such a tradeoff could be better balanced by changing the receive window size dynamically. \fengcomment{Don't mentioned this again in~\S\ref{sec:tcp} and~\S\ref{sec:estimate}}

\textbf{Retransmission Rate.} We study TCP downlink retransmission rate, defined as the number of retransmitted packets divided by all packets, across all downlink flows in our data set. 38.1\% of the flows have retransmission rates of zero, and the median value is only 0.06\%. Such low retransmission rates are comparable to those in wired networks~\cite{qian09}. There are even fewer packet losses since the retransmission rate is an upper bound of the packet loss rate in the downstream (\ie between UE and the monitor, note we are not able to capture losses occurring on the upstream side of the monitor). In fact, in cellular networks, most transport-layer losses are concealed by the physical/MAC-layer retransmission and reduced by buffering.
In particular, buffers in LTE networks upstream from the airmile can play an important in absorbing the burstiness of the
traffic transmitted over the lossy wireless link, helping achieve a low loss rate.

%In particular, large buffers in LTE networks help absorb the burstiness of the traffic transmitted over the lossy wireless link, helping achieve a low loss rate.

%\fengcomment{Figure~\ref{fig:retran} can be removed if run out of space. Can just mention in text. }
%\comment{X-axis label should be retransmission rate, not loss rate}

\nsubsection{Comparison to Previous Studies}
\label{sec:tcp.compare}

\begin{table*}[t]
\scriptsize
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Study         & Our Results       & 3GTest~\cite{mobisys.3gtest}  & \MC{4GTest~\cite{huang_mobisys12}} & \multicolumn{6}{|c|}{SpeedTest~\cite{sommers12}}\\
Time          & Oct 2012      & Aug-Dec 2009               & \MC{Oct-Dec 2011}               & \multicolumn{6}{|c|}{Feb 21 - Jun 5 2011 (15 weeks)}\\
\cline{4-11}
Location      & 1 US City & Across U.S.                   & \MC{Across U.S.}                   & \MC{New York City}    & \MC{Madison WI} & \MC{Manchester UK}\\
\cline{4-11}
Type          & LTE Only          & Four 3G ISPs                  & LTE   & WiMAX                      & Cellular & WiFi      & Cell' & WiFi   & Cell' & WiFi\\
\hline
5\% TCP DL$^*$& 569    & 74 - 222$^{\dag}$    & 2112     & 431     & 108      & 404        & 99       & 347     & 28  & 267\\
50\% TCP DL   & 9185     & 556 - 970          & 12740    & 4670    & 1678     & 7040       & 895      & 5742    & 1077     & 4717\\
95\% TCP DL   & 24229     & 1921 - 2943        & 30812    & 10344   & 12922    & 17617      & 3485     & 14173   & 3842     & 15635\\
\hline
5\% TCP UL    & 38                & 24 - 52            & 387      & 172     & 52       & 177        & 55       & 168     & 25       & 180\\
50\% TCP UL   & 2286              & 207 - 331          & 5640     & 1160    & 772      & 2020       & 478      & 1064    & 396      & 745\\
95\% TCP UL   & 8361              & 434 - 664          & 19358    & 1595    & 5428     & 10094      & 1389     & 5251    & 1659     & 5589\\
\hline
5\% HS RTT    & 30                & 125 - 182          & 37       & 89      & 68       & 21         & 99       & 24      & 98       & 34\\
50\% HS RTT   & 70                & 160 - 200          & 70       & 125     & 159      & 54         & 184      & 69      & 221      & 92\\
95\% HS RTT   & 467               & 645 - 809          & 127      & 213     & 786      & 336        & 773      & 343     & 912      & 313\\
\hline
\end{tabular}
\\
\begin{tabular}{l}
{$^*$ TCP DL: downlink throughput (kbps). TCP UL: uplink throughput (kbps). HS RTT: TCP handshake RTT (ms). 5\%, 50\%, 95\%}\\
{\ \ \ are percentiles.}\\
{$^{\dag}$ For a range $x$\ --\ $y$, $x$ and $y$ are the result of the worst and the best carriers, respectively, for that particular test.}\\
\end{tabular}
\ncaption{Comparing with previous measurement studies}
\label{table:tcp.compare}
\end{table*}


We compare our results with three previous measurement studies, focusing on three important metrics: TCP downlink throughput, TCP uplink throughput, and TCP handshake RTT.
The 3GTest study~\cite{mobisys.3gtest} deployed an app that measures network performance metrics on users' handsets. Their data consisted of 35K cellular (3G only) tests from customers of four large U.S. cellular carriers in late 2009. The 4GTest study~\cite{huang_mobisys12} adopts a similar approach while focusing on LTE users. Its data comprises of about 1K LTE tests and a few WiMAX tests across the U.S. in late 2011. A recent study~\cite{sommers12} examined a 15-week dataset from \texttt{speedtest.net} in 2011. Table~\ref{table:tcp.compare} shows their reported performance metrics for handheld device users from three locations: New York City (246K Wi-Fi tests / 79K cellular tests), Madison Wisconsin U.S. (24K Wi-Fi / 4K cellular), and Manchester U.K. (291K / 31K). The cellular technology ranges from 2G EDGE to 4G LTE, but is dominated by 3G (UMTS/EvDO/HSPA).

%Another study~\cite{canadi12} examined another large \texttt{speedtest} dataset to investigate today's broadband performance for all types of end hosts. Table~\ref{table:compare} considers a small subset of their data \ie 4.4 million tests from wired and wireless Internet users at Los Angeles, CA from Jun to Nov 2011.

We discuss three major issues that may affect the comparison. First, all three previous studies perform throughput measurement using bulk data transfer of a large file without any pause while our flows may consist of idle time periods (\eg due to user think time), leading to a lower throughput. To obtain more fair comparison, here we only consider large non-PEP flows in our dataset (with at least 200 KB for uplink and 1 MB for downlink) with no visible idle time period (with maximum inter-packet time of less than 1 second, which is larger than 99.9$^{th}$ percentile of RTT). Second, in our case, remote servers may impose rate limit~\cite{gerber10} while all previous studies perform active probing using dedicated test servers without any limitation on throughput. Third, we infer performance metrics from traces of real Internet servers, while 3GTest, 4GTest, and SpeedTest employ different server selection policies: 3GTest uses a single server located in U.S. while SpeedTest picks a server geographically close to the UE. This in particular affects the latency estimation.

The comparison results are shown in Table~\ref{table:tcp.compare}. Despite aforementioned differences among diverse measurement approaches, we believe the comparison can still demonstrate the advantage of LTE over other types of cellular access technology, since their performance difference is quite significant: the median downlink throughput, uplink throughput, and handshake RTT are 9.5x, 6.9x, and 0.43x compared with the median values of the best U.S. 3G carrier in 2009, respectively. Compared with the 2011 New York City cellular results, the ratios are 5.5x, 3.0x, and 0.44x for DL throughput, UL throughput, and RTT, respectively. Moreover, on mobile devices, LTE also outperforms Wi-Fi in many cases. Specifically, for the 5$^{th}$/50$^{th}$/95$^{th}$ percentiles of downlink throughput and the median uplink throughput shown in Table~\ref{table:tcp.compare}, LTE performs better handheld Wi-Fi. Based on Table~\ref{table:tcp.compare}, LTE's latency appears higher than that of Wi-Fi. However, recall that Speedtest always picks a nearby test server while we are measuring the RTT between UE and real servers that may be far away. This may lead to an unfair RTT comparison. Furthermore, our performance values are consistently lower than those reported by LTE tests of 4GTest, very likely due to the rate limiting imposed by remote servers as mentioned before. A recent study~\cite{gerber10} indicates such rate limiting is prevalent across today's Internet servers. We also observe that LTE significantly outperforms WiMAX in all three metrics.


\nsection{Abnormal TCP behavior}
\label{sec:tcp.tcp}

Due to their resource usage, we focus on \emph{large flows} defined to be relatively long flows, with more than 5 seconds data transfer time, and total downlink payload exceeding 1MB. These large flows account for only 0.3\% of all TCP flows in our data set, but their total downlink payload contributes to  47.7\% of all downlink payload.

\begin{figure}[h]
\centering
\IG{figures/traffic/dupack.eps}\\
\ncaption{Observed duplicate ACKs and packet reordering in large TCP flows}
\label{fig:dup.ack}
\end{figure}

As a background, upon receiving an out-of-order unacknowledged segment, a TCP receiver sends an immediate duplicate ACK~\cite{rfc5681}. From the sender's perspective, duplicate ACKs can be caused by reordering or loss. Therefore, when there is a large amount of bytes in flight and one data segment $S$ is lost, each data segment with sequence number higher than that of $S$ triggers a duplicate ACK, before a retransmission of $S$ is successfully received. So a long sequence of duplicate ACKs strongly suggests a packet loss. When TCP detects 3 duplicate ACKs, it infers a data packet loss and retransmits it according to the fast retransmit~\cite{rfc5681}. In the monitor traces, we detect this behavior as the data packet sent by fast retransmit is out-of-order relative to other packets.

Figure~\ref{fig:dup.ack} summarizes the observation of duplicate ACKs and packet reordering in the large TCP flows. Although the median of duplicate ACKs in large flows is 17, for over 29.0\% of large flows, there are over 100 duplicate ACKs. We observe that the number of out-of-order data packets in large flows is substantially smaller than that of duplicate ACKs, with a median value of only 2. By studying the ratio between duplicate ACKs and out-of-order data packets, 24.7\% of flows have a ratio of over 25, and for some flows, this ratio can reach up to 5,000. This indicates that even a single out-of-order data packet can trigger a large number of duplicate ACKs when the bytes-in-flight are large, using up more uplink bandwidth.

%, and if the corresponding ACK of the retransmitted segment arrives at the sender within the
Fast retransmission allows TCP to directly send the lost segment to the receiver possibly preventing retransmission timeout (RTO). If so, TCP would resume data transfer with the congestion window size reduced by half. However, as shown earlier, we identified significant queuing build up between UE and monitor. Such large in-network queues capable of holding up to a few megabytes data could delay the receipt of the retransmitted data packet. In the meanwhile, if TCP does not use duplicate ACKs to update RTO, a timeout is likely to happen. If the corresponding ACK does not arrive at the server within the RTO (timeout), the congestion window would drop to 1 segment, triggering slow start, significantly hurting TCP performance. We refer to this as the \emph{undesired slow start} problem.

\begin{figure}[t]
\centering
\IG{figures/traffic/seq_ack_dupack.eps}\\
\ncaption{Duplicate ACKs not triggering a slow start}
%\ncaption{One real sample flow, dup ACK does not trigger slow start \comment{break down cases where slow start happens and not} iPhone, support SACK. Last RTT sample before duplicate ACK, 371ms.RTT for the fast retransmitted packet, 68ms. RTT has high variation, so RTO is relatively larger}
\label{fig:seq.ack1}
\end{figure}

\begin{figure}[t]
\centering
\IG{figures/traffic/seq_ack_dupack2.eps}\\
\ncaption{Duplicate ACKs triggering a slow start}
%\ncaption{One real sample flow, dup ACK DOES trigger slow start. Android Support SACK. Last RTT sample before duplicate ACK, 262ms.RTT for the fast retransmitted packet, 356ms. RTO < 356.\comment{ECO, ECN?}}
\label{fig:seq.ack2}
\end{figure}

Figures~\ref{fig:seq.ack1} and~\ref{fig:seq.ack2} demonstrate two sample cases in the data set, where Figure~\ref{fig:seq.ack1} shows that the train of duplicate ACKs does not trigger slow start and Figure~\ref{fig:seq.ack2} includes a case that slow start is triggered. One key difference is that Figure~\ref{fig:seq.ack2} has about 500KB bytes in flight before the first duplicate ACK, while Figure~\ref{fig:seq.ack1} has much fewer bytes in flight.

In TCP, RTO is computed by the sender using smoothed round-trip time and round-trip time variation~\cite{rfc6298}. However, using duplicate ACKs to update RTO, which may be beneficial by allowing more accurate RTT estimation, is not standardized. In Figure~\ref{fig:seq.ack2}, between 1.1s and 1.5s, the sender receives many duplicate ACKs. Due to the growing queueing size, RTT grows from 262ms (the last RTT sample before the first duplicate ACK) to 356ms, the RTT for the retransmitted packet. The sender's TCP implementation apparently ignores these duplicate ACKs for updating RTO and RTO remains the same without considering the duplicate ACKs. Following the method for calculating RTO~\cite{rfc6298}, we observe that RTO is around 290ms before the first duplicate ACK, which is smaller than the RTT of the retransmitted packet (356ms). This problem does not happen in Figure~\ref{fig:seq.ack1}, because the RTT before the first duplicate ACK is close to that after the last duplicate ACK, due to the small number of bytes in flight. Although it is recommended that the RTO should be at least 1 second~\cite{rfc6298}, depending on the operating systems, different minimum values are used, \eg Linux's minimum RTO is 200ms~\cite{linuxrto}. Such small values of RTO can exacerbate the undesired slow start problem demonstrated in Figure~\ref{fig:seq.ack2}.

We also study the prevalence of the congestion window collapse problem. To tell whether there is a slow start following a long list of duplicate ACKs, we use a heuristic metric $R_{ss}$, ratio of slow start:
$R_{ss} = \frac{\theta_{[100, 200]}}{\theta_{[0, 100]}}$,
where $\theta_{[t_{1}, t_{2}]}$ is the average downlink throughput from $t_{1}$ ms to $t_{2}$ ms after the last duplicate ACK. We choose 200 ms empirically as it is observed to be shorter than a typical slow start in the LTE networks. During slow start, $R_{ss}$ is expected to be larger than that when there is no slow start. For example, the $R_{ss}$ is 1.0 for Figure~\ref{fig:seq.ack1} and $R_{ss}$ is 3.7 for Figure~\ref{fig:seq.ack2}. In practice, we observe that 1.5 is a good threshold for $R_{ss}$ in determining slow start. Using this threshold, we have determined that for all the large TCP flows with at least one lost data packet, 20.1\% of them suffer from the slow start problem, which consists of 12.3\% of all large TCP flows. In one case, a 153-second flow even experience 50 slow starts, resulting in an average throughput of only 2.8Mbps, while the estimated bandwidth actually larger than 10Mbps.
%, \ie if $R_{ss} \geq 1.5$, we classify it as slow start
%\comment{Also discuss the fact that during the DUP ACK time, 1 is still sending data, due to the congestion window inflation by the duplicate ACKs, 2 is not sending data, because congestion window is full, especially after timeout of the retransmitted packet, congestion window is set to be 1 full-sized segment.} Based on this observation, we propose a patch to the existing TCP RTT estimation algorithm in the LTE networks, that the sender should keep track of the bytes in flight $B_{if}$ and take it into consideration for RTT estimation. \comment{how does existing TCP RTT works, can we make head-to-head comparison?}

There are different ways to mitigate this problem. One approach is to update RTO with the help of duplicate ACKs with TCP Selective Acknowledgment options (SACK)~\cite{rfc2018}. Assuming there is no packet reordering, by taking the difference between the SACK window of two consecutive duplicate ACKs, we can identify the exact data packets corresponding to these ACKs. If there is ambiguity, either due to ACK reordering or additional packet loss, we simply ignore this sample. In our data sets, packet reordering rate is less than 1\% and SACK is enabled in 82.3\% of all duplicate ACKs, making this approach promising.

If SACK is disabled, we can use a fall-back approach to estimate RTT based on duplicate ACKs by assuming that they are in response to the data packets sent out in order. This assumption holds in most cases as the packet reordering rate is low. Using these approaches, we can obtain RTT estimations for duplicate ACKs and update RTO accordingly, which effectively prevents the timeout of retransmitted packet due to queueing delay. Note that the RTT estimation method used in TCP Vegas~\cite{tcp.vegas} with help of the TCP Timestamps option is not applicable to duplicate ACKs, since the echo timestamps of all duplicate ACKs are all the same, which is the timestamp of the segment before the lost segment, rather than the timestamp triggering the duplicate ACK. Our initial analysis shows that these two approaches are able to prevent more than 95\% of the slow starts. From the mobile network operators' perspective, one simple solution for this problem could be prioritizing the retransmitted packet. The retransmitted packet could be inferred by tracking the TCP flow status. However, the security and performance implications of this approach are yet to be studied.

\nsection{Bandwidth Estimation}
\label{sec:tcp.estimate}

In order to understand the network utilization efficiency of existing applications in the LTE networks, we first need to know the available bandwidth for each user. Previous work on active bandwidth measurement methodology to estimate available bandwidth, \eg using packet pairs, packet trains, and parallel TCP connections~\cite{prasad03, jain02, hu04}, do not apply here. As existing studies have shown that network condition is highly variable in cellular networks~\cite{mobisys.3gtest}, active probing would require us launch measurements for each user in our data set at the time of trace collection. Using packet traces collected at the monitor, we instead devise a \emph{passive} bandwidth estimation algorithm to capture the available bandwidth for each user using TCP flows that may \emph{not} fully utilize the bandwidth.

\nsubsection{Bandwidth Estimation Algorithm}
\label{sec:tcp.estimation}

\begin{figure}[t]
\centering
\IG{figures/traffic/algorithm.eps}\\
\ncaption{Typical TCP data transfer}
\label{fig:algo}
\end{figure}

Figure~\ref{fig:algo} illustrates a typical TCP data transfer. Our monitor lies between the server and the UE, and we only use packet traces collected at the monitor for the analysis. The high-level idea for our bandwidth estimation algorithm is to select a time window during which the sending rate is fast enough (to be discussed later) and calculate the UE receiving rate. Note that the UE receiving rate is not always equal to the server sending rate, \eg when the in-network buffers fill up, the sending rate might exceed the receiving rate. The idea behind our bandwidth estimation algorithm is to use the TCP Timestamps option to calculate receiving rate.

We use Figure~\ref{fig:algo} to illustrate our bandwidth estimation algorithm. At $t_{2}$, UE sends an ACK in response to the two data packets $P_{1}$ and $P_{2}$. And similarly, at $t_{6}$, the ACK for $P_{n-1}$ and $P_{n}$ is sent. From the monitor's traces, we observe that $n-2$ data packets ($P_{3} \cdots P_{n}$) sent to the UE in a time window between $t_{0}$ and $t_{4}$. Assuming the average payload size of these $n-2$ packets is $S$, the sending rate between $t_{0}$ and $t_{4}$ is
\begin{equation}
\label{eq:snd}
R_{snd} = \frac{S (n - 2)}{t_{4} - t_{0}}
\end{equation}
And from the UE's perspective, the receiving rate for these $n-2$ packets is
\begin{displaymath}
R_{rcv} = \frac{S (n - 2)}{t_{5} - t_{1}}
\end{displaymath}

Typically, $t_{2}$ is very close to $t_{1}$ and similarly $t_{5}\approx t_{6}$. In our controlled lab experiments, for a 30-minute continuous trace, the median value of the delay between a data packet and the corresponding ACK is negligible: 0.3ms. However, such delay, \eg $t_{2} - t_{1}$, could be large in some rare cases. Typically, one ACK in TCP is for two data packets and when there is only one data packet pending acknowledgement, the receiver may delay sending the ACK by up to 500 ms, which is known as the delayed acknowledgement mechanism~\cite{rfc1122}. In our example, if $P_{n-1}$ has already been acknowledged by another ACK and after $t_{5}$ there are no more data packets arriving at the UE side, the ACK for $P_{n}$ could be delayed. For simplicity, we ignore the cases when the last ACK is acknowledging only one data packet, indicating it might be a delayed ACK. We also do not consider cases with out-of-order data packets or duplicate ACKs in the time window for bandwidth estimation, as there may be ambiguity in packet timing. Then we know
\begin{displaymath}
R_{rcv} \approx \frac{S (n - 2)}{t_{6} - t_{2}}
\end{displaymath}
If the uplink delay from the UE to monitor is stable, we can assume $t_{6} - t_{2} = t_{7} - t_{3}$. However, as shown previously that RTT could be significantly affected by the bytes in flight and thus the assumption of $t_{6} - t_{2} = t_{7} - t_{3}$ may not hold. Instead, we use TCP Timestamps option~\cite{rfc1323} to calculate $t_{6} - t_{2}$. Specifically, the ACKs from the UE to the server may contain the Timestamp Value field ({\em TSval}), which contains the current value of the UE's timestamp clock. The unit for {\em TSval} is clock tick and for different devices, the actual time per clock tick could be different. Assume for each clock tick, the corresponding time is $G$ milliseconds and $G$ can be treated as a constant for the same device. Assuming $G$ is known, we can estimate $R_{rcv}$ as
\begin{equation}
\label{eq:rcv}
R_{rcv} \approx \frac{S (n - 2)}{G(TS_{2} - TS_{1})}
\end{equation}
where $TS_{1}$, $TS_{2}$ are the \emph{TSval} in the two corresponding ACKs.

Our bandwidth estimation algorithm only requires ACKs from the UE having TCP Timestamps option enabled, with no requirement on servers and in our data set, for 92.61\% of the TCP flows, this requirement is satisfied.
%We also check the server's support and observe a lower support rate of 79.35\%.

We infer the $G$ value using a similar method from previous work~\cite{imc.ttfb}. Using the example in Figure~\ref{fig:algo}, we have
\begin{equation}
\label{eq:g}
G \approx \frac{TS_{2} - TS_{1}}{t_{7}-t_{3}}
\end{equation}

\begin{figure}[t]
\centering
\IG{figures/traffic/g_infer.eps}\\
\ncaption{$G$ inference and the selection of $\delta_{G}$}
\label{fig:g}
\end{figure}


We require $t_{7} - t_{3}$ to be large enough, because clock tick is discrete, not continuous, and if it is too small, \eg when $t_{7} - t_{3} = 0.6G$ and $TS_{2} - TS_{1} = 1$, the calculated value is actually $0.6G$. Also, there might be variations in uplink one-way delay from UE to monitor, and such variation is only negligible in calculating $G$ when $t_{7} - t_{3}$ is large enough. Assuming the threshold for $t_{7} - t_{3}$ to calculate $G$ with small error rate is $\delta_{G}$, we present the error curve for calculating $G$ value of two UEs in Figure~\ref{fig:g} based on controlled experiments. The actual $G$ values for device 1 and 2 are measured at the UE side using 30-minute long traces. We observe that the error for $G$ inference drops drastically as $\delta_{G}$ increases and we conservatively select $\delta_{G} = 3$ seconds, incurring less than 0.1\% error. Note that the error curve may be different for different $G$ values. In our data set, among all large flows, except for 5.93\% of them without UE Timestamps option enabled, 57.33\% have $G\approx 1$ms/tick,  36.37\% have $G\approx 10$ms/tick, and the rest 0.37\% have other values, \eg $G\approx 100$ms/tick. With $\delta_{G} = 3$ seconds, we estimate the inferred $G$ has less than 0.1\% error for majority of the flows.

\paragraph{Summary: } for a target TCP flow, if $G$ value is not known, the initial $\delta_{G} = 3$ seconds is used to infer $G$ using Formula~\ref{eq:g}, with two sample packets more than $\delta_{G}$ apart selected. Flows without UE TCP Timestamps option support are ignored. Then the algorithm scans for a window with high sending rate $R_{snd}$ calculated by Formula~\ref{eq:snd}. If $R_{snd} \geq C$, a pre-known constant of the maximum possible available bandwidth in the studied network, and there is no out-of-order data packets or duplicate ACKs within the time window for estimation, and the last ACK of this window is not a delayed ACK, one bandwidth estimation sample is obtained using Formula~\ref{eq:rcv}. The selection of $C$ is important, \ie if $C$ is too small, the bandwidth may be underestimated, as small $R_{snd}$ may be chosen; if $C$ is too large, we may not be able obtain many estimation samples. We conservatively choose $C$ = 30Mbps, which is verified to be higher than the rate of most flows, and in the meanwhile allows us to predict bandwidth for over 90\% of the large downlink flows during our analysis.

%This bandwidth estimation algorithm is not applicable for TCP flows that are too short, especially when $G$ value is not pre-known. Also, since the requirement on $R_{snd}$ is high, it is hard to find proper bandwidth estimation samples for short flows without passing the slow start stage.

In addition to downlink bandwidth, our algorithm is also applicable to uplink bandwidth estimation, by interchanging the UE and the server in Figure~\ref{fig:algo}. However, some tuning is required, \eg the choice of $C$ and $\Delta_{G}$, \etc Similarly, our bandwidth estimation algorithm also works in other network types, such as 3G, WiFi and even wired networks, with proper parameter settings.

Although the described algorithm is based on one single TCP flow per user, a similar idea can be applied to multiple concurrent flows per user by summing up the predicted bandwidth for different flows. As long as we ensure that the total sending rate for all concurrent flows are larger than $C$, the total receiving rate would be the accurate estimation of the available bandwidth. In this work, we focus on the application of our bandwidth algorithm for the downlink traffic (UEs downloading contents from servers) in the LTE networks for single TCP flows, \ie with no competing downlink flows for the same user.

\nsubsection{Validation with Local Experiments}

To validate the bandwidth estimation algorithm, we use controlled experiments with the setup described in Section~\ref{sec:tcp.testbed}.

\begin{figure}[t]
\centering
\IG{figures/traffic/bw_validation.eps}\\
\ncaption{Time series of bandwidth estimation for LTE network (controlled lab experiments )}
\label{fig:bw.time}
\end{figure}

\begin{figure}[t]
\centering
\IG{figures/traffic/bw_validation_cdf.eps}\\
\ncaption{CDF of bandwidth estimation results for LTE network (controlled lab experiments )}
\label{fig:bw.cdf}
\end{figure}

Figure~\ref{fig:bw.time} shows the UE-perceived throughput over 30-minute duration, as well as the absolute error for the estimated bandwidth. The actual throughput fluctuates frequently around 10Mbps and error fluctuates within $\pm2$Mbps in most cases. In Figure~\ref{fig:bw.cdf}, we compare the distribution of estimated bandwidth calculated from the server-side packet traces with actual used bandwidth calculated from UE-side packet traces. We can see that the estimated bandwidth curve is very close to the actual throughput curve. Based on UE traces, we can select different window size to calculate actual throughput. For each window, we also get one bandwidth estimation sample whose timestamp is closest to the center of that window, and we compare this sample with the average actual throughput to obtain an error sample. The error distribution for two window sizes, \ie 1.0s and 0.1s, are shown in Figure~\ref{fig:bw.cdf}. For 1.0-second window, the average error is 7.92\% and for 0.1s window, the UE throughput has higher variation and the average error is slightly higher. Note that the term ``error'' here is relative to the actual throughput observed from UE-side traces, which itself might not be the actual available bandwidth, and the true error rate for our estimation algorithm could be even smaller.

\nsubsection{Bandwidth Utilization by TCP Flows}

In this section, we analyze the LTE traffic data set to understand network utilization efficiency of TCP flows.
As shown in Figure~\ref{fig:concurrency} (\S\ref{subsec:char_flow}), most users have only one TCP flow actively downloading data. This allows us to 
focus on applying our bandwidth estimation algorithm to single TCP flows with no competing downlink flows from the same user.

\begin{figure}[t]
\centering
\IG{figures/traffic/bwe.eps}\\
\ncaption{Bandwidth utilization ratio for large downlink TCP flows}
%\comment{add one curve for variations, calculate the time saved if using full bandwidth, plot of saved time}
\label{fig:bwe}
\end{figure}

We then apply the bandwidth estimation algorithm to the large TCP downlink flows that are not concurrent with other large flows and summarize the results in Figure~\ref{fig:bwe}. We use a time window size of 250ms and for each window, we take one bandwidth estimation sample that is closest to the center of the window. For some flows, there exist windows that do not contain any valid bandwidth estimation samples and we simply assume the bandwidth distribution for these unknown windows is the same for the known windows. The possible bias will not likely significantly affect our analysis as such unknown duration accounts for less than 20\% of the total flow duration. For each flow, we use the average value of all bandwidth estimation samples as the estimated bandwidth and compare it with the actual utilized bandwidth.

%For Figure~\ref{fig:bwe}, the median estimated bandwidth among all large flows is 12.20Mbps and the median used bandwidth is only 1.49Mbps. 
For Figure~\ref{fig:bwe}, we plot the ratio of used bandwidth to estimated bandwidth per flow. The median value is only 19.78\%. For 71.26\% of the large flows, the bandwidth utilization ratio is below 50\%. We notice that for 6.41\% of the flows, the used bandwidth is slightly larger than the estimated bandwidth and this is possibly due to estimation error. On average, the utilization ratio is 34.60\%, meaning that the data transfer for a TCP flow takes 52.91\% longer than if the bandwidth is fully utilized, while keeping the ratio interface in the high-power state, incurring significant additional radio energy overhead~\cite{huang_mobisys12}.


\begin{figure}[t]
\centering
\IG{figures/traffic/bwe_sample.eps}\\
\ncaption{Bandwidth estimation timeline for two sample large TCP flows}
\label{fig:bwe.sample}
\end{figure}

Figure~\ref{fig:bwe.sample} shows two sample large TCP flows and their estimated bandwidth in the LTE data set. Note that these flows belong to two separate users at different time and the time is aligned only for presentation purposes. We can see that the available bandwidth varies significantly over time and even on the scale of seconds. This could be due to network condition changes, such as signal strength, as well as the changes of the network load in the associated eNB. In order to dissect the root cause of such variability, more information, \eg load information of eNB, is needed.

To understand how well TCP performs under highly variable available bandwidth, we use \texttt{iptables} to redirect packets to a packet scheduler we designed, which controls the variation of available bandwidth following the variations observed in LTE networks. The packet scheduler also adds different delays to each packet allowing us to understand the impact of RTT. Intuitively, TCP would adapt slower to the fast varying available bandwidth under large RTTs during congestion avoidance, as the congestion window is updated only once per RTT. We measure the bandwidth utilization ratio with the packet scheduler changing the available bandwidth every 500ms. We observe that under small RTTs, TCP can utilize over 95\% of the available bandwidth. However, when RTT exceeds 400$\sim$600ms, the utilization ratio drops to below 50\%. We also observe that for the same RTT, higher variation leads to lower utilization. These observations further suggest that long RTTs can degrade TCP performance in the LTE networks, which have inherently varying available bandwidth (likely caused by changes in load and channel conditions).





\nsection{Network Applications in LTE}
\label{sec:tcp.app}

In this section, we characterize the network applications and traffic patterns in the LTE data set. Specifically, we study the causes of inefficient bandwidth usage observed in Section~\ref{sec:tcp.estimate}.


\nsubsection{HTTP Content Characterization}
\label{sec:http_content}

\begin{figure}[t]
\centering
\IGM{figures/traffic/content_type_new.eps}\\
\ncaption{Breakdown of content type for all HTTP traffic based on total content size}
\label{fig:content.type}
\end{figure}

We break down the content type based on total bytes for HTTP traffic in Figure~\ref{fig:content.type}.
About 37.8\% is video, followed by 19.5\% of image and 11.8\% of text. Zip contributes to 8.3\% of the HTTP traffic and we observe that these mostly correspond to file downloads, such as application updates, and audio contents consume 6.5\%. The other files consist of 5.6\% , and the remaining 10.5\% is unknown. Within video contents, we observe 12.9\% to be \emph{octet-stream} type, \ie byte stream in binary format, and most of them generated by video players via byte-range requests.

Previous studies show that the multimedia content (video and audio) correspond to 40\% of the traffic generated by mobile hand-held devices in DSL networks~\cite{maier10}, and video contributes to 30\% of the 3G cellular traffic~\cite{erman11}. Although we observe slightly higher percentage of multimedia traffic in this LTE network, the difference is insignificant. Overall, we observe multimedia content is still dominant in the LTE network studied followed by image content.
%The median for the download size of video flows is 777.3KB

\nsubsection{Inefficient Network Usage}


We investigate the large flows with under-utilized network bandwidth and observe that the TCP receive window size~\cite{rfc1323} has become the bottleneck in many cases.

\begin{figure}[t]
\centering
\IG{figures/traffic/seq_ack_shazam_local.eps}\\
\ncaption{Full receive window slows Shazam player (a popular app) in downloading a 30-second music file}
\label{fig:shazam}
\end{figure}

Figure~\ref{fig:shazam} shows one such example: an iOS user uses the popular Shazam application~\cite{shazam} to download a 30-second music file of 1MB size. Initially, the data transfer speed is fast and between time 0s and 2s the average downlink throughput is over 3Mbps. However, between 2s and 9s, the average throughput decreases to less than 300Kbps. The total download time is 9 seconds and as indicated by the \emph{ideal case} curve, the download could have been completed within 2.5 seconds, based on our estimation of the available bandwidth. In addition, we notice that the TCP connection is not immediately closed after the file transfer is complete, although the HTTP request specifies the connection type to be \texttt{Connection: close}. Instead, the connection is only torn down at 30s, after the music clip has finished playing, and the client sends some TCP receive window updates to the server between 20s and 25s with increasing receive window size. Ideally, the connection is closed immediately after the file transfer completion. Overall, the total download process keeps the radio interface of the device active for around 38 seconds. Assuming a tail time of 10 seconds, in the ideal case, the active radio time is only 12.5 seconds.

The performance drop at 2s in Figure~\ref{fig:shazam} is due to the full TCP receive window, \ie byte in-flight fully occupy the window size. Between 0s and 2s, the window size has gradually dropped to a small value, \eg at the turning point around 2s, the window size is 816 bytes, even smaller than the maximum payload size  (1358 bytes in our traces). As TCP rate is controlled by both congestion window and receive window jointly, the full receive window would prevent the server from sending more data, regardless of the congestion window, leaving the network resource underutilized.

The reason for the full TCP receive window is two-fold. First, the initial receive window size is not large, \eg 131.76KB in the case of Figure~\ref{fig:shazam}, much smaller than the file size. We explore the initial advertised receive window size in all TCP flows, and observe that the values fall in 131,712$\pm$600 bytes for over 99\% of the flows, for iOS, Android and Windows Phone devices. Second, the application is not consuming the data fast enough from the receiver's buffer, otherwise, even if the initial receive window is small, the receive window size should not drop to close to 0 afterwards.

We further study the prevalence of such TCP performance throttling by the receive window. We find that for all the downlink TCP flows, 52.61\% of them experienced full receive window or even zero receive window. And for 91.24\% of these affected flows, throttling of receive window happens in the initial 10\% of the data transfer duration. These observations suggest that about half of the TCP flows in the LTE network studied are experiencing lower network performance limited by receive window size.

\begin{figure}[t]
\centering
\IG{figures/traffic/netflix.eps}\\
\ncaption{The periodic request behavior of Netflix player limiting its overall throughput}
%\comment{HLS, http live streaming, this is iOS, for Android, it is different}
\label{fig:netflix}
\end{figure}

We also observe that some applications under-utilize the bandwidth due to the application design. Figure~\ref{fig:netflix} shows the network behavior of the popular Netflix application~\cite{netflix} on iOS. The upper half of the figure shows the HTTP requests and the HTTP object transfers on different client ports. At around 70s, the user browses through a list of video thumbnails and switches to another video. We observe that all HTTP requests for video download are HTTP byte-range requests and the corresponding HTTP object transfers are mostly short in duration, \ie smaller than 1s. The content size of the requests ranges from 1MB to 4MB. The client periodically requests chucks of data every 10s, with each TCP connection typically reused by two consecutive HTTP requests. The bottom half of the figure shows the aggregate downlink throughput, showing a clear periodic pattern corresponding to the periodic requests. While the peak throughput can reach up to 30Mbps, for most of the time, the network interface is idle. This type of traffic pattern is known for incurring high tail energy~\cite{qian12_www}. Especially in this case, we know that the tail timer for the studied network is 10s, and based on the radio resource control RRC state machine of the LTE networks~\cite{huang_mobisys12}, the 10-second request cycle would keep the radio interface of the device always in the high-power state, incurring high energy overheads.

\nsubsection{Discussions}
\label{sec:discussion}

We have shown that multimedia traffic is dominant in the LTE network studied and the available bandwidth is far from effectively utilized by many applications. Hence optimizing the network utilization efficiency for these applications is critical for better user experiences, in terms of both responsiveness and energy consumption.

About the TCP receive window problem, existing studies~\cite{jiang12} have shown that smartphone venders may have been reducing maximum receive window size to mitigate the buffer bloat problem, resulting in TCP performance degradation. Dynamic receive window adjustment (DRWA)~\cite{jiang12} is proposed towards this problem. However, such proposals require changes to TCP stacks, making the deployment potentially challenging. Also, the potential drawbacks for such TCP modifications remain to be studied in the real LTE networks. Alternatively, a more lightweight and yet still effective solution could be that applications have their own buffers, or increase the buffer size if the existing buffer size is small, to consume the bytes in the TCP's receiver buffer quickly. For example, the ideal behavior of the Shazam player described in Figure~\ref{fig:shazam} is that it downloads the file as fast as possible and stores the contents in an application-layer buffer, no matter how much of the music has been played, and closes the connection immediately when the file transfer is complete. The application-layer buffer is beneficial for both network and energy efficiency.

As for the periodical behavior of the Netflix player in Figure~\ref{fig:netflix}, in addition to the application-layer buffer, it is also recommended 
that it sends fewer requests and downloads more content for each request. We have shown that transferring data in a large batch significantly reduces the radio energy than otherwise~\cite{huang_mobisys12}. Bulk data transfers also allow TCP to make better use of the available bandwidth. 

\nsection{Summary}
In this chapter, we use a large-scale LTE data set to study the impact of network protocol and application behaviors on network performance. We show that the RTT in the LTE network studied grows with the TCP bytes in flight due to long queueing delay. Such high RTTs have caused performance problems for TCP upon packet losses. We have also shown that the available bandwidth for the LTE networks has high variation and long RTTs prevent TCP from fully utilizing the bandwidth as the congestion window cannot adapt fast enough. We further notice that the limited TCP receive window size has constrained TCP data rate for 52.61\% of the downlink flows. By devising a bandwidth estimation algorithm, we observe that for 71.26\% of the large flows, the bandwidth utilization ratio is below 50\%. In addition, we also observe that the application design may result in underutilized bandwidth. We have seen that about 44.3\% of the HTTP content is multimedia; therefore, increasing the network efficiency of the corresponding applications is important in improving the application responsiveness and energy efficiency.

