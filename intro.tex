\chapter{Introduction}
\label{chap:intro}

Smartphones with cellular data access have become increasingly popular across the globe, with the wide deployment of 3G and emerging LTE~\cite{3gpp.lte} networks, and a plethora of applications of all kinds. In the third quarter of 2009, the global smartphone shipments reached 41.4 million units~\cite{smartphoneStat}. As of the third quarter of 2012, the global smartphone shipments reached 173.7 million~\cite{smartphoneStat2} with 61.3\% year-on-year increase in average. It is expected that in the next few years smartphone sales will continue to grow.  Vendors, such as Samsung, Apple, and HTC offer a variety of smartphones equipped with increasingly faster CPUs and larger memory, though still lagging behind desktop or laptop systems. With access to various high-speed 3G networks, such as EVDO and UMTS, and the LTE 4G networks, they are powerful enough to run modern operating systems and sophisticated network applications such as web browsing, email, and streaming media. 

Unlike traditional Internet-based applications, whose performance is mostly constrained by the wired network, network application performance on smartphones with limited physical resources also heavily depends on factors including hardware and software on the phone as well as the quality and load of wireless link. Understanding the application performance on smartphones is important for the purpose of assisting consumers in choosing carriers and phones and guiding application developers in designing intelligent software. Moreover, cellular network operators and smartphone hardware and software vendors can use this knowledge to optimize networks and phones for better end-user experiences. Similarly, content providers can leverage this knowledge to better customize content for mobile users. However, this task is quite challenging since the performance of network applications on smartphones is poorly understood thus far, due to a lack of a systematic approach for controlled experiments and comparative analysis, and especially because the network performance of the underlying cellular networks is not well understood. In this thesis, we take one of the first steps to thoroughly study the performance of cellular networks and smartphone applications.

In addition to network performance aspect, we also study the energy footprint for smartphone applications. Today’s cellular systems operate under diverse resource constraints: limited frequency spectrum, network processing capability, and handset battery life. Optimizing energy footprint for smartphone applications is important for end-users. In the meanwhile, optimizing the radio resource usage is of great interest for mobile operators to minimize cost and guarantee quality of service.


\nsection{Characterizing Cellular Network Performance} Since 2008, We have been working on devising systematical methodologies and developing tools for characterizing cellular network performance directly from end users. The tools developed includes \TT~\cite{3gtest}, \FT~\cite{4gtest} and \emph{MobiPerf}~\cite{mobiperf}~\footnote{Notably, \emph{MobiPerf} has received both the \emph{Open Internet App Award} and the \emph{People's Choice App Award} in the \emph{FCC Open Internet Apps Challenge}~\cite{fcc.award}. It is now an open-source project~\cite{mobiperf.repo} that we are actively working on and this project is in joint collaboration among University of Michigan, M-Lab~\cite{mlab} and University of Washington.}, which have cumulatively over 150,000 users from over 190 countries or regions. In these measurement tools, we have devised methods to accurately measure round-trip time (RTT), DNS lookup time, uplink/downlink bandwidth, loss rate, and other network performance metrics for 3G, WiMAX and LTE 4G networks and compare those with WiFi networks.

Our study is among one of the first studies of the network characteristics of commercial LTE networks. Initiated in 2004 by {\em 3rd Generation Partnership Project} (3GPP),  the {\em Long Term Evolution} (LTE), commonly referred to as a type of 4G wireless service, aims at enhancing the {\em Universal Terrestrial Radio Access Network} (UTRAN)  and optimizing radio access architecture~\cite{3gpp.lte}. Since 2009, LTE starts entering the commercial markets and is available now in more than 10 countries, with an expectedly fast-growing user base. The targeted user throughput is 100Mbps for downlink and 50Mbps for uplink, significantly higher than the existing 3G networks, with less than 5ms user-plane latency~\cite{tr25.913}. Understanding the actual user-perceived network performance for LTE network and how it compares with its predecessor 3G and its competitors, \eg WiFi and WiMAX, is important, yet not straightforward. Our forementioned tool \FT, with enhanced measurement design and global server support, allows us to characterize network performance of LTE and other mobile networks~\cite{huang_mobisys12}.


\nsection{Anatomizing Smartphone Application Performance}
In order to understand the key factors that affect smartphone application performance, we develop a systematic methodology for comparing this performance along several key dimensions such as carrier networks, device capabilities, and server configurations~\cite{mobisys.3gtest}. We perform detailed analysis to help carriers, phone vendors, content providers, and application developers gain insight. For example, for carriers, we infer various network level problems, \eg high latency or high loss rate, which they can directly take action on. For phone vendors, we identify performance bottlenecks on the devices or issues associated with the content. These issues can be resolved either independently or by cooperating with content providers. And for application developers, we evaluate factors such as the overhead of HTML rendering and Javascript execution given a particular software configuration. 

%anatomize app performance
Compared with 3G, LTE significantly improves the network performance. Meanwhile, device processing capability and software design have also improved remarkably over the last two years~\cite{mobisys.3gtest}. To understand the potential performance bottleneck shift for smartphone applications, we perform longitudinal case studies of several popular applications on Android, especially for web browsing applications. With the help of CPU, network and power traces, we compare the determinant factors on smartphone applications in 2009~\cite{mobisys.3gtest} and in 2011~\cite{huang_mobisys12}. We identify that the performance bottleneck for web-based applications lies more in the device’s processing power than in the network for the LTE networks.

\nsection{Studying Effect of Network Protocol and Application Behavior on Performance for LTE Networks}
Despite it fast increasing user base, the interplay between mobile applications, protocol and the network for the commercial LTE networks still remain unexplored. We thoroughly study these topics of the LTE network with a data set covering around 300,000 real LTE users in a large metropolitan area for 10 days. We revisit basic network metrics in the LTE network and compare with previously studied network conditions. We also observe that a high downstream queueing delay, likely due to bufferbloat,  has caused TCP congestion window collapse upon one packet loss. With the help of TCP Timestamps option, we have devised a lightweight passive bandwidth estimation algorithm, allowing us to observe that for 71.26\% of the large flows, the bandwidth utilization ratio is below 50\%. We find that TCP may not fully utilize the fast-varying available bandwidth when RTT is large in the LTE network. Upon further analysis, we identify 52.61\% of all downlink TCP flows have been throttled by TCP receive window and data transfer patterns for some popular applications are both energy and network unfriendly.

\nsection{Understanding Power Characteristics of 4G LTE Networks}
Besides higher bit rate, lower latency and many other service offerings for LTE, {\em user equipment} (UE) power saving is an important issue and there has been increasing interest in understanding the power characteristics of LTE networks, compared with 3G/WiFi networks. LTE employs {\em Orthogonal Frequency Division Multiplex} (OFDM~\cite{ts36.211}) technology, which suffers from poor power efficiency. To save power, LTE uplink uses an special implementation of OFDM called SC-FDMA for uplink, with improved power efficiency. {\em Discontinuous reception} (DRX) has been employed by existing wireless mobile networks to reduce UE energy consumption. In UMTS~\cite{ts25.304}, during the idle state, UE periodically wakes up to check paging messages and sleeps for the remaining time. LTE supports DRX for both \RC and \RI modes~\cite{ts36.321}, seeking more opportunities to conserve UE battery. DRX is configured at a per-UE basis and its configuration incurs tradeoff among UE power saving, channel scheduling delay, and signaling overhead.

To understand this tradeoff, existing studies use either total DRX-on time to estimate UE power usage~\cite{vtc.drx, ieee.drx}, or a simplified LTE power model~\cite{iswcs.lte, icc.drx}, which ignores the impact of downlink/uplink data rates. In this paper, we develop the first empirically derived comprehensive power model of a commercial LTE network, which accurately models UE energy usage with less than 6\% error rate. Also, existing studies~\cite{vtc.drx, ieee.drx, iswcs.lte, icc.drx} heavily rely on synthetic packet models instead of real user traces. Our study is the first that leverages a comprehensive real user data set, we call \UMICH, consisting of 5-month traces of 20 smartphone users, to analyze the impact of LTE parameter configuration on realistic application usage patterns. We carefully investigate the energy usage in 3G, LTE, and WiFi networks and evaluate the impact of configuring LTE-related parameters. Despite several new power saving improvements, we find that LTE is as much as 23 times less power efficient compared with WiFi, and even less power efficient than 3G, based on the user traces and the long high power tail is found to be a key contributor.

\nsection{Optimizing Energy Usage in Cellular Networks}
With the knowledge of performance and power characteristics of 3G/4G cellular networks, we study how we can optimize the resource (radio and energy) utilization of smartphone applications. Cellular networks are typically characterized by limited radio resources and significant device power consumption for network communications. The battery capacity of smartphones cannot be easily improved due to physical constraints in size and weight. Hence, battery life remains a key determinant of end-user experience. Given the limited radio resources in these networks and device battery capacity constraints, optimizing the usage of these resources is critical for cellular carriers and application developers. Achieving such energy efficiency for mobile devices when connected to cellular networks without incurring excessive network signaling overhead, even despite diverse application and user behavior, still remains a rather difficult and yet important challenge to tackle. Energy use due to network access, particularly cellular networks, is becoming increasingly dominant due to numerous network-based smartphone applications. In many cases, achieving network energy savings must reside on the mobile device's OS to effectively and centrally manage the data scheduling decisions transparent to applications and with minimal changes to the network.

The key mechanism that determines the energy consumed by cellular network interface is the radio resource control (RRC) state machine~\cite{imc.3g} pre-defined by carriers (covered in more details in Section~\ref{sec:bkg.rrc}) that governs when radio resources are acquired and released. Previous studies~\cite{imc.tailender, imc.3g, qian10_icnp, huang_mobisys12} have shown that the origins of low resource efficiency comes from the way radio resources are \emph{released}. Radio resources are only released after an idle time (\aka Radio Resource Control (RRC) tail~\cite{imc.tailender}) controlled by a statically configured inactivity timer. The tail is necessary and important for cellular networks to prevent frequent state promotions (resource allocation), which can cause unacceptably long delays for the UE, as well as additional processing overheads for the radio access network~\cite{poor, infocom_lee}. During the tail time, radio energy is essentially wasted. Values as large as 11.6 seconds are configured~\cite{huang12_mobisys} in current networks, contributing to about half of the total radio energy on user handsets (UEs) spent in idle times for common usage scenarios.

Without knowing when network traffic will occur, large tail timer settings are essentially a conservative way to ensure low signaling overhead due to state transitions, as signaling is known to be a bottleneck for cellular networks.  Furthermore, they also help minimize performance impact experienced by users caused by state promotion delays incurred whenever radio resource is acquired.
Given that application and user behavior are not random, using a statically configured inactivity timer is clearly suboptimal. Smaller static timer values would help reduce radio energy, but is not an option due to the risk of overloading cellular networks caused by signaling load increase.

An attractive alternative is to configure the timer dynamically --- adaptively performing radio resource release either signaled by the UE or triggered by the network itself by monitoring the UE traffic, accommodating different traffic patterns, improving the overall resource efficiency. But the key challenge is determining \emph{when} to release resources, which essentially comes down to accurate and efficient prediction of the idle time period. Clearly, the best time to do so is when the UE is about to experience a long idle time period, otherwise the incurred resource allocation overhead (\ie signaling load) is wasteful due to unnecessary radio state transitions, and the achieved resource savings are very small. Therefore, accurate and efficient prediction of the idle time period is a critical prerequisite for dynamic timer schemes.

We propose \NAMEFULL (\NAME), a practical system that makes dynamic decisions to deallocate radio resources based on accurate and efficient prediction of network idle times. Using 7-month-long real-world cellular traces, we comprehensively evaluate it using various traffic features and machine learning algorithms. Properly configured, it correctly predicts 85.88\% of idle time instances, achieving radio energy savings of 59.07\%, at the cost of 91.01\% of additional signaling overhead, significantly outperforming existing proposals. It incurs negligible energy overhead and has fast response times, demonstrating the practicality of deploying the system on contemporary smartphones.

Besides, we also consider a novel angle to the above problem orthogonal to \NAME and explore the impact of {\bf screen status}, \ie whether the screen is on or off, on the device's network traffic patterns. We find that off-screen traffic accounts for 58.5\% of the total radio energy consumption although their traffic volume contribution is much smaller. Such unexpected results are attributed to the unique cellular resource management policy that is not well understood by developers, leading to cellular unfriendly mobile apps. We then make a further step by proposing screen-aware optimization, given that the screen status is easy to monitor for most mobile OSes. We propose that the screen-off traffic should not be treated the same as the screen-on traffic for traffic optimization purposes, and the former can be optimized more aggressively.  The main intuition is that the user (and possibly application)  behavior have significant differences when the screen is on {\em v.s.} off, resulting in different traffic patterns and different performance requirements. When the screen is off, there is a much higher chance that the user is not actively interacting with the device and the network traffic is most likely to be more delay tolerant. Hence we can be more aggressive in optimizing this traffic using techniques such as batching and fast dormancy. In contrast, when the screen is on, it is harder to predict the delay sensitivity of the network traffic and aggressive optimizations may harm the user experience. To validate this intuition, we characterize the screen-off traffic for a real-world user data set and evaluate the benefits of using ``screen-aware'' optimization for balancing UE energy savings and the resulting overheads in radio resource usage and response delay. The proposed screen-aware optimization focuses on the overall device traffic, and is complementary to other efficiency improvement efforts, \eg better application design. Our proposal can better balance the key tradeoffs in cellular networks. 






\section{Thesis Organization}

This dissertation is structured as follows. Chapter~\ref{chap:bkg} provides necessary backgrounds for resource management in cellular networks. We present the design of \mobiperf tool and network performance characterization in Chapter~\ref{chap:net} followed by the smartphone application performance study in Chapter~\ref{chap:app}. We study the effect of network protocol and application behavior on performance for one large commercial LTE Network in Chapter~\ref{chap:tcp}.  We analyze the power characterization of smartphone applications in Chapter~\ref{chap:power} and present the design, implementation and evaluation for the smartphone energy optimization system \NAME in Chapter~\ref{chap:optimize}. We summarize the related works in Chapter~\ref{chap:related}, before concluding in Chapter~\ref{chap:conc}.




\oldstuff{
Chapter~\ref{chap:net}: MobiPerf: 3G/4G network performance characterization
	+ Motivation
	+ Metrics
	+ Design and implementation of tools
	+ Results
	+ Implications (compare across network types, compare over time)
	
Chapter~\ref{chap:app}: smartphone application study
	+ Motivation
	+ Web based application running time breakdown.
	+ Video/audio streaming applications (sigcomm submission, mobisys10)
	
Chapter~\ref{chap:tcp}: LTE traffic pattern study Interplay of TCP, smartphone applications and cellular networks
	+ Motivation
	+ Sigcomm draft characterization part
	+ Sigcomm draft inefficient TCP part
	
Chapter~\ref{chap:power}: smartphone power model
	+ Motivation
	+ Measurement methodology: tools, experiments
	+ Power model
	+ How to use power model: trace-driven network/power simulation methodology (validation)
	+ Screen status's impact on traffic pattern
	+ Power footprint of apps (mobisys12)
		
Chapter~\ref{chap:optimize}: RadioProphet: optimize for better balance of performance and energy
	+ RadioProphet
	+ screen-aware optimization

Chapter~\ref{chap:related}: related work
}
